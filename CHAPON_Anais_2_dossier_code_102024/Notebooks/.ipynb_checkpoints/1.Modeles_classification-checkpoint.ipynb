{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eaeb412-a43a-44b7-a39e-7157a3d90532",
   "metadata": {},
   "source": [
    "<center><img src=\"logo_pret_a_depenser.png\">\n",
    "\n",
    "\n",
    "<center> Notebook 2 - Modèles de classification </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a6dac-8ecd-4f15-9467-c33252309bba",
   "metadata": {},
   "source": [
    "PYCARAT classification binaire et feature importance : \n",
    "    https://medium.com/@afadhilen/feature-importance-and-binary-classification-using-pycaret-a94b9a6a5b03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32991fec-0d3b-4c18-ae30-afe6d00fb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "###importation des bibliothèques : \n",
    "### bibliothèques générales : \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### visualisation : \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn import set_config\n",
    "\n",
    "### preprocessing : \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn import datasets, preprocessing, model_selection, ensemble, pipeline\n",
    "\n",
    "### labelisation : \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "### validation croisée : \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "###features importance : \n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "### Modèles : \n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "### PYCARAT : \n",
    "#import pycaret\n",
    "#from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a56c77-e9a1-4c29-9941-24820b7fa750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.4.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "#print(f\"pycaret version: {pycaret.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8054e861-b496-48e1-a7bf-fc7c1c3f51d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Importation du dataset : \n",
    "df_encoded = pd.read_csv('../Données/df_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fe852b-2e45-4a11-9ce3-47d6dcbb0851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355972, 626)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_AMT_INSTALMENT_MAX</th>\n",
       "      <th>CC_AMT_INSTALMENT_MEAN</th>\n",
       "      <th>CC_AMT_INSTALMENT_SUM</th>\n",
       "      <th>CC_AMT_INSTALMENT_VAR</th>\n",
       "      <th>CC_AMT_PAYMENT_MIN</th>\n",
       "      <th>CC_AMT_PAYMENT_MAX</th>\n",
       "      <th>CC_AMT_PAYMENT_MEAN</th>\n",
       "      <th>CC_AMT_PAYMENT_SUM</th>\n",
       "      <th>CC_AMT_PAYMENT_VAR</th>\n",
       "      <th>CC_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>202500</td>\n",
       "      <td>406597</td>\n",
       "      <td>24700</td>\n",
       "      <td>351000</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>-3648</td>\n",
       "      <td>-2120</td>\n",
       "      <td>...</td>\n",
       "      <td>53093.0</td>\n",
       "      <td>11558.473684</td>\n",
       "      <td>219611.0</td>\n",
       "      <td>1.011643e+08</td>\n",
       "      <td>9251.0</td>\n",
       "      <td>53093.0</td>\n",
       "      <td>11558.473684</td>\n",
       "      <td>219611.0</td>\n",
       "      <td>1.011643e+08</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>1293502</td>\n",
       "      <td>35698</td>\n",
       "      <td>1129500</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>-1186</td>\n",
       "      <td>-291</td>\n",
       "      <td>...</td>\n",
       "      <td>560835.0</td>\n",
       "      <td>64754.000000</td>\n",
       "      <td>1618850.0</td>\n",
       "      <td>1.221965e+10</td>\n",
       "      <td>6662.0</td>\n",
       "      <td>560835.0</td>\n",
       "      <td>64754.000000</td>\n",
       "      <td>1618850.0</td>\n",
       "      <td>1.221965e+10</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>67500</td>\n",
       "      <td>135000</td>\n",
       "      <td>6750</td>\n",
       "      <td>135000</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>-4260</td>\n",
       "      <td>-2531</td>\n",
       "      <td>...</td>\n",
       "      <td>10573.0</td>\n",
       "      <td>7095.666667</td>\n",
       "      <td>21287.0</td>\n",
       "      <td>9.068885e+06</td>\n",
       "      <td>5357.0</td>\n",
       "      <td>10573.0</td>\n",
       "      <td>7095.666667</td>\n",
       "      <td>21287.0</td>\n",
       "      <td>9.068885e+06</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>135000</td>\n",
       "      <td>312682</td>\n",
       "      <td>29686</td>\n",
       "      <td>297000</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>-9833</td>\n",
       "      <td>-2437</td>\n",
       "      <td>...</td>\n",
       "      <td>691786.0</td>\n",
       "      <td>62946.437500</td>\n",
       "      <td>1007143.0</td>\n",
       "      <td>2.825679e+10</td>\n",
       "      <td>2482.0</td>\n",
       "      <td>691786.0</td>\n",
       "      <td>62946.437500</td>\n",
       "      <td>1007143.0</td>\n",
       "      <td>2.825679e+10</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>121500</td>\n",
       "      <td>513000</td>\n",
       "      <td>21865</td>\n",
       "      <td>513000</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>-4311</td>\n",
       "      <td>-3458</td>\n",
       "      <td>...</td>\n",
       "      <td>22678.0</td>\n",
       "      <td>12665.878788</td>\n",
       "      <td>835948.0</td>\n",
       "      <td>6.166609e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22678.0</td>\n",
       "      <td>12213.500000</td>\n",
       "      <td>806091.0</td>\n",
       "      <td>6.476915e+07</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0      100002             0            202500      406597        24700   \n",
       "1      100003             0            270000     1293502        35698   \n",
       "2      100004             0             67500      135000         6750   \n",
       "3      100006             0            135000      312682        29686   \n",
       "4      100007             0            121500      513000        21865   \n",
       "\n",
       "   AMT_GOODS_PRICE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "0           351000       -9461           -637              -3648   \n",
       "1          1129500      -16765          -1188              -1186   \n",
       "2           135000      -19046           -225              -4260   \n",
       "3           297000      -19005          -3039              -9833   \n",
       "4           513000      -19932          -3038              -4311   \n",
       "\n",
       "   DAYS_ID_PUBLISH  ...  CC_AMT_INSTALMENT_MAX  CC_AMT_INSTALMENT_MEAN  \\\n",
       "0            -2120  ...                53093.0            11558.473684   \n",
       "1             -291  ...               560835.0            64754.000000   \n",
       "2            -2531  ...                10573.0             7095.666667   \n",
       "3            -2437  ...               691786.0            62946.437500   \n",
       "4            -3458  ...                22678.0            12665.878788   \n",
       "\n",
       "   CC_AMT_INSTALMENT_SUM  CC_AMT_INSTALMENT_VAR  CC_AMT_PAYMENT_MIN  \\\n",
       "0               219611.0           1.011643e+08              9251.0   \n",
       "1              1618850.0           1.221965e+10              6662.0   \n",
       "2                21287.0           9.068885e+06              5357.0   \n",
       "3              1007143.0           2.825679e+10              2482.0   \n",
       "4               835948.0           6.166609e+07                 0.0   \n",
       "\n",
       "   CC_AMT_PAYMENT_MAX  CC_AMT_PAYMENT_MEAN  CC_AMT_PAYMENT_SUM  \\\n",
       "0             53093.0         11558.473684            219611.0   \n",
       "1            560835.0         64754.000000           1618850.0   \n",
       "2             10573.0          7095.666667             21287.0   \n",
       "3            691786.0         62946.437500           1007143.0   \n",
       "4             22678.0         12213.500000            806091.0   \n",
       "\n",
       "   CC_AMT_PAYMENT_VAR  CC_COUNT  \n",
       "0        1.011643e+08      19.0  \n",
       "1        1.221965e+10      25.0  \n",
       "2        9.068885e+06       3.0  \n",
       "3        2.825679e+10      16.0  \n",
       "4        6.476915e+07      66.0  \n",
       "\n",
       "[5 rows x 626 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_encoded.shape)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f40d6f-1142-448e-8c1d-4d3befb356cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a544ad6b-ba49-4b5f-ae29-9895c5733303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Importation du dataset : \n",
    "df_petit = pd.read_csv('../Données/df_petit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0acff90b-cb5f-4914-a516-0c88494ab772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355972, 263)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>WALLSMATERIAL_MODE_Mixed</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Others</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>WALLSMATERIAL_MODE_Wooden</th>\n",
       "      <th>WALLSMATERIAL_MODE_nan</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>EMERGENCYSTATE_MODE_Yes</th>\n",
       "      <th>EMERGENCYSTATE_MODE_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100002     1.0             0          202500.0    406597.5   \n",
       "1      100003     0.0             0          270000.0   1293502.5   \n",
       "2      100004     0.0             0           67500.0    135000.0   \n",
       "3      100006     0.0             0          135000.0    312682.5   \n",
       "4      100007     0.0             0          121500.0    513000.0   \n",
       "\n",
       "   AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  \\\n",
       "0      24700.5         351000.0                    0.018801       -9461   \n",
       "1      35698.5        1129500.0                    0.003541      -16765   \n",
       "2       6750.0         135000.0                    0.010032      -19046   \n",
       "3      29686.5         297000.0                    0.008019      -19005   \n",
       "4      21865.5         513000.0                    0.028663      -19932   \n",
       "\n",
       "   DAYS_EMPLOYED  ...  WALLSMATERIAL_MODE_Mixed  \\\n",
       "0           -637  ...                     False   \n",
       "1          -1188  ...                     False   \n",
       "2           -225  ...                     False   \n",
       "3          -3039  ...                     False   \n",
       "4          -3038  ...                     False   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Monolithic  WALLSMATERIAL_MODE_Others  \\\n",
       "0                          False                      False   \n",
       "1                          False                      False   \n",
       "2                          False                      False   \n",
       "3                          False                      False   \n",
       "4                          False                      False   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Panel  WALLSMATERIAL_MODE_Stone, brick  \\\n",
       "0                     False                             True   \n",
       "1                     False                            False   \n",
       "2                     False                            False   \n",
       "3                     False                            False   \n",
       "4                     False                            False   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Wooden  WALLSMATERIAL_MODE_nan  EMERGENCYSTATE_MODE_No  \\\n",
       "0                      False                   False                    True   \n",
       "1                      False                   False                    True   \n",
       "2                      False                    True                   False   \n",
       "3                      False                    True                   False   \n",
       "4                      False                    True                   False   \n",
       "\n",
       "   EMERGENCYSTATE_MODE_Yes  EMERGENCYSTATE_MODE_nan  \n",
       "0                    False                    False  \n",
       "1                    False                    False  \n",
       "2                    False                     True  \n",
       "3                    False                     True  \n",
       "4                    False                     True  \n",
       "\n",
       "[5 rows x 263 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_petit.shape)\n",
    "df_petit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eece46c7-a529-4d4b-a19a-04bd8c06d7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME_CONTRACT_TYPE_Cash loans', 'NAME_CONTRACT_TYPE_Revolving loans', 'NAME_CONTRACT_TYPE_nan', 'CODE_GENDER_F', 'CODE_GENDER_M', 'CODE_GENDER_nan', 'FLAG_OWN_CAR_N', 'FLAG_OWN_CAR_Y', 'FLAG_OWN_CAR_nan', 'FLAG_OWN_REALTY_N', 'FLAG_OWN_REALTY_Y', 'FLAG_OWN_REALTY_nan', 'NAME_TYPE_SUITE_Children', 'NAME_TYPE_SUITE_Family', 'NAME_TYPE_SUITE_Group of people', 'NAME_TYPE_SUITE_Other_A', 'NAME_TYPE_SUITE_Other_B', 'NAME_TYPE_SUITE_Spouse, partner', 'NAME_TYPE_SUITE_Unaccompanied', 'NAME_TYPE_SUITE_nan', 'NAME_INCOME_TYPE_Businessman', 'NAME_INCOME_TYPE_Commercial associate', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_INCOME_TYPE_Pensioner', 'NAME_INCOME_TYPE_State servant', 'NAME_INCOME_TYPE_Student', 'NAME_INCOME_TYPE_Unemployed', 'NAME_INCOME_TYPE_Working', 'NAME_INCOME_TYPE_nan', 'NAME_EDUCATION_TYPE_Academic degree', 'NAME_EDUCATION_TYPE_Higher education', 'NAME_EDUCATION_TYPE_Incomplete higher', 'NAME_EDUCATION_TYPE_Lower secondary', 'NAME_EDUCATION_TYPE_Secondary / secondary special', 'NAME_EDUCATION_TYPE_nan', 'NAME_FAMILY_STATUS_Civil marriage', 'NAME_FAMILY_STATUS_Married', 'NAME_FAMILY_STATUS_Separated', 'NAME_FAMILY_STATUS_Single / not married', 'NAME_FAMILY_STATUS_Widow', 'NAME_FAMILY_STATUS_nan', 'NAME_HOUSING_TYPE_Co-op apartment', 'NAME_HOUSING_TYPE_House / apartment', 'NAME_HOUSING_TYPE_Municipal apartment', 'NAME_HOUSING_TYPE_Office apartment', 'NAME_HOUSING_TYPE_Rented apartment', 'NAME_HOUSING_TYPE_With parents', 'NAME_HOUSING_TYPE_nan', 'OCCUPATION_TYPE_Accountants', 'OCCUPATION_TYPE_Cleaning staff', 'OCCUPATION_TYPE_Cooking staff', 'OCCUPATION_TYPE_Core staff', 'OCCUPATION_TYPE_Drivers', 'OCCUPATION_TYPE_HR staff', 'OCCUPATION_TYPE_High skill tech staff', 'OCCUPATION_TYPE_IT staff', 'OCCUPATION_TYPE_Laborers', 'OCCUPATION_TYPE_Low-skill Laborers', 'OCCUPATION_TYPE_Managers', 'OCCUPATION_TYPE_Medicine staff', 'OCCUPATION_TYPE_Private service staff', 'OCCUPATION_TYPE_Realty agents', 'OCCUPATION_TYPE_Sales staff', 'OCCUPATION_TYPE_Secretaries', 'OCCUPATION_TYPE_Security staff', 'OCCUPATION_TYPE_Waiters/barmen staff', 'OCCUPATION_TYPE_nan', 'WEEKDAY_APPR_PROCESS_START_FRIDAY', 'WEEKDAY_APPR_PROCESS_START_MONDAY', 'WEEKDAY_APPR_PROCESS_START_SATURDAY', 'WEEKDAY_APPR_PROCESS_START_SUNDAY', 'WEEKDAY_APPR_PROCESS_START_THURSDAY', 'WEEKDAY_APPR_PROCESS_START_TUESDAY', 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY', 'WEEKDAY_APPR_PROCESS_START_nan', 'ORGANIZATION_TYPE_Advertising', 'ORGANIZATION_TYPE_Agriculture', 'ORGANIZATION_TYPE_Bank', 'ORGANIZATION_TYPE_Business Entity Type 1', 'ORGANIZATION_TYPE_Business Entity Type 2', 'ORGANIZATION_TYPE_Business Entity Type 3', 'ORGANIZATION_TYPE_Cleaning', 'ORGANIZATION_TYPE_Construction', 'ORGANIZATION_TYPE_Culture', 'ORGANIZATION_TYPE_Electricity', 'ORGANIZATION_TYPE_Emergency', 'ORGANIZATION_TYPE_Government', 'ORGANIZATION_TYPE_Hotel', 'ORGANIZATION_TYPE_Housing', 'ORGANIZATION_TYPE_Industry: type 1', 'ORGANIZATION_TYPE_Industry: type 10', 'ORGANIZATION_TYPE_Industry: type 11', 'ORGANIZATION_TYPE_Industry: type 12', 'ORGANIZATION_TYPE_Industry: type 13', 'ORGANIZATION_TYPE_Industry: type 2', 'ORGANIZATION_TYPE_Industry: type 3', 'ORGANIZATION_TYPE_Industry: type 4', 'ORGANIZATION_TYPE_Industry: type 5', 'ORGANIZATION_TYPE_Industry: type 6', 'ORGANIZATION_TYPE_Industry: type 7', 'ORGANIZATION_TYPE_Industry: type 8', 'ORGANIZATION_TYPE_Industry: type 9', 'ORGANIZATION_TYPE_Insurance', 'ORGANIZATION_TYPE_Kindergarten', 'ORGANIZATION_TYPE_Legal Services', 'ORGANIZATION_TYPE_Medicine', 'ORGANIZATION_TYPE_Military', 'ORGANIZATION_TYPE_Mobile', 'ORGANIZATION_TYPE_Other', 'ORGANIZATION_TYPE_Police', 'ORGANIZATION_TYPE_Postal', 'ORGANIZATION_TYPE_Realtor', 'ORGANIZATION_TYPE_Religion', 'ORGANIZATION_TYPE_Restaurant', 'ORGANIZATION_TYPE_School', 'ORGANIZATION_TYPE_Security', 'ORGANIZATION_TYPE_Security Ministries', 'ORGANIZATION_TYPE_Self-employed', 'ORGANIZATION_TYPE_Services', 'ORGANIZATION_TYPE_Telecom', 'ORGANIZATION_TYPE_Trade: type 1', 'ORGANIZATION_TYPE_Trade: type 2', 'ORGANIZATION_TYPE_Trade: type 3', 'ORGANIZATION_TYPE_Trade: type 4', 'ORGANIZATION_TYPE_Trade: type 5', 'ORGANIZATION_TYPE_Trade: type 6', 'ORGANIZATION_TYPE_Trade: type 7', 'ORGANIZATION_TYPE_Transport: type 1', 'ORGANIZATION_TYPE_Transport: type 2', 'ORGANIZATION_TYPE_Transport: type 3', 'ORGANIZATION_TYPE_Transport: type 4', 'ORGANIZATION_TYPE_University', 'ORGANIZATION_TYPE_XNA', 'ORGANIZATION_TYPE_nan', 'FONDKAPREMONT_MODE_not specified', 'FONDKAPREMONT_MODE_org spec account', 'FONDKAPREMONT_MODE_reg oper account', 'FONDKAPREMONT_MODE_reg oper spec account', 'FONDKAPREMONT_MODE_nan', 'HOUSETYPE_MODE_block of flats', 'HOUSETYPE_MODE_specific housing', 'HOUSETYPE_MODE_terraced house', 'HOUSETYPE_MODE_nan', 'WALLSMATERIAL_MODE_Block', 'WALLSMATERIAL_MODE_Mixed', 'WALLSMATERIAL_MODE_Monolithic', 'WALLSMATERIAL_MODE_Others', 'WALLSMATERIAL_MODE_Panel', 'WALLSMATERIAL_MODE_Stone, brick', 'WALLSMATERIAL_MODE_Wooden', 'WALLSMATERIAL_MODE_nan', 'EMERGENCYSTATE_MODE_No', 'EMERGENCYSTATE_MODE_Yes', 'EMERGENCYSTATE_MODE_nan']\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner les colonnes de type booléen\n",
    "bool_columns = df_petit.select_dtypes(include='bool').columns.tolist()\n",
    "\n",
    "# Afficher la liste des colonnes booléennes\n",
    "print(bool_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9434064-e492-4063-ac9b-943bc172c3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          True\n",
       "1          True\n",
       "2         False\n",
       "3          True\n",
       "4          True\n",
       "          ...  \n",
       "355967     True\n",
       "355968     True\n",
       "355969     True\n",
       "355970     True\n",
       "355971     True\n",
       "Name: NAME_CONTRACT_TYPE_Cash loans, Length: 355972, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_petit['NAME_CONTRACT_TYPE_Cash loans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f218ee9-9e55-45e6-b33c-07791edf0c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_petit_encoded = pd.read_csv('../Données/df_petit_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73286e3e-84bd-4695-a31f-f77d58a56188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN dans TARGET : 48744\n"
     ]
    }
   ],
   "source": [
    "# Compter les NaN dans une colonne spécifique (par exemple, 'col1')\n",
    "nan_count_TARGET = df_petit_encoded['TARGET'].isna().sum()\n",
    "print(f\"Nombre de NaN dans TARGET : {nan_count_TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50bdc3f9-5c2f-47b7-8254-96575be0ea31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355972, 247)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>...</th>\n",
       "      <th>WALLSMATERIAL_MODE_Block</th>\n",
       "      <th>WALLSMATERIAL_MODE_Mixed</th>\n",
       "      <th>WALLSMATERIAL_MODE_Monolithic</th>\n",
       "      <th>WALLSMATERIAL_MODE_Others</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>WALLSMATERIAL_MODE_Wooden</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>EMERGENCYSTATE_MODE_Yes</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>202500</td>\n",
       "      <td>406597</td>\n",
       "      <td>24700</td>\n",
       "      <td>351000</td>\n",
       "      <td>0</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>-3648</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>1293502</td>\n",
       "      <td>35698</td>\n",
       "      <td>1129500</td>\n",
       "      <td>0</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>-1186</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>67500</td>\n",
       "      <td>135000</td>\n",
       "      <td>6750</td>\n",
       "      <td>135000</td>\n",
       "      <td>0</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>-4260</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>135000</td>\n",
       "      <td>312682</td>\n",
       "      <td>29686</td>\n",
       "      <td>297000</td>\n",
       "      <td>0</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>-9833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>121500</td>\n",
       "      <td>513000</td>\n",
       "      <td>21865</td>\n",
       "      <td>513000</td>\n",
       "      <td>0</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>-4311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0      100002             0            202500      406597        24700   \n",
       "1      100003             0            270000     1293502        35698   \n",
       "2      100004             0             67500      135000         6750   \n",
       "3      100006             0            135000      312682        29686   \n",
       "4      100007             0            121500      513000        21865   \n",
       "\n",
       "   AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "0           351000                           0       -9461           -637   \n",
       "1          1129500                           0      -16765          -1188   \n",
       "2           135000                           0      -19046           -225   \n",
       "3           297000                           0      -19005          -3039   \n",
       "4           513000                           0      -19932          -3038   \n",
       "\n",
       "   DAYS_REGISTRATION  ...  WALLSMATERIAL_MODE_Block  WALLSMATERIAL_MODE_Mixed  \\\n",
       "0              -3648  ...                         0                         0   \n",
       "1              -1186  ...                         1                         0   \n",
       "2              -4260  ...                         0                         0   \n",
       "3              -9833  ...                         0                         0   \n",
       "4              -4311  ...                         0                         0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Monolithic  WALLSMATERIAL_MODE_Others  \\\n",
       "0                              0                          0   \n",
       "1                              0                          0   \n",
       "2                              0                          0   \n",
       "3                              0                          0   \n",
       "4                              0                          0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Panel  WALLSMATERIAL_MODE_Stone, brick  \\\n",
       "0                         0                                1   \n",
       "1                         0                                0   \n",
       "2                         0                                0   \n",
       "3                         0                                0   \n",
       "4                         0                                0   \n",
       "\n",
       "   WALLSMATERIAL_MODE_Wooden  EMERGENCYSTATE_MODE_No  EMERGENCYSTATE_MODE_Yes  \\\n",
       "0                          0                       1                        0   \n",
       "1                          0                       1                        0   \n",
       "2                          0                       0                        0   \n",
       "3                          0                       0                        0   \n",
       "4                          0                       0                        0   \n",
       "\n",
       "   TARGET  \n",
       "0     1.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_petit_encoded.shape)\n",
    "df_petit_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec876a-088c-4aa3-ae1a-3568d2ba75b8",
   "metadata": {},
   "source": [
    "# 2. Modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef741e-1ed8-49ea-861b-11ce932e5e81",
   "metadata": {},
   "source": [
    "Les étapes ici sont les suivantes : \n",
    "- Entraînement Initial du Modèle : entraîner un modèle sur toutes les features pour obtenir les importances des features.\n",
    "\n",
    "- Sélection des Features Importantes : utiliser les importances des features pour sélectionner les plus pertinentes.\n",
    "\n",
    "- Optimiser les Hyperparamètres pour chaque Modèle et Stratégie avec les Features Sélectionnées.\n",
    "\n",
    "- Entraînement Final du Modèle avec les Features Sélectionnées : entraîner et évaluer le modèle sur le dataset réduit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336636d3-c9e2-420b-b68e-78409df7e697",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons préparer notre dataset. \n",
    "Puis nous testerons différents paramètres : \n",
    "- 3 modèles : \n",
    "    - Dummy Regressor, qui nous sert de ligne de base\n",
    "    - Regression logistique\n",
    "    - Light GBM\n",
    "- 2 méthodes d'équilibrage : \n",
    "    - ClassWeight : \n",
    "    - SMOTE : \n",
    "- 2 méthodes de calcul de performances business : \n",
    "    - F-beta Score :\n",
    "    - Business Score : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd593426-36cb-458f-816d-b86b2414835f",
   "metadata": {},
   "source": [
    "## 2.0 Découpage Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b0942-f8b6-48da-8172-2dd3b88ba06b",
   "metadata": {},
   "source": [
    "### 2.0.1 Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d57f1e5-c870-4944-af17-8d59fd60a623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Définition du jeu de données d'entrainement et de test:\n",
    "application_train = df_encoded.dropna(subset=['TARGET'])\n",
    "application_test = df_encoded[df_encoded['TARGET'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e76e09-d745-4113-8eea-71dadd9272ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307228, 626)\n",
      "(48744, 626)\n"
     ]
    }
   ],
   "source": [
    "print (application_train.shape)\n",
    "print (application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4403c918-0b83-4447-ae7f-3085f6a1257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### enregistrement des jeux de données : \n",
    "application_train.to_csv('application_train_encoded.csv', index=False)\n",
    "application_test.to_csv('application_test_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d3258-c4f4-442d-b722-c1e5505e45f4",
   "metadata": {},
   "source": [
    "### 2.0.2 Dataset petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "650bc38e-aafe-4367-a25c-3fa62bb676ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Définition du jeu de données d'entrainement et de test:\n",
    "application_train_petit = df_petit.dropna(subset=['TARGET'])\n",
    "application_test_petit = df_petit[df_petit['TARGET'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e82d6e7-3d2a-42b2-a105-23d305433662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307228, 263)\n",
      "(48744, 263)\n"
     ]
    }
   ],
   "source": [
    "print (application_train_petit.shape)\n",
    "print (application_test_petit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72e1402a-02a7-44ea-9abe-4c33f4909cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Remplissage valeur manquante par moyenne : \n",
    "application_train_petit_clean = application_train_petit.copy()\n",
    "\n",
    "# Calculer la moyenne uniquement pour les colonnes numériques\n",
    "mean_values = application_train_petit.select_dtypes(include=[np.number]).mean()\n",
    "application_train_petit_clean = application_train_petit.fillna(mean_values)\n",
    "\n",
    "# Réinitialiser l'index après le dropna pour éviter des indices discontinus\n",
    "application_train_petit = application_train_petit.dropna(subset=['TARGET']).reset_index(drop=True)\n",
    "application_test_petit = df_petit[df_petit['TARGET'].isna()].reset_index(drop=True)\n",
    "\n",
    "#application_train_petit_clean = application_train_petit.fillna(application_train_petit.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc850349-d4f6-4537-ae61-afe412a69f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_native_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### enregistrement des jeux de données : \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m application_train_petit\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_train_petit.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m application_test_petit\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_test_petit.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   3963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   3964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;124;03m    Return the elements in the given *positional* indices along an axis.\u001b[39;00m\n\u001b[0;32m   3966\u001b[0m \n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;124;03m    This means that we are not indexing according to actual values in\u001b[39;00m\n\u001b[0;32m   3968\u001b[0m \u001b[38;5;124;03m    the index attribute of the object. We are indexing according to the\u001b[39;00m\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;124;03m    actual position of the element in the object.\u001b[39;00m\n\u001b[0;32m   3970\u001b[0m \n\u001b[0;32m   3971\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   3972\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   3973\u001b[0m \u001b[38;5;124;03m    indices : array-like\u001b[39;00m\n\u001b[0;32m   3974\u001b[0m \u001b[38;5;124;03m        An array of ints indicating which positions to take.\u001b[39;00m\n\u001b[0;32m   3975\u001b[0m \u001b[38;5;124;03m    axis : {0 or 'index', 1 or 'columns', None}, default 0\u001b[39;00m\n\u001b[0;32m   3976\u001b[0m \u001b[38;5;124;03m        The axis on which to select elements. ``0`` means that we are\u001b[39;00m\n\u001b[0;32m   3977\u001b[0m \u001b[38;5;124;03m        selecting rows, ``1`` means that we are selecting columns.\u001b[39;00m\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;124;03m        For `Series` this parameter is unused and defaults to 0.\u001b[39;00m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;124;03m    **kwargs\u001b[39;00m\n\u001b[0;32m   3980\u001b[0m \u001b[38;5;124;03m        For compatibility with :meth:`numpy.take`. Has no effect on the\u001b[39;00m\n\u001b[0;32m   3981\u001b[0m \u001b[38;5;124;03m        output.\u001b[39;00m\n\u001b[0;32m   3982\u001b[0m \n\u001b[0;32m   3983\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \u001b[38;5;124;03m    same type as caller\u001b[39;00m\n\u001b[0;32m   3986\u001b[0m \u001b[38;5;124;03m        An array-like containing the elements taken from the object.\u001b[39;00m\n\u001b[0;32m   3987\u001b[0m \n\u001b[0;32m   3988\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m   3989\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   3990\u001b[0m \u001b[38;5;124;03m    DataFrame.loc : Select a subset of a DataFrame by labels.\u001b[39;00m\n\u001b[0;32m   3991\u001b[0m \u001b[38;5;124;03m    DataFrame.iloc : Select a subset of a DataFrame by positions.\u001b[39;00m\n\u001b[0;32m   3992\u001b[0m \u001b[38;5;124;03m    numpy.take : Take elements from an array along an axis.\u001b[39;00m\n\u001b[0;32m   3993\u001b[0m \n\u001b[0;32m   3994\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   3995\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;124;03m    >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m \u001b[38;5;124;03m    ...                    ('parrot', 'bird', 24.0),\u001b[39;00m\n\u001b[0;32m   3998\u001b[0m \u001b[38;5;124;03m    ...                    ('lion', 'mammal', 80.5),\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m    ...                    ('monkey', 'mammal', np.nan)],\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m    ...                   columns=['name', 'class', 'max_speed'],\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;124;03m    ...                   index=[0, 2, 3, 1])\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;124;03m    >>> df\u001b[39;00m\n\u001b[0;32m   4003\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4004\u001b[0m \u001b[38;5;124;03m    0  falcon    bird      389.0\u001b[39;00m\n\u001b[0;32m   4005\u001b[0m \u001b[38;5;124;03m    2  parrot    bird       24.0\u001b[39;00m\n\u001b[0;32m   4006\u001b[0m \u001b[38;5;124;03m    3    lion  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4008\u001b[0m \n\u001b[0;32m   4009\u001b[0m \u001b[38;5;124;03m    Take elements at positions 0 and 3 along the axis 0 (default).\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m \n\u001b[0;32m   4011\u001b[0m \u001b[38;5;124;03m    Note how the actual indices selected (0 and 1) do not correspond to\u001b[39;00m\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;124;03m    our selected indices 0 and 3. That's because we are selecting the 0th\u001b[39;00m\n\u001b[0;32m   4013\u001b[0m \u001b[38;5;124;03m    and 3rd rows, not rows whose indices equal 0 and 3.\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m \n\u001b[0;32m   4015\u001b[0m \u001b[38;5;124;03m    >>> df.take([0, 3])\u001b[39;00m\n\u001b[0;32m   4016\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4017\u001b[0m \u001b[38;5;124;03m    0  falcon    bird      389.0\u001b[39;00m\n\u001b[0;32m   4018\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4019\u001b[0m \n\u001b[0;32m   4020\u001b[0m \u001b[38;5;124;03m    Take elements at indices 1 and 2 along the axis 1 (column selection).\u001b[39;00m\n\u001b[0;32m   4021\u001b[0m \n\u001b[0;32m   4022\u001b[0m \u001b[38;5;124;03m    >>> df.take([1, 2], axis=1)\u001b[39;00m\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;124;03m        class  max_speed\u001b[39;00m\n\u001b[0;32m   4024\u001b[0m \u001b[38;5;124;03m    0    bird      389.0\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m \u001b[38;5;124;03m    2    bird       24.0\u001b[39;00m\n\u001b[0;32m   4026\u001b[0m \u001b[38;5;124;03m    3  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4027\u001b[0m \u001b[38;5;124;03m    1  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4028\u001b[0m \n\u001b[0;32m   4029\u001b[0m \u001b[38;5;124;03m    We may take elements using negative integers for positive indices,\u001b[39;00m\n\u001b[0;32m   4030\u001b[0m \u001b[38;5;124;03m    starting from the end of the object, just like with Python lists.\u001b[39;00m\n\u001b[0;32m   4031\u001b[0m \n\u001b[0;32m   4032\u001b[0m \u001b[38;5;124;03m    >>> df.take([-1, -2])\u001b[39;00m\n\u001b[0;32m   4033\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4034\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4035\u001b[0m \u001b[38;5;124;03m    3    lion  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4038\u001b[0m     nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[0;32m   4040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indices, \u001b[38;5;28mslice\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:995\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:96\u001b[0m, in \u001b[0;36mCSVFormatter.__init__\u001b[1;34m(self, formatter, path_or_buf, sep, cols, index_label, mode, encoding, errors, compression, quoting, lineterminator, chunksize, quotechar, date_format, doublequote, escapechar, storage_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator \u001b[38;5;241m=\u001b[39m lineterminator \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_format \u001b[38;5;241m=\u001b[39m date_format\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_columns(cols)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_chunksize(chunksize)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:168\u001b[0m, in \u001b[0;36mCSVFormatter._initialize_columns\u001b[1;34m(self, cols)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# update columns to include possible multiplicity of dupes\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# and make sure cols is just a list of labels\u001b[39;00m\n\u001b[0;32m    167\u001b[0m new_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_cols\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_native_types'"
     ]
    }
   ],
   "source": [
    "### enregistrement des jeux de données : \n",
    "application_train_petit.to_csv('application_train_petit.csv', index=False)\n",
    "application_test_petit.to_csv('application_test_petit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405318f-2e61-496b-97dd-6a4d530de34a",
   "metadata": {},
   "source": [
    "### 2.0.3 Dataset petit encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e73c53e-1a90-4b31-af24-9879daf8264c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Définition du jeu de données d'entrainement et de test:\n",
    "application_train_petit_encoded = df_petit_encoded.dropna(subset=['TARGET'])\n",
    "application_test_petit_encoded = df_petit_encoded[df_petit_encoded['TARGET'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fd6234e-3f6d-4d08-a4e8-7605b171ab6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307228, 247)\n",
      "(48744, 247)\n"
     ]
    }
   ],
   "source": [
    "print (application_train_petit_encoded.shape)\n",
    "print (application_test_petit_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd96454e-2783-44ba-8101-855ed5b0f442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_native_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### enregistrement des jeux de données : \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m application_train_petit_encoded\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_train_petit_encoded.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m application_test_petit_encoded\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_test_petit_encoded.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   3963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   3964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;124;03m    Return the elements in the given *positional* indices along an axis.\u001b[39;00m\n\u001b[0;32m   3966\u001b[0m \n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;124;03m    This means that we are not indexing according to actual values in\u001b[39;00m\n\u001b[0;32m   3968\u001b[0m \u001b[38;5;124;03m    the index attribute of the object. We are indexing according to the\u001b[39;00m\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;124;03m    actual position of the element in the object.\u001b[39;00m\n\u001b[0;32m   3970\u001b[0m \n\u001b[0;32m   3971\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   3972\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   3973\u001b[0m \u001b[38;5;124;03m    indices : array-like\u001b[39;00m\n\u001b[0;32m   3974\u001b[0m \u001b[38;5;124;03m        An array of ints indicating which positions to take.\u001b[39;00m\n\u001b[0;32m   3975\u001b[0m \u001b[38;5;124;03m    axis : {0 or 'index', 1 or 'columns', None}, default 0\u001b[39;00m\n\u001b[0;32m   3976\u001b[0m \u001b[38;5;124;03m        The axis on which to select elements. ``0`` means that we are\u001b[39;00m\n\u001b[0;32m   3977\u001b[0m \u001b[38;5;124;03m        selecting rows, ``1`` means that we are selecting columns.\u001b[39;00m\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;124;03m        For `Series` this parameter is unused and defaults to 0.\u001b[39;00m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;124;03m    **kwargs\u001b[39;00m\n\u001b[0;32m   3980\u001b[0m \u001b[38;5;124;03m        For compatibility with :meth:`numpy.take`. Has no effect on the\u001b[39;00m\n\u001b[0;32m   3981\u001b[0m \u001b[38;5;124;03m        output.\u001b[39;00m\n\u001b[0;32m   3982\u001b[0m \n\u001b[0;32m   3983\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \u001b[38;5;124;03m    same type as caller\u001b[39;00m\n\u001b[0;32m   3986\u001b[0m \u001b[38;5;124;03m        An array-like containing the elements taken from the object.\u001b[39;00m\n\u001b[0;32m   3987\u001b[0m \n\u001b[0;32m   3988\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m   3989\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   3990\u001b[0m \u001b[38;5;124;03m    DataFrame.loc : Select a subset of a DataFrame by labels.\u001b[39;00m\n\u001b[0;32m   3991\u001b[0m \u001b[38;5;124;03m    DataFrame.iloc : Select a subset of a DataFrame by positions.\u001b[39;00m\n\u001b[0;32m   3992\u001b[0m \u001b[38;5;124;03m    numpy.take : Take elements from an array along an axis.\u001b[39;00m\n\u001b[0;32m   3993\u001b[0m \n\u001b[0;32m   3994\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   3995\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;124;03m    >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m \u001b[38;5;124;03m    ...                    ('parrot', 'bird', 24.0),\u001b[39;00m\n\u001b[0;32m   3998\u001b[0m \u001b[38;5;124;03m    ...                    ('lion', 'mammal', 80.5),\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m    ...                    ('monkey', 'mammal', np.nan)],\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m    ...                   columns=['name', 'class', 'max_speed'],\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;124;03m    ...                   index=[0, 2, 3, 1])\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;124;03m    >>> df\u001b[39;00m\n\u001b[0;32m   4003\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4004\u001b[0m \u001b[38;5;124;03m    0  falcon    bird      389.0\u001b[39;00m\n\u001b[0;32m   4005\u001b[0m \u001b[38;5;124;03m    2  parrot    bird       24.0\u001b[39;00m\n\u001b[0;32m   4006\u001b[0m \u001b[38;5;124;03m    3    lion  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4008\u001b[0m \n\u001b[0;32m   4009\u001b[0m \u001b[38;5;124;03m    Take elements at positions 0 and 3 along the axis 0 (default).\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m \n\u001b[0;32m   4011\u001b[0m \u001b[38;5;124;03m    Note how the actual indices selected (0 and 1) do not correspond to\u001b[39;00m\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;124;03m    our selected indices 0 and 3. That's because we are selecting the 0th\u001b[39;00m\n\u001b[0;32m   4013\u001b[0m \u001b[38;5;124;03m    and 3rd rows, not rows whose indices equal 0 and 3.\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m \n\u001b[0;32m   4015\u001b[0m \u001b[38;5;124;03m    >>> df.take([0, 3])\u001b[39;00m\n\u001b[0;32m   4016\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4017\u001b[0m \u001b[38;5;124;03m    0  falcon    bird      389.0\u001b[39;00m\n\u001b[0;32m   4018\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4019\u001b[0m \n\u001b[0;32m   4020\u001b[0m \u001b[38;5;124;03m    Take elements at indices 1 and 2 along the axis 1 (column selection).\u001b[39;00m\n\u001b[0;32m   4021\u001b[0m \n\u001b[0;32m   4022\u001b[0m \u001b[38;5;124;03m    >>> df.take([1, 2], axis=1)\u001b[39;00m\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;124;03m        class  max_speed\u001b[39;00m\n\u001b[0;32m   4024\u001b[0m \u001b[38;5;124;03m    0    bird      389.0\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m \u001b[38;5;124;03m    2    bird       24.0\u001b[39;00m\n\u001b[0;32m   4026\u001b[0m \u001b[38;5;124;03m    3  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4027\u001b[0m \u001b[38;5;124;03m    1  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4028\u001b[0m \n\u001b[0;32m   4029\u001b[0m \u001b[38;5;124;03m    We may take elements using negative integers for positive indices,\u001b[39;00m\n\u001b[0;32m   4030\u001b[0m \u001b[38;5;124;03m    starting from the end of the object, just like with Python lists.\u001b[39;00m\n\u001b[0;32m   4031\u001b[0m \n\u001b[0;32m   4032\u001b[0m \u001b[38;5;124;03m    >>> df.take([-1, -2])\u001b[39;00m\n\u001b[0;32m   4033\u001b[0m \u001b[38;5;124;03m         name   class  max_speed\u001b[39;00m\n\u001b[0;32m   4034\u001b[0m \u001b[38;5;124;03m    1  monkey  mammal        NaN\u001b[39;00m\n\u001b[0;32m   4035\u001b[0m \u001b[38;5;124;03m    3    lion  mammal       80.5\u001b[39;00m\n\u001b[0;32m   4036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4038\u001b[0m     nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[0;32m   4040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indices, \u001b[38;5;28mslice\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:995\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:96\u001b[0m, in \u001b[0;36mCSVFormatter.__init__\u001b[1;34m(self, formatter, path_or_buf, sep, cols, index_label, mode, encoding, errors, compression, quoting, lineterminator, chunksize, quotechar, date_format, doublequote, escapechar, storage_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator \u001b[38;5;241m=\u001b[39m lineterminator \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_format \u001b[38;5;241m=\u001b[39m date_format\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_columns(cols)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_chunksize(chunksize)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:168\u001b[0m, in \u001b[0;36mCSVFormatter._initialize_columns\u001b[1;34m(self, cols)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# update columns to include possible multiplicity of dupes\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# and make sure cols is just a list of labels\u001b[39;00m\n\u001b[0;32m    167\u001b[0m new_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_cols\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_native_types'"
     ]
    }
   ],
   "source": [
    "### enregistrement des jeux de données : \n",
    "application_train_petit_encoded.to_csv('application_train_petit_encoded.csv', index=False)\n",
    "application_test_petit_encoded.to_csv('application_test_petit_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5ef3b-29bf-4c10-8690-9a244fdf9900",
   "metadata": {},
   "source": [
    "## 2.1 Dummy Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f916ee-bc8c-41ec-844c-d87d7284da68",
   "metadata": {},
   "source": [
    "### 2.1.1 Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1a1b0c-8395-47f5-bc1e-93a005eaaa21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn  # pour log le modèle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "import gc\n",
    "import time  # Pour mesurer le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4075cd6a-5ec1-4776-83ce-b2b1d199f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Définition de X et Y : \n",
    "# 1. Prétraiter les données\n",
    "X = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae56c57-b2a7-4a9a-9ac4-a2688ec0a0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15510"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Préparer la Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Hyperparamètres du DummyClassifier\n",
    "strategies = ['stratified', 'most_frequent', 'prior', 'uniform', 'constant']\n",
    "\n",
    "results = []\n",
    "\n",
    "mlflow.set_experiment(\"DummyClassifier\")\n",
    "\n",
    "for strategy in strategies:\n",
    "    with mlflow.start_run():\n",
    "        dummy = DummyClassifier(strategy=strategy, constant=1 if strategy == 'constant' else None)\n",
    "\n",
    "        # Mesurer le temps de traitement de fit\n",
    "        start_time_fit = time.time()\n",
    "        dummy.fit(X, y)\n",
    "        elapsed_time_fit = time.time() - start_time_fit\n",
    "\n",
    "        # Mesurer le temps de traitement de prédiction\n",
    "        start_time_predict = time.time()\n",
    "        y_prob = cross_val_predict(dummy, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "        elapsed_time_predict = time.time() - start_time_predict\n",
    "        \n",
    "        y_pred = (y_prob > 0.5)\n",
    "\n",
    "        # Calcul des scores\n",
    "        auc = roc_auc_score(y, y_prob)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "        cost = fp + 10 * fn\n",
    "\n",
    "        # Log les hyperparamètres, métriques et temps à MLflow\n",
    "        mlflow.log_param(\"Strategy\", strategy)\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        mlflow.log_metric(\"Fit Time\", elapsed_time_fit)\n",
    "        mlflow.log_metric(\"Prediction Time\", elapsed_time_predict)\n",
    "\n",
    "        # Log le modèle à MLflow\n",
    "        mlflow.sklearn.log_model(dummy, \"dummy_model\")\n",
    "\n",
    "        # Stockage et log des courbes ROC\n",
    "        fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "        \n",
    "        # Ajouter les résultats à la liste\n",
    "        results.append({\n",
    "            \"Strategy\": strategy,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Business Score\": cost,\n",
    "            \"Fit Time\": elapsed_time_fit,  \n",
    "            \"Prediction Time\": elapsed_time_predict  \n",
    "        })\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "956af165-4d00-4ce7-8737-f8f0d31157cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Dummy = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3205682f-266a-43ba-958e-1196e4c2a61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Dummy .to_csv('results_Dummy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7939dd6-a9fe-496c-94ce-c320f3803576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cd8f6_row0_col1, #T_cd8f6_row0_col2, #T_cd8f6_row0_col3, #T_cd8f6_row1_col2, #T_cd8f6_row1_col3, #T_cd8f6_row2_col1, #T_cd8f6_row2_col2, #T_cd8f6_row2_col3, #T_cd8f6_row4_col1, #T_cd8f6_row4_col4, #T_cd8f6_row4_col5 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cd8f6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cd8f6_level0_col0\" class=\"col_heading level0 col0\" >Strategy</th>\n",
       "      <th id=\"T_cd8f6_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_cd8f6_level0_col2\" class=\"col_heading level0 col2\" >Accuracy</th>\n",
       "      <th id=\"T_cd8f6_level0_col3\" class=\"col_heading level0 col3\" >Business Score</th>\n",
       "      <th id=\"T_cd8f6_level0_col4\" class=\"col_heading level0 col4\" >Fit Time</th>\n",
       "      <th id=\"T_cd8f6_level0_col5\" class=\"col_heading level0 col5\" >Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cd8f6_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_cd8f6_row0_col0\" class=\"data row0 col0\" >most_frequent</td>\n",
       "      <td id=\"T_cd8f6_row0_col1\" class=\"data row0 col1\" >0.500000</td>\n",
       "      <td id=\"T_cd8f6_row0_col2\" class=\"data row0 col2\" >0.919268</td>\n",
       "      <td id=\"T_cd8f6_row0_col3\" class=\"data row0 col3\" >248030</td>\n",
       "      <td id=\"T_cd8f6_row0_col4\" class=\"data row0 col4\" >0.009000</td>\n",
       "      <td id=\"T_cd8f6_row0_col5\" class=\"data row0 col5\" >2.271351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd8f6_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_cd8f6_row1_col0\" class=\"data row1 col0\" >prior</td>\n",
       "      <td id=\"T_cd8f6_row1_col1\" class=\"data row1 col1\" >0.499976</td>\n",
       "      <td id=\"T_cd8f6_row1_col2\" class=\"data row1 col2\" >0.919268</td>\n",
       "      <td id=\"T_cd8f6_row1_col3\" class=\"data row1 col3\" >248030</td>\n",
       "      <td id=\"T_cd8f6_row1_col4\" class=\"data row1 col4\" >0.008113</td>\n",
       "      <td id=\"T_cd8f6_row1_col5\" class=\"data row1 col5\" >2.215810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd8f6_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_cd8f6_row2_col0\" class=\"data row2 col0\" >uniform</td>\n",
       "      <td id=\"T_cd8f6_row2_col1\" class=\"data row2 col1\" >0.500000</td>\n",
       "      <td id=\"T_cd8f6_row2_col2\" class=\"data row2 col2\" >0.919268</td>\n",
       "      <td id=\"T_cd8f6_row2_col3\" class=\"data row2 col3\" >248030</td>\n",
       "      <td id=\"T_cd8f6_row2_col4\" class=\"data row2 col4\" >0.012138</td>\n",
       "      <td id=\"T_cd8f6_row2_col5\" class=\"data row2 col5\" >2.188634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd8f6_level0_row3\" class=\"row_heading level0 row3\" >0</th>\n",
       "      <td id=\"T_cd8f6_row3_col0\" class=\"data row3 col0\" >stratified</td>\n",
       "      <td id=\"T_cd8f6_row3_col1\" class=\"data row3 col1\" >0.498595</td>\n",
       "      <td id=\"T_cd8f6_row3_col2\" class=\"data row3 col2\" >0.851065</td>\n",
       "      <td id=\"T_cd8f6_row3_col3\" class=\"data row3 col3\" >251515</td>\n",
       "      <td id=\"T_cd8f6_row3_col4\" class=\"data row3 col4\" >0.015630</td>\n",
       "      <td id=\"T_cd8f6_row3_col5\" class=\"data row3 col5\" >2.528759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd8f6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cd8f6_row4_col0\" class=\"data row4 col0\" >constant</td>\n",
       "      <td id=\"T_cd8f6_row4_col1\" class=\"data row4 col1\" >0.500000</td>\n",
       "      <td id=\"T_cd8f6_row4_col2\" class=\"data row4 col2\" >0.080732</td>\n",
       "      <td id=\"T_cd8f6_row4_col3\" class=\"data row4 col3\" >282425</td>\n",
       "      <td id=\"T_cd8f6_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "      <td id=\"T_cd8f6_row4_col5\" class=\"data row4 col5\" >2.165263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x238a1fb23f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_Dummy  = (styled_df_Dummy.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score', 'Fit Time', 'Prediction Time'])\n",
    "                          .format({'Threshold': \"{:g}\"}))\n",
    "\n",
    "styled_df_Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059513f-c5ee-4c90-8956-7320c43923cf",
   "metadata": {},
   "source": [
    "### 2.1.2 Dataset petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a08d1b-078e-4fd7-b7a8-50d830c733d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Définition de X et Y : \n",
    "# 1. Prétraiter les données\n",
    "X = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_petit_clean[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5591c-a13f-4174-8d41-0271f8543170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "application_train_petit_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35aff8d4-ac87-4878-986f-0ba32650e508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Préparer la Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Hyperparamètres du DummyClassifier\n",
    "strategies = ['stratified', 'most_frequent', 'prior', 'uniform', 'constant']\n",
    "\n",
    "results = []\n",
    "\n",
    "mlflow.set_experiment(\"DummyClassifier\")\n",
    "\n",
    "for strategy in strategies:\n",
    "    with mlflow.start_run():\n",
    "        dummy = DummyClassifier(strategy=strategy, constant=1 if strategy == 'constant' else None)\n",
    "\n",
    "        # Mesurer le temps de traitement de fit\n",
    "        start_time_fit = time.time()\n",
    "        dummy.fit(X, y)\n",
    "        elapsed_time_fit = time.time() - start_time_fit\n",
    "\n",
    "        # Mesurer le temps de traitement de prédiction\n",
    "        start_time_predict = time.time()\n",
    "        y_prob = cross_val_predict(dummy, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "        elapsed_time_predict = time.time() - start_time_predict\n",
    "        \n",
    "        y_pred = (y_prob > 0.5)\n",
    "\n",
    "        # Calcul des scores\n",
    "        auc = roc_auc_score(y, y_prob)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "        cost = fp + 10 * fn\n",
    "\n",
    "        # Log les hyperparamètres, métriques et temps à MLflow\n",
    "        mlflow.log_param(\"Strategy\", strategy)\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        mlflow.log_metric(\"Fit Time\", elapsed_time_fit)\n",
    "        mlflow.log_metric(\"Prediction Time\", elapsed_time_predict)\n",
    "\n",
    "        # Log le modèle à MLflow\n",
    "        mlflow.sklearn.log_model(dummy, \"dummy_model\")\n",
    "\n",
    "        # Stockage et log des courbes ROC\n",
    "        fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "        \n",
    "        # Ajouter les résultats à la liste\n",
    "        results.append({\n",
    "            \"Strategy\": strategy,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Business Score\": cost\n",
    "        })\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d45dc86d-a6cc-4236-b3d3-245e0fc127e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Dummy = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3692281-97b0-4e5e-9535-d592323c9f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Dummy .to_csv('results_Dummy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "79a9e660-cb4b-400e-a1fa-e727306625ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_166a5_row0_col4, #T_166a5_row2_col2, #T_166a5_row46_col3 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_166a5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_166a5_level0_col0\" class=\"col_heading level0 col0\" >C</th>\n",
       "      <th id=\"T_166a5_level0_col1\" class=\"col_heading level0 col1\" >Threshold</th>\n",
       "      <th id=\"T_166a5_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_166a5_level0_col3\" class=\"col_heading level0 col3\" >Accuracy</th>\n",
       "      <th id=\"T_166a5_level0_col4\" class=\"col_heading level0 col4\" >Business Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row0\" class=\"row_heading level0 row0\" >27</th>\n",
       "      <td id=\"T_166a5_row0_col0\" class=\"data row0 col0\" >122.465193</td>\n",
       "      <td id=\"T_166a5_row0_col1\" class=\"data row0 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row0_col2\" class=\"data row0 col2\" >0.747856</td>\n",
       "      <td id=\"T_166a5_row0_col3\" class=\"data row0 col3\" >0.723782</td>\n",
       "      <td id=\"T_166a5_row0_col4\" class=\"data row0 col4\" >166996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row1\" class=\"row_heading level0 row1\" >31</th>\n",
       "      <td id=\"T_166a5_row1_col0\" class=\"data row1 col0\" >102.430697</td>\n",
       "      <td id=\"T_166a5_row1_col1\" class=\"data row1 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row1_col2\" class=\"data row1 col2\" >0.747856</td>\n",
       "      <td id=\"T_166a5_row1_col3\" class=\"data row1 col3\" >0.723837</td>\n",
       "      <td id=\"T_166a5_row1_col4\" class=\"data row1 col4\" >167033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row2\" class=\"row_heading level0 row2\" >22</th>\n",
       "      <td id=\"T_166a5_row2_col0\" class=\"data row2 col0\" >70.949651</td>\n",
       "      <td id=\"T_166a5_row2_col1\" class=\"data row2 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row2_col2\" class=\"data row2 col2\" >0.747866</td>\n",
       "      <td id=\"T_166a5_row2_col3\" class=\"data row2 col3\" >0.723752</td>\n",
       "      <td id=\"T_166a5_row2_col4\" class=\"data row2 col4\" >167041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row3\" class=\"row_heading level0 row3\" >5</th>\n",
       "      <td id=\"T_166a5_row3_col0\" class=\"data row3 col0\" >134.291697</td>\n",
       "      <td id=\"T_166a5_row3_col1\" class=\"data row3 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row3_col2\" class=\"data row3 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row3_col3\" class=\"data row3 col3\" >0.723762</td>\n",
       "      <td id=\"T_166a5_row3_col4\" class=\"data row3 col4\" >167056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row4\" class=\"row_heading level0 row4\" >42</th>\n",
       "      <td id=\"T_166a5_row4_col0\" class=\"data row4 col0\" >87.214196</td>\n",
       "      <td id=\"T_166a5_row4_col1\" class=\"data row4 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row4_col2\" class=\"data row4 col2\" >0.747853</td>\n",
       "      <td id=\"T_166a5_row4_col3\" class=\"data row4 col3\" >0.723811</td>\n",
       "      <td id=\"T_166a5_row4_col4\" class=\"data row4 col4\" >167068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row5\" class=\"row_heading level0 row5\" >36</th>\n",
       "      <td id=\"T_166a5_row5_col0\" class=\"data row5 col0\" >93.172129</td>\n",
       "      <td id=\"T_166a5_row5_col1\" class=\"data row5 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row5_col2\" class=\"data row5 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row5_col3\" class=\"data row5 col3\" >0.723808</td>\n",
       "      <td id=\"T_166a5_row5_col4\" class=\"data row5 col4\" >167069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row6\" class=\"row_heading level0 row6\" >41</th>\n",
       "      <td id=\"T_166a5_row6_col0\" class=\"data row6 col0\" >98.929739</td>\n",
       "      <td id=\"T_166a5_row6_col1\" class=\"data row6 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row6_col2\" class=\"data row6 col2\" >0.747853</td>\n",
       "      <td id=\"T_166a5_row6_col3\" class=\"data row6 col3\" >0.723759</td>\n",
       "      <td id=\"T_166a5_row6_col4\" class=\"data row6 col4\" >167075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row7\" class=\"row_heading level0 row7\" >49</th>\n",
       "      <td id=\"T_166a5_row7_col0\" class=\"data row7 col0\" >90.057235</td>\n",
       "      <td id=\"T_166a5_row7_col1\" class=\"data row7 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row7_col2\" class=\"data row7 col2\" >0.747849</td>\n",
       "      <td id=\"T_166a5_row7_col3\" class=\"data row7 col3\" >0.723762</td>\n",
       "      <td id=\"T_166a5_row7_col4\" class=\"data row7 col4\" >167083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row8\" class=\"row_heading level0 row8\" >23</th>\n",
       "      <td id=\"T_166a5_row8_col0\" class=\"data row8 col0\" >74.703880</td>\n",
       "      <td id=\"T_166a5_row8_col1\" class=\"data row8 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row8_col2\" class=\"data row8 col2\" >0.747859</td>\n",
       "      <td id=\"T_166a5_row8_col3\" class=\"data row8 col3\" >0.723788</td>\n",
       "      <td id=\"T_166a5_row8_col4\" class=\"data row8 col4\" >167084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row9\" class=\"row_heading level0 row9\" >11</th>\n",
       "      <td id=\"T_166a5_row9_col0\" class=\"data row9 col0\" >89.549079</td>\n",
       "      <td id=\"T_166a5_row9_col1\" class=\"data row9 col1\" >0.53</td>\n",
       "      <td id=\"T_166a5_row9_col2\" class=\"data row9 col2\" >0.747853</td>\n",
       "      <td id=\"T_166a5_row9_col3\" class=\"data row9 col3\" >0.723775</td>\n",
       "      <td id=\"T_166a5_row9_col4\" class=\"data row9 col4\" >167088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row10\" class=\"row_heading level0 row10\" >43</th>\n",
       "      <td id=\"T_166a5_row10_col0\" class=\"data row10 col0\" >85.979856</td>\n",
       "      <td id=\"T_166a5_row10_col1\" class=\"data row10 col1\" >0.51</td>\n",
       "      <td id=\"T_166a5_row10_col2\" class=\"data row10 col2\" >0.747856</td>\n",
       "      <td id=\"T_166a5_row10_col3\" class=\"data row10 col3\" >0.701147</td>\n",
       "      <td id=\"T_166a5_row10_col4\" class=\"data row10 col4\" >167254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row11\" class=\"row_heading level0 row11\" >18</th>\n",
       "      <td id=\"T_166a5_row11_col0\" class=\"data row11 col0\" >51.148449</td>\n",
       "      <td id=\"T_166a5_row11_col1\" class=\"data row11 col1\" >0.51</td>\n",
       "      <td id=\"T_166a5_row11_col2\" class=\"data row11 col2\" >0.747857</td>\n",
       "      <td id=\"T_166a5_row11_col3\" class=\"data row11 col3\" >0.701046</td>\n",
       "      <td id=\"T_166a5_row11_col4\" class=\"data row11 col4\" >167258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row12\" class=\"row_heading level0 row12\" >17</th>\n",
       "      <td id=\"T_166a5_row12_col0\" class=\"data row12 col0\" >155.737928</td>\n",
       "      <td id=\"T_166a5_row12_col1\" class=\"data row12 col1\" >0.55</td>\n",
       "      <td id=\"T_166a5_row12_col2\" class=\"data row12 col2\" >0.747852</td>\n",
       "      <td id=\"T_166a5_row12_col3\" class=\"data row12 col3\" >0.745856</td>\n",
       "      <td id=\"T_166a5_row12_col4\" class=\"data row12 col4\" >167261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row13\" class=\"row_heading level0 row13\" >39</th>\n",
       "      <td id=\"T_166a5_row13_col0\" class=\"data row13 col0\" >127.119228</td>\n",
       "      <td id=\"T_166a5_row13_col1\" class=\"data row13 col1\" >0.55</td>\n",
       "      <td id=\"T_166a5_row13_col2\" class=\"data row13 col2\" >0.747846</td>\n",
       "      <td id=\"T_166a5_row13_col3\" class=\"data row13 col3\" >0.745899</td>\n",
       "      <td id=\"T_166a5_row13_col4\" class=\"data row13 col4\" >167266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row14\" class=\"row_heading level0 row14\" >37</th>\n",
       "      <td id=\"T_166a5_row14_col0\" class=\"data row14 col0\" >119.047844</td>\n",
       "      <td id=\"T_166a5_row14_col1\" class=\"data row14 col1\" >0.51</td>\n",
       "      <td id=\"T_166a5_row14_col2\" class=\"data row14 col2\" >0.747843</td>\n",
       "      <td id=\"T_166a5_row14_col3\" class=\"data row14 col3\" >0.701131</td>\n",
       "      <td id=\"T_166a5_row14_col4\" class=\"data row14 col4\" >167277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row15\" class=\"row_heading level0 row15\" >48</th>\n",
       "      <td id=\"T_166a5_row15_col0\" class=\"data row15 col0\" >139.215797</td>\n",
       "      <td id=\"T_166a5_row15_col1\" class=\"data row15 col1\" >0.55</td>\n",
       "      <td id=\"T_166a5_row15_col2\" class=\"data row15 col2\" >0.747845</td>\n",
       "      <td id=\"T_166a5_row15_col3\" class=\"data row15 col3\" >0.745889</td>\n",
       "      <td id=\"T_166a5_row15_col4\" class=\"data row15 col4\" >167314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row16\" class=\"row_heading level0 row16\" >35</th>\n",
       "      <td id=\"T_166a5_row16_col0\" class=\"data row16 col0\" >148.702013</td>\n",
       "      <td id=\"T_166a5_row16_col1\" class=\"data row16 col1\" >0.55</td>\n",
       "      <td id=\"T_166a5_row16_col2\" class=\"data row16 col2\" >0.747845</td>\n",
       "      <td id=\"T_166a5_row16_col3\" class=\"data row16 col3\" >0.745876</td>\n",
       "      <td id=\"T_166a5_row16_col4\" class=\"data row16 col4\" >167318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row17\" class=\"row_heading level0 row17\" >2</th>\n",
       "      <td id=\"T_166a5_row17_col0\" class=\"data row17 col0\" >93.365760</td>\n",
       "      <td id=\"T_166a5_row17_col1\" class=\"data row17 col1\" >0.54</td>\n",
       "      <td id=\"T_166a5_row17_col2\" class=\"data row17 col2\" >0.747860</td>\n",
       "      <td id=\"T_166a5_row17_col3\" class=\"data row17 col3\" >0.735047</td>\n",
       "      <td id=\"T_166a5_row17_col4\" class=\"data row17 col4\" >167324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row18\" class=\"row_heading level0 row18\" >16</th>\n",
       "      <td id=\"T_166a5_row18_col0\" class=\"data row18 col0\" >83.037338</td>\n",
       "      <td id=\"T_166a5_row18_col1\" class=\"data row18 col1\" >0.5</td>\n",
       "      <td id=\"T_166a5_row18_col2\" class=\"data row18 col2\" >0.747862</td>\n",
       "      <td id=\"T_166a5_row18_col3\" class=\"data row18 col3\" >0.689211</td>\n",
       "      <td id=\"T_166a5_row18_col4\" class=\"data row18 col4\" >167330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row19\" class=\"row_heading level0 row19\" >32</th>\n",
       "      <td id=\"T_166a5_row19_col0\" class=\"data row19 col0\" >102.504110</td>\n",
       "      <td id=\"T_166a5_row19_col1\" class=\"data row19 col1\" >0.51</td>\n",
       "      <td id=\"T_166a5_row19_col2\" class=\"data row19 col2\" >0.747850</td>\n",
       "      <td id=\"T_166a5_row19_col3\" class=\"data row19 col3\" >0.701010</td>\n",
       "      <td id=\"T_166a5_row19_col4\" class=\"data row19 col4\" >167332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row20\" class=\"row_heading level0 row20\" >14</th>\n",
       "      <td id=\"T_166a5_row20_col0\" class=\"data row20 col0\" >57.867233</td>\n",
       "      <td id=\"T_166a5_row20_col1\" class=\"data row20 col1\" >0.54</td>\n",
       "      <td id=\"T_166a5_row20_col2\" class=\"data row20 col2\" >0.747861</td>\n",
       "      <td id=\"T_166a5_row20_col3\" class=\"data row20 col3\" >0.735096</td>\n",
       "      <td id=\"T_166a5_row20_col4\" class=\"data row20 col4\" >167345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row21\" class=\"row_heading level0 row21\" >29</th>\n",
       "      <td id=\"T_166a5_row21_col0\" class=\"data row21 col0\" >166.994487</td>\n",
       "      <td id=\"T_166a5_row21_col1\" class=\"data row21 col1\" >0.54</td>\n",
       "      <td id=\"T_166a5_row21_col2\" class=\"data row21 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row21_col3\" class=\"data row21 col3\" >0.735053</td>\n",
       "      <td id=\"T_166a5_row21_col4\" class=\"data row21 col4\" >167358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row22\" class=\"row_heading level0 row22\" >44</th>\n",
       "      <td id=\"T_166a5_row22_col0\" class=\"data row22 col0\" >79.201849</td>\n",
       "      <td id=\"T_166a5_row22_col1\" class=\"data row22 col1\" >0.54</td>\n",
       "      <td id=\"T_166a5_row22_col2\" class=\"data row22 col2\" >0.747853</td>\n",
       "      <td id=\"T_166a5_row22_col3\" class=\"data row22 col3\" >0.735037</td>\n",
       "      <td id=\"T_166a5_row22_col4\" class=\"data row22 col4\" >167372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row23\" class=\"row_heading level0 row23\" >47</th>\n",
       "      <td id=\"T_166a5_row23_col0\" class=\"data row23 col0\" >95.747400</td>\n",
       "      <td id=\"T_166a5_row23_col1\" class=\"data row23 col1\" >0.52</td>\n",
       "      <td id=\"T_166a5_row23_col2\" class=\"data row23 col2\" >0.747851</td>\n",
       "      <td id=\"T_166a5_row23_col3\" class=\"data row23 col3\" >0.712201</td>\n",
       "      <td id=\"T_166a5_row23_col4\" class=\"data row23 col4\" >167377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row24\" class=\"row_heading level0 row24\" >34</th>\n",
       "      <td id=\"T_166a5_row24_col0\" class=\"data row24 col0\" >112.246125</td>\n",
       "      <td id=\"T_166a5_row24_col1\" class=\"data row24 col1\" >0.54</td>\n",
       "      <td id=\"T_166a5_row24_col2\" class=\"data row24 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row24_col3\" class=\"data row24 col3\" >0.735053</td>\n",
       "      <td id=\"T_166a5_row24_col4\" class=\"data row24 col4\" >167394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row25\" class=\"row_heading level0 row25\" >40</th>\n",
       "      <td id=\"T_166a5_row25_col0\" class=\"data row25 col0\" >107.689738</td>\n",
       "      <td id=\"T_166a5_row25_col1\" class=\"data row25 col1\" >0.52</td>\n",
       "      <td id=\"T_166a5_row25_col2\" class=\"data row25 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row25_col3\" class=\"data row25 col3\" >0.712145</td>\n",
       "      <td id=\"T_166a5_row25_col4\" class=\"data row25 col4\" >167403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row26\" class=\"row_heading level0 row26\" >13</th>\n",
       "      <td id=\"T_166a5_row26_col0\" class=\"data row26 col0\" >101.440894</td>\n",
       "      <td id=\"T_166a5_row26_col1\" class=\"data row26 col1\" >0.52</td>\n",
       "      <td id=\"T_166a5_row26_col2\" class=\"data row26 col2\" >0.747860</td>\n",
       "      <td id=\"T_166a5_row26_col3\" class=\"data row26 col3\" >0.712168</td>\n",
       "      <td id=\"T_166a5_row26_col4\" class=\"data row26 col4\" >167405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row27\" class=\"row_heading level0 row27\" >24</th>\n",
       "      <td id=\"T_166a5_row27_col0\" class=\"data row27 col0\" >70.330154</td>\n",
       "      <td id=\"T_166a5_row27_col1\" class=\"data row27 col1\" >0.5</td>\n",
       "      <td id=\"T_166a5_row27_col2\" class=\"data row27 col2\" >0.747856</td>\n",
       "      <td id=\"T_166a5_row27_col3\" class=\"data row27 col3\" >0.689140</td>\n",
       "      <td id=\"T_166a5_row27_col4\" class=\"data row27 col4\" >167406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row28\" class=\"row_heading level0 row28\" >21</th>\n",
       "      <td id=\"T_166a5_row28_col0\" class=\"data row28 col0\" >52.983132</td>\n",
       "      <td id=\"T_166a5_row28_col1\" class=\"data row28 col1\" >0.52</td>\n",
       "      <td id=\"T_166a5_row28_col2\" class=\"data row28 col2\" >0.747854</td>\n",
       "      <td id=\"T_166a5_row28_col3\" class=\"data row28 col3\" >0.712210</td>\n",
       "      <td id=\"T_166a5_row28_col4\" class=\"data row28 col4\" >167410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row29\" class=\"row_heading level0 row29\" >6</th>\n",
       "      <td id=\"T_166a5_row29_col0\" class=\"data row29 col0\" >113.240549</td>\n",
       "      <td id=\"T_166a5_row29_col1\" class=\"data row29 col1\" >0.52</td>\n",
       "      <td id=\"T_166a5_row29_col2\" class=\"data row29 col2\" >0.747846</td>\n",
       "      <td id=\"T_166a5_row29_col3\" class=\"data row29 col3\" >0.712145</td>\n",
       "      <td id=\"T_166a5_row29_col4\" class=\"data row29 col4\" >167412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row30\" class=\"row_heading level0 row30\" >25</th>\n",
       "      <td id=\"T_166a5_row30_col0\" class=\"data row30 col0\" >199.483567</td>\n",
       "      <td id=\"T_166a5_row30_col1\" class=\"data row30 col1\" >0.56</td>\n",
       "      <td id=\"T_166a5_row30_col2\" class=\"data row30 col2\" >0.747846</td>\n",
       "      <td id=\"T_166a5_row30_col3\" class=\"data row30 col3\" >0.755996</td>\n",
       "      <td id=\"T_166a5_row30_col4\" class=\"data row30 col4\" >167476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row31\" class=\"row_heading level0 row31\" >20</th>\n",
       "      <td id=\"T_166a5_row31_col0\" class=\"data row31 col0\" >141.636946</td>\n",
       "      <td id=\"T_166a5_row31_col1\" class=\"data row31 col1\" >0.56</td>\n",
       "      <td id=\"T_166a5_row31_col2\" class=\"data row31 col2\" >0.747846</td>\n",
       "      <td id=\"T_166a5_row31_col3\" class=\"data row31 col3\" >0.756018</td>\n",
       "      <td id=\"T_166a5_row31_col4\" class=\"data row31 col4\" >167505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row32\" class=\"row_heading level0 row32\" >8</th>\n",
       "      <td id=\"T_166a5_row32_col0\" class=\"data row32 col0\" >66.387614</td>\n",
       "      <td id=\"T_166a5_row32_col1\" class=\"data row32 col1\" >0.56</td>\n",
       "      <td id=\"T_166a5_row32_col2\" class=\"data row32 col2\" >0.747852</td>\n",
       "      <td id=\"T_166a5_row32_col3\" class=\"data row32 col3\" >0.755960</td>\n",
       "      <td id=\"T_166a5_row32_col4\" class=\"data row32 col4\" >167568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row33\" class=\"row_heading level0 row33\" >7</th>\n",
       "      <td id=\"T_166a5_row33_col0\" class=\"data row33 col0\" >183.140296</td>\n",
       "      <td id=\"T_166a5_row33_col1\" class=\"data row33 col1\" >0.49</td>\n",
       "      <td id=\"T_166a5_row33_col2\" class=\"data row33 col2\" >0.747854</td>\n",
       "      <td id=\"T_166a5_row33_col3\" class=\"data row33 col3\" >0.677018</td>\n",
       "      <td id=\"T_166a5_row33_col4\" class=\"data row33 col4\" >168043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row34\" class=\"row_heading level0 row34\" >30</th>\n",
       "      <td id=\"T_166a5_row34_col0\" class=\"data row34 col0\" >121.909289</td>\n",
       "      <td id=\"T_166a5_row34_col1\" class=\"data row34 col1\" >0.49</td>\n",
       "      <td id=\"T_166a5_row34_col2\" class=\"data row34 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row34_col3\" class=\"data row34 col3\" >0.677041</td>\n",
       "      <td id=\"T_166a5_row34_col4\" class=\"data row34 col4\" >168054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row35\" class=\"row_heading level0 row35\" >45</th>\n",
       "      <td id=\"T_166a5_row35_col0\" class=\"data row35 col0\" >62.830747</td>\n",
       "      <td id=\"T_166a5_row35_col1\" class=\"data row35 col1\" >0.49</td>\n",
       "      <td id=\"T_166a5_row35_col2\" class=\"data row35 col2\" >0.747857</td>\n",
       "      <td id=\"T_166a5_row35_col3\" class=\"data row35 col3\" >0.677002</td>\n",
       "      <td id=\"T_166a5_row35_col4\" class=\"data row35 col4\" >168057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row36\" class=\"row_heading level0 row36\" >28</th>\n",
       "      <td id=\"T_166a5_row36_col0\" class=\"data row36 col0\" >123.972072</td>\n",
       "      <td id=\"T_166a5_row36_col1\" class=\"data row36 col1\" >0.57</td>\n",
       "      <td id=\"T_166a5_row36_col2\" class=\"data row36 col2\" >0.747845</td>\n",
       "      <td id=\"T_166a5_row36_col3\" class=\"data row36 col3\" >0.765930</td>\n",
       "      <td id=\"T_166a5_row36_col4\" class=\"data row36 col4\" >168294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row37\" class=\"row_heading level0 row37\" >38</th>\n",
       "      <td id=\"T_166a5_row37_col0\" class=\"data row37 col0\" >166.312393</td>\n",
       "      <td id=\"T_166a5_row37_col1\" class=\"data row37 col1\" >0.57</td>\n",
       "      <td id=\"T_166a5_row37_col2\" class=\"data row37 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row37_col3\" class=\"data row37 col3\" >0.765949</td>\n",
       "      <td id=\"T_166a5_row37_col4\" class=\"data row37 col4\" >168297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row38\" class=\"row_heading level0 row38\" >19</th>\n",
       "      <td id=\"T_166a5_row38_col0\" class=\"data row38 col0\" >106.044173</td>\n",
       "      <td id=\"T_166a5_row38_col1\" class=\"data row38 col1\" >0.48</td>\n",
       "      <td id=\"T_166a5_row38_col2\" class=\"data row38 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row38_col3\" class=\"data row38 col3\" >0.664712</td>\n",
       "      <td id=\"T_166a5_row38_col4\" class=\"data row38 col4\" >169070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row39\" class=\"row_heading level0 row39\" >33</th>\n",
       "      <td id=\"T_166a5_row39_col0\" class=\"data row39 col0\" >136.977968</td>\n",
       "      <td id=\"T_166a5_row39_col1\" class=\"data row39 col1\" >0.58</td>\n",
       "      <td id=\"T_166a5_row39_col2\" class=\"data row39 col2\" >0.747846</td>\n",
       "      <td id=\"T_166a5_row39_col3\" class=\"data row39 col3\" >0.775577</td>\n",
       "      <td id=\"T_166a5_row39_col4\" class=\"data row39 col4\" >169101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row40\" class=\"row_heading level0 row40\" >0</th>\n",
       "      <td id=\"T_166a5_row40_col0\" class=\"data row40 col0\" >176.144678</td>\n",
       "      <td id=\"T_166a5_row40_col1\" class=\"data row40 col1\" >0.58</td>\n",
       "      <td id=\"T_166a5_row40_col2\" class=\"data row40 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row40_col3\" class=\"data row40 col3\" >0.775577</td>\n",
       "      <td id=\"T_166a5_row40_col4\" class=\"data row40 col4\" >169128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row41\" class=\"row_heading level0 row41\" >1</th>\n",
       "      <td id=\"T_166a5_row41_col0\" class=\"data row41 col0\" >156.615783</td>\n",
       "      <td id=\"T_166a5_row41_col1\" class=\"data row41 col1\" >0.58</td>\n",
       "      <td id=\"T_166a5_row41_col2\" class=\"data row41 col2\" >0.747850</td>\n",
       "      <td id=\"T_166a5_row41_col3\" class=\"data row41 col3\" >0.775597</td>\n",
       "      <td id=\"T_166a5_row41_col4\" class=\"data row41 col4\" >169140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row42\" class=\"row_heading level0 row42\" >10</th>\n",
       "      <td id=\"T_166a5_row42_col0\" class=\"data row42 col0\" >147.069601</td>\n",
       "      <td id=\"T_166a5_row42_col1\" class=\"data row42 col1\" >0.47</td>\n",
       "      <td id=\"T_166a5_row42_col2\" class=\"data row42 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row42_col3\" class=\"data row42 col3\" >0.651969</td>\n",
       "      <td id=\"T_166a5_row42_col4\" class=\"data row42 col4\" >169826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row43\" class=\"row_heading level0 row43\" >9</th>\n",
       "      <td id=\"T_166a5_row43_col0\" class=\"data row43 col0\" >81.191053</td>\n",
       "      <td id=\"T_166a5_row43_col1\" class=\"data row43 col1\" >0.59</td>\n",
       "      <td id=\"T_166a5_row43_col2\" class=\"data row43 col2\" >0.747850</td>\n",
       "      <td id=\"T_166a5_row43_col3\" class=\"data row43 col3\" >0.785016</td>\n",
       "      <td id=\"T_166a5_row43_col4\" class=\"data row43 col4\" >170053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row44\" class=\"row_heading level0 row44\" >12</th>\n",
       "      <td id=\"T_166a5_row44_col0\" class=\"data row44 col0\" >133.378647</td>\n",
       "      <td id=\"T_166a5_row44_col1\" class=\"data row44 col1\" >0.46</td>\n",
       "      <td id=\"T_166a5_row44_col2\" class=\"data row44 col2\" >0.747850</td>\n",
       "      <td id=\"T_166a5_row44_col3\" class=\"data row44 col3\" >0.638832</td>\n",
       "      <td id=\"T_166a5_row44_col4\" class=\"data row44 col4\" >170883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row45\" class=\"row_heading level0 row45\" >3</th>\n",
       "      <td id=\"T_166a5_row45_col0\" class=\"data row45 col0\" >115.811429</td>\n",
       "      <td id=\"T_166a5_row45_col1\" class=\"data row45 col1\" >0.6</td>\n",
       "      <td id=\"T_166a5_row45_col2\" class=\"data row45 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row45_col3\" class=\"data row45 col3\" >0.793971</td>\n",
       "      <td id=\"T_166a5_row45_col4\" class=\"data row45 col4\" >171019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_166a5_row46_col0\" class=\"data row46 col0\" >114.575147</td>\n",
       "      <td id=\"T_166a5_row46_col1\" class=\"data row46 col1\" >0.6</td>\n",
       "      <td id=\"T_166a5_row46_col2\" class=\"data row46 col2\" >0.747847</td>\n",
       "      <td id=\"T_166a5_row46_col3\" class=\"data row46 col3\" >0.793974</td>\n",
       "      <td id=\"T_166a5_row46_col4\" class=\"data row46 col4\" >171045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row47\" class=\"row_heading level0 row47\" >26</th>\n",
       "      <td id=\"T_166a5_row47_col0\" class=\"data row47 col0\" >72.624602</td>\n",
       "      <td id=\"T_166a5_row47_col1\" class=\"data row47 col1\" >0.45</td>\n",
       "      <td id=\"T_166a5_row47_col2\" class=\"data row47 col2\" >0.747858</td>\n",
       "      <td id=\"T_166a5_row47_col3\" class=\"data row47 col3\" >0.625675</td>\n",
       "      <td id=\"T_166a5_row47_col4\" class=\"data row47 col4\" >171937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row48\" class=\"row_heading level0 row48\" >15</th>\n",
       "      <td id=\"T_166a5_row48_col0\" class=\"data row48 col0\" >130.608957</td>\n",
       "      <td id=\"T_166a5_row48_col1\" class=\"data row48 col1\" >0.43</td>\n",
       "      <td id=\"T_166a5_row48_col2\" class=\"data row48 col2\" >0.747848</td>\n",
       "      <td id=\"T_166a5_row48_col3\" class=\"data row48 col3\" >0.597996</td>\n",
       "      <td id=\"T_166a5_row48_col4\" class=\"data row48 col4\" >174663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_166a5_level0_row49\" class=\"row_heading level0 row49\" >4</th>\n",
       "      <td id=\"T_166a5_row49_col0\" class=\"data row49 col0\" >185.406278</td>\n",
       "      <td id=\"T_166a5_row49_col1\" class=\"data row49 col1\" >0.4</td>\n",
       "      <td id=\"T_166a5_row49_col2\" class=\"data row49 col2\" >0.747850</td>\n",
       "      <td id=\"T_166a5_row49_col3\" class=\"data row49 col3\" >0.553768</td>\n",
       "      <td id=\"T_166a5_row49_col4\" class=\"data row49 col4\" >179863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17b1c6674f0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_Dummy  = (styled_df_Dummy.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score'])\n",
    "                          .format({'Threshold': \"{:g}\"}))\n",
    "\n",
    "styled_df_Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a00c6-3035-400b-a83a-ee7ca348b4b3",
   "metadata": {},
   "source": [
    "## 2.2 Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91535fdc-0437-4ddd-b08f-5980a02bb65e",
   "metadata": {},
   "source": [
    "### 2.2.1 Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ff288b6-6b73-4b9a-8d31-f6ef6f65432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 3689.93 secondes\n",
      "{'C_val': 152.3230731177815, 'threshold': 0.52}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('Logistic_Regression')\n",
    "\n",
    "nb_0 = (application_train['TARGET'] == 0).sum()\n",
    "nb_1 = (application_train['TARGET'] == 1).sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='mean')  # Vous pouvez utiliser 'median' ou 'constant' selon votre besoin\n",
    "X = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X = imputer.fit_transform(X)\n",
    "y = application_train[\"TARGET\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "results = []\n",
    "\n",
    "total_combinations = 50 \n",
    "\n",
    "def objective(trial):\n",
    "    C_val = trial.suggest_float('C_val', 50, 200)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LogisticRegression(C=C_val, class_weight=class_weights, verbose=0, max_iter=1000)\n",
    "    \n",
    "    # Enregistrement du temps de début pour le fit\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    # Calculer le temps de fit\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    # Enregistrement du temps de début pour la prédiction\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    # Calculer le temps de prédiction\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "    \n",
    "    results.append({\n",
    "        \"C\": C_val,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost,\n",
    "        \"Fit Time\": elapsed_time_fit,  \n",
    "        \"Prediction Time\": elapsed_time_predict \n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"C\", C_val)\n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        \n",
    "        # Enregistrer les temps dans mlflow\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74f5500-489a-4e19-8613-5cca2638ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df_Log_Reg = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd0e9ce9-d45e-424c-b0ba-bb2ef94ad054",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df_Log_Reg .to_csv('results_Log_Reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23431128-cf88-4c93-9d15-64afce383ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3f12d_row0_col4, #T_3f12d_row0_col5, #T_3f12d_row0_col6, #T_3f12d_row1_col2, #T_3f12d_row1_col5, #T_3f12d_row1_col6, #T_3f12d_row2_col5, #T_3f12d_row2_col6, #T_3f12d_row3_col5, #T_3f12d_row3_col6, #T_3f12d_row4_col5, #T_3f12d_row4_col6, #T_3f12d_row5_col5, #T_3f12d_row5_col6, #T_3f12d_row6_col5, #T_3f12d_row6_col6, #T_3f12d_row7_col5, #T_3f12d_row7_col6, #T_3f12d_row8_col5, #T_3f12d_row8_col6, #T_3f12d_row9_col5, #T_3f12d_row9_col6, #T_3f12d_row10_col5, #T_3f12d_row10_col6, #T_3f12d_row11_col5, #T_3f12d_row11_col6, #T_3f12d_row12_col5, #T_3f12d_row12_col6, #T_3f12d_row13_col5, #T_3f12d_row13_col6, #T_3f12d_row14_col5, #T_3f12d_row14_col6, #T_3f12d_row15_col5, #T_3f12d_row15_col6, #T_3f12d_row16_col5, #T_3f12d_row16_col6, #T_3f12d_row17_col5, #T_3f12d_row17_col6, #T_3f12d_row18_col5, #T_3f12d_row18_col6, #T_3f12d_row19_col5, #T_3f12d_row19_col6, #T_3f12d_row20_col5, #T_3f12d_row20_col6, #T_3f12d_row21_col5, #T_3f12d_row21_col6, #T_3f12d_row22_col5, #T_3f12d_row22_col6, #T_3f12d_row23_col5, #T_3f12d_row23_col6, #T_3f12d_row24_col5, #T_3f12d_row24_col6, #T_3f12d_row25_col5, #T_3f12d_row25_col6, #T_3f12d_row26_col5, #T_3f12d_row26_col6, #T_3f12d_row27_col5, #T_3f12d_row27_col6, #T_3f12d_row28_col5, #T_3f12d_row28_col6, #T_3f12d_row29_col5, #T_3f12d_row29_col6, #T_3f12d_row30_col5, #T_3f12d_row30_col6, #T_3f12d_row31_col5, #T_3f12d_row31_col6, #T_3f12d_row32_col5, #T_3f12d_row32_col6, #T_3f12d_row33_col5, #T_3f12d_row33_col6, #T_3f12d_row34_col5, #T_3f12d_row34_col6, #T_3f12d_row35_col5, #T_3f12d_row35_col6, #T_3f12d_row36_col5, #T_3f12d_row36_col6, #T_3f12d_row37_col5, #T_3f12d_row37_col6, #T_3f12d_row38_col5, #T_3f12d_row38_col6, #T_3f12d_row39_col5, #T_3f12d_row39_col6, #T_3f12d_row40_col5, #T_3f12d_row40_col6, #T_3f12d_row41_col5, #T_3f12d_row41_col6, #T_3f12d_row42_col5, #T_3f12d_row42_col6, #T_3f12d_row43_col3, #T_3f12d_row43_col5, #T_3f12d_row43_col6, #T_3f12d_row44_col5, #T_3f12d_row44_col6, #T_3f12d_row45_col5, #T_3f12d_row45_col6, #T_3f12d_row46_col5, #T_3f12d_row46_col6, #T_3f12d_row47_col5, #T_3f12d_row47_col6, #T_3f12d_row48_col5, #T_3f12d_row48_col6, #T_3f12d_row49_col5, #T_3f12d_row49_col6 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3f12d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3f12d_level0_col0\" class=\"col_heading level0 col0\" >C</th>\n",
       "      <th id=\"T_3f12d_level0_col1\" class=\"col_heading level0 col1\" >Threshold</th>\n",
       "      <th id=\"T_3f12d_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_3f12d_level0_col3\" class=\"col_heading level0 col3\" >Accuracy</th>\n",
       "      <th id=\"T_3f12d_level0_col4\" class=\"col_heading level0 col4\" >Business Score</th>\n",
       "      <th id=\"T_3f12d_level0_col5\" class=\"col_heading level0 col5\" >Fit Time</th>\n",
       "      <th id=\"T_3f12d_level0_col6\" class=\"col_heading level0 col6\" >Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row0\" class=\"row_heading level0 row0\" >48</th>\n",
       "      <td id=\"T_3f12d_row0_col0\" class=\"data row0 col0\" >152.323073</td>\n",
       "      <td id=\"T_3f12d_row0_col1\" class=\"data row0 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row0_col2\" class=\"data row0 col2\" >0.740357</td>\n",
       "      <td id=\"T_3f12d_row0_col3\" class=\"data row0 col3\" >0.704669</td>\n",
       "      <td id=\"T_3f12d_row0_col4\" class=\"data row0 col4\" >169286</td>\n",
       "      <td id=\"T_3f12d_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row0_col6\" class=\"data row0 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row1\" class=\"row_heading level0 row1\" >20</th>\n",
       "      <td id=\"T_3f12d_row1_col0\" class=\"data row1 col0\" >180.553675</td>\n",
       "      <td id=\"T_3f12d_row1_col1\" class=\"data row1 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row1_col2\" class=\"data row1 col2\" >0.740396</td>\n",
       "      <td id=\"T_3f12d_row1_col3\" class=\"data row1 col3\" >0.704568</td>\n",
       "      <td id=\"T_3f12d_row1_col4\" class=\"data row1 col4\" >169308</td>\n",
       "      <td id=\"T_3f12d_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row1_col6\" class=\"data row1 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row2\" class=\"row_heading level0 row2\" >11</th>\n",
       "      <td id=\"T_3f12d_row2_col0\" class=\"data row2 col0\" >145.795856</td>\n",
       "      <td id=\"T_3f12d_row2_col1\" class=\"data row2 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row2_col2\" class=\"data row2 col2\" >0.740369</td>\n",
       "      <td id=\"T_3f12d_row2_col3\" class=\"data row2 col3\" >0.704513</td>\n",
       "      <td id=\"T_3f12d_row2_col4\" class=\"data row2 col4\" >169316</td>\n",
       "      <td id=\"T_3f12d_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row2_col6\" class=\"data row2 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row3\" class=\"row_heading level0 row3\" >38</th>\n",
       "      <td id=\"T_3f12d_row3_col0\" class=\"data row3 col0\" >101.555236</td>\n",
       "      <td id=\"T_3f12d_row3_col1\" class=\"data row3 col1\" >0.55</td>\n",
       "      <td id=\"T_3f12d_row3_col2\" class=\"data row3 col2\" >0.740386</td>\n",
       "      <td id=\"T_3f12d_row3_col3\" class=\"data row3 col3\" >0.739897</td>\n",
       "      <td id=\"T_3f12d_row3_col4\" class=\"data row3 col4\" >169335</td>\n",
       "      <td id=\"T_3f12d_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row3_col6\" class=\"data row3 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row4\" class=\"row_heading level0 row4\" >27</th>\n",
       "      <td id=\"T_3f12d_row4_col0\" class=\"data row4 col0\" >157.011641</td>\n",
       "      <td id=\"T_3f12d_row4_col1\" class=\"data row4 col1\" >0.54</td>\n",
       "      <td id=\"T_3f12d_row4_col2\" class=\"data row4 col2\" >0.740355</td>\n",
       "      <td id=\"T_3f12d_row4_col3\" class=\"data row4 col3\" >0.728335</td>\n",
       "      <td id=\"T_3f12d_row4_col4\" class=\"data row4 col4\" >169341</td>\n",
       "      <td id=\"T_3f12d_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row4_col6\" class=\"data row4 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row5\" class=\"row_heading level0 row5\" >42</th>\n",
       "      <td id=\"T_3f12d_row5_col0\" class=\"data row5 col0\" >125.633267</td>\n",
       "      <td id=\"T_3f12d_row5_col1\" class=\"data row5 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row5_col2\" class=\"data row5 col2\" >0.740367</td>\n",
       "      <td id=\"T_3f12d_row5_col3\" class=\"data row5 col3\" >0.704565</td>\n",
       "      <td id=\"T_3f12d_row5_col4\" class=\"data row5 col4\" >169345</td>\n",
       "      <td id=\"T_3f12d_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row5_col6\" class=\"data row5 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row6\" class=\"row_heading level0 row6\" >33</th>\n",
       "      <td id=\"T_3f12d_row6_col0\" class=\"data row6 col0\" >120.565459</td>\n",
       "      <td id=\"T_3f12d_row6_col1\" class=\"data row6 col1\" >0.53</td>\n",
       "      <td id=\"T_3f12d_row6_col2\" class=\"data row6 col2\" >0.740340</td>\n",
       "      <td id=\"T_3f12d_row6_col3\" class=\"data row6 col3\" >0.716364</td>\n",
       "      <td id=\"T_3f12d_row6_col4\" class=\"data row6 col4\" >169347</td>\n",
       "      <td id=\"T_3f12d_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row6_col6\" class=\"data row6 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row7\" class=\"row_heading level0 row7\" >12</th>\n",
       "      <td id=\"T_3f12d_row7_col0\" class=\"data row7 col0\" >162.173449</td>\n",
       "      <td id=\"T_3f12d_row7_col1\" class=\"data row7 col1\" >0.53</td>\n",
       "      <td id=\"T_3f12d_row7_col2\" class=\"data row7 col2\" >0.740357</td>\n",
       "      <td id=\"T_3f12d_row7_col3\" class=\"data row7 col3\" >0.716526</td>\n",
       "      <td id=\"T_3f12d_row7_col4\" class=\"data row7 col4\" >169351</td>\n",
       "      <td id=\"T_3f12d_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row7_col6\" class=\"data row7 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row8\" class=\"row_heading level0 row8\" >46</th>\n",
       "      <td id=\"T_3f12d_row8_col0\" class=\"data row8 col0\" >104.643313</td>\n",
       "      <td id=\"T_3f12d_row8_col1\" class=\"data row8 col1\" >0.54</td>\n",
       "      <td id=\"T_3f12d_row8_col2\" class=\"data row8 col2\" >0.740384</td>\n",
       "      <td id=\"T_3f12d_row8_col3\" class=\"data row8 col3\" >0.728413</td>\n",
       "      <td id=\"T_3f12d_row8_col4\" class=\"data row8 col4\" >169362</td>\n",
       "      <td id=\"T_3f12d_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row8_col6\" class=\"data row8 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row9\" class=\"row_heading level0 row9\" >41</th>\n",
       "      <td id=\"T_3f12d_row9_col0\" class=\"data row9 col0\" >112.149261</td>\n",
       "      <td id=\"T_3f12d_row9_col1\" class=\"data row9 col1\" >0.55</td>\n",
       "      <td id=\"T_3f12d_row9_col2\" class=\"data row9 col2\" >0.740387</td>\n",
       "      <td id=\"T_3f12d_row9_col3\" class=\"data row9 col3\" >0.739757</td>\n",
       "      <td id=\"T_3f12d_row9_col4\" class=\"data row9 col4\" >169369</td>\n",
       "      <td id=\"T_3f12d_row9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row9_col6\" class=\"data row9 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row10\" class=\"row_heading level0 row10\" >6</th>\n",
       "      <td id=\"T_3f12d_row10_col0\" class=\"data row10 col0\" >109.002043</td>\n",
       "      <td id=\"T_3f12d_row10_col1\" class=\"data row10 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row10_col2\" class=\"data row10 col2\" >0.740392</td>\n",
       "      <td id=\"T_3f12d_row10_col3\" class=\"data row10 col3\" >0.704552</td>\n",
       "      <td id=\"T_3f12d_row10_col4\" class=\"data row10 col4\" >169385</td>\n",
       "      <td id=\"T_3f12d_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row10_col6\" class=\"data row10 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row11\" class=\"row_heading level0 row11\" >44</th>\n",
       "      <td id=\"T_3f12d_row11_col0\" class=\"data row11 col0\" >68.948956</td>\n",
       "      <td id=\"T_3f12d_row11_col1\" class=\"data row11 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row11_col2\" class=\"data row11 col2\" >0.740343</td>\n",
       "      <td id=\"T_3f12d_row11_col3\" class=\"data row11 col3\" >0.704643</td>\n",
       "      <td id=\"T_3f12d_row11_col4\" class=\"data row11 col4\" >169420</td>\n",
       "      <td id=\"T_3f12d_row11_col5\" class=\"data row11 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row11_col6\" class=\"data row11 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row12\" class=\"row_heading level0 row12\" >24</th>\n",
       "      <td id=\"T_3f12d_row12_col0\" class=\"data row12 col0\" >179.077283</td>\n",
       "      <td id=\"T_3f12d_row12_col1\" class=\"data row12 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row12_col2\" class=\"data row12 col2\" >0.740354</td>\n",
       "      <td id=\"T_3f12d_row12_col3\" class=\"data row12 col3\" >0.704506</td>\n",
       "      <td id=\"T_3f12d_row12_col4\" class=\"data row12 col4\" >169444</td>\n",
       "      <td id=\"T_3f12d_row12_col5\" class=\"data row12 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row12_col6\" class=\"data row12 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row13\" class=\"row_heading level0 row13\" >10</th>\n",
       "      <td id=\"T_3f12d_row13_col0\" class=\"data row13 col0\" >148.975543</td>\n",
       "      <td id=\"T_3f12d_row13_col1\" class=\"data row13 col1\" >0.53</td>\n",
       "      <td id=\"T_3f12d_row13_col2\" class=\"data row13 col2\" >0.740363</td>\n",
       "      <td id=\"T_3f12d_row13_col3\" class=\"data row13 col3\" >0.716585</td>\n",
       "      <td id=\"T_3f12d_row13_col4\" class=\"data row13 col4\" >169450</td>\n",
       "      <td id=\"T_3f12d_row13_col5\" class=\"data row13 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row13_col6\" class=\"data row13 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row14\" class=\"row_heading level0 row14\" >18</th>\n",
       "      <td id=\"T_3f12d_row14_col0\" class=\"data row14 col0\" >142.404365</td>\n",
       "      <td id=\"T_3f12d_row14_col1\" class=\"data row14 col1\" >0.55</td>\n",
       "      <td id=\"T_3f12d_row14_col2\" class=\"data row14 col2\" >0.740364</td>\n",
       "      <td id=\"T_3f12d_row14_col3\" class=\"data row14 col3\" >0.739793</td>\n",
       "      <td id=\"T_3f12d_row14_col4\" class=\"data row14 col4\" >169457</td>\n",
       "      <td id=\"T_3f12d_row14_col5\" class=\"data row14 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row14_col6\" class=\"data row14 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row15\" class=\"row_heading level0 row15\" >17</th>\n",
       "      <td id=\"T_3f12d_row15_col0\" class=\"data row15 col0\" >169.850540</td>\n",
       "      <td id=\"T_3f12d_row15_col1\" class=\"data row15 col1\" >0.52</td>\n",
       "      <td id=\"T_3f12d_row15_col2\" class=\"data row15 col2\" >0.740350</td>\n",
       "      <td id=\"T_3f12d_row15_col3\" class=\"data row15 col3\" >0.704513</td>\n",
       "      <td id=\"T_3f12d_row15_col4\" class=\"data row15 col4\" >169460</td>\n",
       "      <td id=\"T_3f12d_row15_col5\" class=\"data row15 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row15_col6\" class=\"data row15 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row16\" class=\"row_heading level0 row16\" >37</th>\n",
       "      <td id=\"T_3f12d_row16_col0\" class=\"data row16 col0\" >116.567537</td>\n",
       "      <td id=\"T_3f12d_row16_col1\" class=\"data row16 col1\" >0.53</td>\n",
       "      <td id=\"T_3f12d_row16_col2\" class=\"data row16 col2\" >0.740365</td>\n",
       "      <td id=\"T_3f12d_row16_col3\" class=\"data row16 col3\" >0.716533</td>\n",
       "      <td id=\"T_3f12d_row16_col4\" class=\"data row16 col4\" >169466</td>\n",
       "      <td id=\"T_3f12d_row16_col5\" class=\"data row16 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row16_col6\" class=\"data row16 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row17\" class=\"row_heading level0 row17\" >22</th>\n",
       "      <td id=\"T_3f12d_row17_col0\" class=\"data row17 col0\" >183.023352</td>\n",
       "      <td id=\"T_3f12d_row17_col1\" class=\"data row17 col1\" >0.54</td>\n",
       "      <td id=\"T_3f12d_row17_col2\" class=\"data row17 col2\" >0.740351</td>\n",
       "      <td id=\"T_3f12d_row17_col3\" class=\"data row17 col3\" >0.728329</td>\n",
       "      <td id=\"T_3f12d_row17_col4\" class=\"data row17 col4\" >169487</td>\n",
       "      <td id=\"T_3f12d_row17_col5\" class=\"data row17 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row17_col6\" class=\"data row17 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row18\" class=\"row_heading level0 row18\" >31</th>\n",
       "      <td id=\"T_3f12d_row18_col0\" class=\"data row18 col0\" >161.118985</td>\n",
       "      <td id=\"T_3f12d_row18_col1\" class=\"data row18 col1\" >0.54</td>\n",
       "      <td id=\"T_3f12d_row18_col2\" class=\"data row18 col2\" >0.740326</td>\n",
       "      <td id=\"T_3f12d_row18_col3\" class=\"data row18 col3\" >0.728306</td>\n",
       "      <td id=\"T_3f12d_row18_col4\" class=\"data row18 col4\" >169494</td>\n",
       "      <td id=\"T_3f12d_row18_col5\" class=\"data row18 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row18_col6\" class=\"data row18 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row19\" class=\"row_heading level0 row19\" >34</th>\n",
       "      <td id=\"T_3f12d_row19_col0\" class=\"data row19 col0\" >121.396803</td>\n",
       "      <td id=\"T_3f12d_row19_col1\" class=\"data row19 col1\" >0.55</td>\n",
       "      <td id=\"T_3f12d_row19_col2\" class=\"data row19 col2\" >0.740368</td>\n",
       "      <td id=\"T_3f12d_row19_col3\" class=\"data row19 col3\" >0.739786</td>\n",
       "      <td id=\"T_3f12d_row19_col4\" class=\"data row19 col4\" >169495</td>\n",
       "      <td id=\"T_3f12d_row19_col5\" class=\"data row19 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row19_col6\" class=\"data row19 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row20\" class=\"row_heading level0 row20\" >13</th>\n",
       "      <td id=\"T_3f12d_row20_col0\" class=\"data row20 col0\" >167.006481</td>\n",
       "      <td id=\"T_3f12d_row20_col1\" class=\"data row20 col1\" >0.54</td>\n",
       "      <td id=\"T_3f12d_row20_col2\" class=\"data row20 col2\" >0.740357</td>\n",
       "      <td id=\"T_3f12d_row20_col3\" class=\"data row20 col3\" >0.728270</td>\n",
       "      <td id=\"T_3f12d_row20_col4\" class=\"data row20 col4\" >169514</td>\n",
       "      <td id=\"T_3f12d_row20_col5\" class=\"data row20 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row20_col6\" class=\"data row20 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_3f12d_row21_col0\" class=\"data row21 col0\" >185.925729</td>\n",
       "      <td id=\"T_3f12d_row21_col1\" class=\"data row21 col1\" >0.51</td>\n",
       "      <td id=\"T_3f12d_row21_col2\" class=\"data row21 col2\" >0.740360</td>\n",
       "      <td id=\"T_3f12d_row21_col3\" class=\"data row21 col3\" >0.691919</td>\n",
       "      <td id=\"T_3f12d_row21_col4\" class=\"data row21 col4\" >169801</td>\n",
       "      <td id=\"T_3f12d_row21_col5\" class=\"data row21 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row21_col6\" class=\"data row21 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row22\" class=\"row_heading level0 row22\" >36</th>\n",
       "      <td id=\"T_3f12d_row22_col0\" class=\"data row22 col0\" >85.307573</td>\n",
       "      <td id=\"T_3f12d_row22_col1\" class=\"data row22 col1\" >0.51</td>\n",
       "      <td id=\"T_3f12d_row22_col2\" class=\"data row22 col2\" >0.740324</td>\n",
       "      <td id=\"T_3f12d_row22_col3\" class=\"data row22 col3\" >0.691994</td>\n",
       "      <td id=\"T_3f12d_row22_col4\" class=\"data row22 col4\" >169805</td>\n",
       "      <td id=\"T_3f12d_row22_col5\" class=\"data row22 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row22_col6\" class=\"data row22 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row23\" class=\"row_heading level0 row23\" >32</th>\n",
       "      <td id=\"T_3f12d_row23_col0\" class=\"data row23 col0\" >140.845597</td>\n",
       "      <td id=\"T_3f12d_row23_col1\" class=\"data row23 col1\" >0.51</td>\n",
       "      <td id=\"T_3f12d_row23_col2\" class=\"data row23 col2\" >0.740359</td>\n",
       "      <td id=\"T_3f12d_row23_col3\" class=\"data row23 col3\" >0.692115</td>\n",
       "      <td id=\"T_3f12d_row23_col4\" class=\"data row23 col4\" >169858</td>\n",
       "      <td id=\"T_3f12d_row23_col5\" class=\"data row23 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row23_col6\" class=\"data row23 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row24\" class=\"row_heading level0 row24\" >7</th>\n",
       "      <td id=\"T_3f12d_row24_col0\" class=\"data row24 col0\" >198.301155</td>\n",
       "      <td id=\"T_3f12d_row24_col1\" class=\"data row24 col1\" >0.56</td>\n",
       "      <td id=\"T_3f12d_row24_col2\" class=\"data row24 col2\" >0.740365</td>\n",
       "      <td id=\"T_3f12d_row24_col3\" class=\"data row24 col3\" >0.750612</td>\n",
       "      <td id=\"T_3f12d_row24_col4\" class=\"data row24 col4\" >169931</td>\n",
       "      <td id=\"T_3f12d_row24_col5\" class=\"data row24 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row24_col6\" class=\"data row24 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row25\" class=\"row_heading level0 row25\" >39</th>\n",
       "      <td id=\"T_3f12d_row25_col0\" class=\"data row25 col0\" >94.927133</td>\n",
       "      <td id=\"T_3f12d_row25_col1\" class=\"data row25 col1\" >0.56</td>\n",
       "      <td id=\"T_3f12d_row25_col2\" class=\"data row25 col2\" >0.740352</td>\n",
       "      <td id=\"T_3f12d_row25_col3\" class=\"data row25 col3\" >0.750560</td>\n",
       "      <td id=\"T_3f12d_row25_col4\" class=\"data row25 col4\" >169947</td>\n",
       "      <td id=\"T_3f12d_row25_col5\" class=\"data row25 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row25_col6\" class=\"data row25 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row26\" class=\"row_heading level0 row26\" >23</th>\n",
       "      <td id=\"T_3f12d_row26_col0\" class=\"data row26 col0\" >131.157916</td>\n",
       "      <td id=\"T_3f12d_row26_col1\" class=\"data row26 col1\" >0.5</td>\n",
       "      <td id=\"T_3f12d_row26_col2\" class=\"data row26 col2\" >0.740368</td>\n",
       "      <td id=\"T_3f12d_row26_col3\" class=\"data row26 col3\" >0.679499</td>\n",
       "      <td id=\"T_3f12d_row26_col4\" class=\"data row26 col4\" >169972</td>\n",
       "      <td id=\"T_3f12d_row26_col5\" class=\"data row26 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row26_col6\" class=\"data row26 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row27\" class=\"row_heading level0 row27\" >14</th>\n",
       "      <td id=\"T_3f12d_row27_col0\" class=\"data row27 col0\" >169.762298</td>\n",
       "      <td id=\"T_3f12d_row27_col1\" class=\"data row27 col1\" >0.5</td>\n",
       "      <td id=\"T_3f12d_row27_col2\" class=\"data row27 col2\" >0.740358</td>\n",
       "      <td id=\"T_3f12d_row27_col3\" class=\"data row27 col3\" >0.679398</td>\n",
       "      <td id=\"T_3f12d_row27_col4\" class=\"data row27 col4\" >169985</td>\n",
       "      <td id=\"T_3f12d_row27_col5\" class=\"data row27 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row27_col6\" class=\"data row27 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row28\" class=\"row_heading level0 row28\" >26</th>\n",
       "      <td id=\"T_3f12d_row28_col0\" class=\"data row28 col0\" >135.197382</td>\n",
       "      <td id=\"T_3f12d_row28_col1\" class=\"data row28 col1\" >0.56</td>\n",
       "      <td id=\"T_3f12d_row28_col2\" class=\"data row28 col2\" >0.740355</td>\n",
       "      <td id=\"T_3f12d_row28_col3\" class=\"data row28 col3\" >0.750560</td>\n",
       "      <td id=\"T_3f12d_row28_col4\" class=\"data row28 col4\" >170046</td>\n",
       "      <td id=\"T_3f12d_row28_col5\" class=\"data row28 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row28_col6\" class=\"data row28 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row29\" class=\"row_heading level0 row29\" >45</th>\n",
       "      <td id=\"T_3f12d_row29_col0\" class=\"data row29 col0\" >126.281014</td>\n",
       "      <td id=\"T_3f12d_row29_col1\" class=\"data row29 col1\" >0.5</td>\n",
       "      <td id=\"T_3f12d_row29_col2\" class=\"data row29 col2\" >0.740349</td>\n",
       "      <td id=\"T_3f12d_row29_col3\" class=\"data row29 col3\" >0.679463</td>\n",
       "      <td id=\"T_3f12d_row29_col4\" class=\"data row29 col4\" >170100</td>\n",
       "      <td id=\"T_3f12d_row29_col5\" class=\"data row29 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row29_col6\" class=\"data row29 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row30\" class=\"row_heading level0 row30\" >49</th>\n",
       "      <td id=\"T_3f12d_row30_col0\" class=\"data row30 col0\" >152.877644</td>\n",
       "      <td id=\"T_3f12d_row30_col1\" class=\"data row30 col1\" >0.56</td>\n",
       "      <td id=\"T_3f12d_row30_col2\" class=\"data row30 col2\" >0.740340</td>\n",
       "      <td id=\"T_3f12d_row30_col3\" class=\"data row30 col3\" >0.750498</td>\n",
       "      <td id=\"T_3f12d_row30_col4\" class=\"data row30 col4\" >170128</td>\n",
       "      <td id=\"T_3f12d_row30_col5\" class=\"data row30 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row30_col6\" class=\"data row30 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row31\" class=\"row_heading level0 row31\" >35</th>\n",
       "      <td id=\"T_3f12d_row31_col0\" class=\"data row31 col0\" >97.077651</td>\n",
       "      <td id=\"T_3f12d_row31_col1\" class=\"data row31 col1\" >0.57</td>\n",
       "      <td id=\"T_3f12d_row31_col2\" class=\"data row31 col2\" >0.740361</td>\n",
       "      <td id=\"T_3f12d_row31_col3\" class=\"data row31 col3\" >0.761210</td>\n",
       "      <td id=\"T_3f12d_row31_col4\" class=\"data row31 col4\" >170698</td>\n",
       "      <td id=\"T_3f12d_row31_col5\" class=\"data row31 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row31_col6\" class=\"data row31 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row32\" class=\"row_heading level0 row32\" >47</th>\n",
       "      <td id=\"T_3f12d_row32_col0\" class=\"data row32 col0\" >138.510367</td>\n",
       "      <td id=\"T_3f12d_row32_col1\" class=\"data row32 col1\" >0.57</td>\n",
       "      <td id=\"T_3f12d_row32_col2\" class=\"data row32 col2\" >0.740330</td>\n",
       "      <td id=\"T_3f12d_row32_col3\" class=\"data row32 col3\" >0.761181</td>\n",
       "      <td id=\"T_3f12d_row32_col4\" class=\"data row32 col4\" >170788</td>\n",
       "      <td id=\"T_3f12d_row32_col5\" class=\"data row32 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row32_col6\" class=\"data row32 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row33\" class=\"row_heading level0 row33\" >4</th>\n",
       "      <td id=\"T_3f12d_row33_col0\" class=\"data row33 col0\" >108.439631</td>\n",
       "      <td id=\"T_3f12d_row33_col1\" class=\"data row33 col1\" >0.57</td>\n",
       "      <td id=\"T_3f12d_row33_col2\" class=\"data row33 col2\" >0.740352</td>\n",
       "      <td id=\"T_3f12d_row33_col3\" class=\"data row33 col3\" >0.761103</td>\n",
       "      <td id=\"T_3f12d_row33_col4\" class=\"data row33 col4\" >170830</td>\n",
       "      <td id=\"T_3f12d_row33_col5\" class=\"data row33 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row33_col6\" class=\"data row33 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row34\" class=\"row_heading level0 row34\" >29</th>\n",
       "      <td id=\"T_3f12d_row34_col0\" class=\"data row34 col0\" >176.295956</td>\n",
       "      <td id=\"T_3f12d_row34_col1\" class=\"data row34 col1\" >0.49</td>\n",
       "      <td id=\"T_3f12d_row34_col2\" class=\"data row34 col2\" >0.740328</td>\n",
       "      <td id=\"T_3f12d_row34_col3\" class=\"data row34 col3\" >0.666277</td>\n",
       "      <td id=\"T_3f12d_row34_col4\" class=\"data row34 col4\" >171019</td>\n",
       "      <td id=\"T_3f12d_row34_col5\" class=\"data row34 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row34_col6\" class=\"data row34 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row35\" class=\"row_heading level0 row35\" >0</th>\n",
       "      <td id=\"T_3f12d_row35_col0\" class=\"data row35 col0\" >110.184246</td>\n",
       "      <td id=\"T_3f12d_row35_col1\" class=\"data row35 col1\" >0.49</td>\n",
       "      <td id=\"T_3f12d_row35_col2\" class=\"data row35 col2\" >0.740344</td>\n",
       "      <td id=\"T_3f12d_row35_col3\" class=\"data row35 col3\" >0.666248</td>\n",
       "      <td id=\"T_3f12d_row35_col4\" class=\"data row35 col4\" >171028</td>\n",
       "      <td id=\"T_3f12d_row35_col5\" class=\"data row35 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row35_col6\" class=\"data row35 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row36\" class=\"row_heading level0 row36\" >43</th>\n",
       "      <td id=\"T_3f12d_row36_col0\" class=\"data row36 col0\" >100.292039</td>\n",
       "      <td id=\"T_3f12d_row36_col1\" class=\"data row36 col1\" >0.49</td>\n",
       "      <td id=\"T_3f12d_row36_col2\" class=\"data row36 col2\" >0.740353</td>\n",
       "      <td id=\"T_3f12d_row36_col3\" class=\"data row36 col3\" >0.666215</td>\n",
       "      <td id=\"T_3f12d_row36_col4\" class=\"data row36 col4\" >171029</td>\n",
       "      <td id=\"T_3f12d_row36_col5\" class=\"data row36 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row36_col6\" class=\"data row36 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row37\" class=\"row_heading level0 row37\" >40</th>\n",
       "      <td id=\"T_3f12d_row37_col0\" class=\"data row37 col0\" >191.625322</td>\n",
       "      <td id=\"T_3f12d_row37_col1\" class=\"data row37 col1\" >0.58</td>\n",
       "      <td id=\"T_3f12d_row37_col2\" class=\"data row37 col2\" >0.740360</td>\n",
       "      <td id=\"T_3f12d_row37_col3\" class=\"data row37 col3\" >0.771268</td>\n",
       "      <td id=\"T_3f12d_row37_col4\" class=\"data row37 col4\" >171622</td>\n",
       "      <td id=\"T_3f12d_row37_col5\" class=\"data row37 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row37_col6\" class=\"data row37 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row38\" class=\"row_heading level0 row38\" >8</th>\n",
       "      <td id=\"T_3f12d_row38_col0\" class=\"data row38 col0\" >130.733410</td>\n",
       "      <td id=\"T_3f12d_row38_col1\" class=\"data row38 col1\" >0.58</td>\n",
       "      <td id=\"T_3f12d_row38_col2\" class=\"data row38 col2\" >0.740380</td>\n",
       "      <td id=\"T_3f12d_row38_col3\" class=\"data row38 col3\" >0.771365</td>\n",
       "      <td id=\"T_3f12d_row38_col4\" class=\"data row38 col4\" >171628</td>\n",
       "      <td id=\"T_3f12d_row38_col5\" class=\"data row38 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row38_col6\" class=\"data row38 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row39\" class=\"row_heading level0 row39\" >28</th>\n",
       "      <td id=\"T_3f12d_row39_col0\" class=\"data row39 col0\" >118.960450</td>\n",
       "      <td id=\"T_3f12d_row39_col1\" class=\"data row39 col1\" >0.48</td>\n",
       "      <td id=\"T_3f12d_row39_col2\" class=\"data row39 col2\" >0.740385</td>\n",
       "      <td id=\"T_3f12d_row39_col3\" class=\"data row39 col3\" >0.652909</td>\n",
       "      <td id=\"T_3f12d_row39_col4\" class=\"data row39 col4\" >171697</td>\n",
       "      <td id=\"T_3f12d_row39_col5\" class=\"data row39 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row39_col6\" class=\"data row39 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row40\" class=\"row_heading level0 row40\" >5</th>\n",
       "      <td id=\"T_3f12d_row40_col0\" class=\"data row40 col0\" >60.165162</td>\n",
       "      <td id=\"T_3f12d_row40_col1\" class=\"data row40 col1\" >0.48</td>\n",
       "      <td id=\"T_3f12d_row40_col2\" class=\"data row40 col2\" >0.740372</td>\n",
       "      <td id=\"T_3f12d_row40_col3\" class=\"data row40 col3\" >0.652847</td>\n",
       "      <td id=\"T_3f12d_row40_col4\" class=\"data row40 col4\" >171779</td>\n",
       "      <td id=\"T_3f12d_row40_col5\" class=\"data row40 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row40_col6\" class=\"data row40 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row41\" class=\"row_heading level0 row41\" >25</th>\n",
       "      <td id=\"T_3f12d_row41_col0\" class=\"data row41 col0\" >157.441053</td>\n",
       "      <td id=\"T_3f12d_row41_col1\" class=\"data row41 col1\" >0.59</td>\n",
       "      <td id=\"T_3f12d_row41_col2\" class=\"data row41 col2\" >0.740350</td>\n",
       "      <td id=\"T_3f12d_row41_col3\" class=\"data row41 col3\" >0.781416</td>\n",
       "      <td id=\"T_3f12d_row41_col4\" class=\"data row41 col4\" >172374</td>\n",
       "      <td id=\"T_3f12d_row41_col5\" class=\"data row41 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row41_col6\" class=\"data row41 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row42\" class=\"row_heading level0 row42\" >19</th>\n",
       "      <td id=\"T_3f12d_row42_col0\" class=\"data row42 col0\" >157.979389</td>\n",
       "      <td id=\"T_3f12d_row42_col1\" class=\"data row42 col1\" >0.47</td>\n",
       "      <td id=\"T_3f12d_row42_col2\" class=\"data row42 col2\" >0.740328</td>\n",
       "      <td id=\"T_3f12d_row42_col3\" class=\"data row42 col3\" >0.639079</td>\n",
       "      <td id=\"T_3f12d_row42_col4\" class=\"data row42 col4\" >173066</td>\n",
       "      <td id=\"T_3f12d_row42_col5\" class=\"data row42 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row42_col6\" class=\"data row42 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row43\" class=\"row_heading level0 row43\" >15</th>\n",
       "      <td id=\"T_3f12d_row43_col0\" class=\"data row43 col0\" >146.349800</td>\n",
       "      <td id=\"T_3f12d_row43_col1\" class=\"data row43 col1\" >0.6</td>\n",
       "      <td id=\"T_3f12d_row43_col2\" class=\"data row43 col2\" >0.740368</td>\n",
       "      <td id=\"T_3f12d_row43_col3\" class=\"data row43 col3\" >0.791067</td>\n",
       "      <td id=\"T_3f12d_row43_col4\" class=\"data row43 col4\" >173513</td>\n",
       "      <td id=\"T_3f12d_row43_col5\" class=\"data row43 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row43_col6\" class=\"data row43 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row44\" class=\"row_heading level0 row44\" >16</th>\n",
       "      <td id=\"T_3f12d_row44_col0\" class=\"data row44 col0\" >194.850533</td>\n",
       "      <td id=\"T_3f12d_row44_col1\" class=\"data row44 col1\" >0.46</td>\n",
       "      <td id=\"T_3f12d_row44_col2\" class=\"data row44 col2\" >0.740354</td>\n",
       "      <td id=\"T_3f12d_row44_col3\" class=\"data row44 col3\" >0.625024</td>\n",
       "      <td id=\"T_3f12d_row44_col4\" class=\"data row44 col4\" >174108</td>\n",
       "      <td id=\"T_3f12d_row44_col5\" class=\"data row44 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row44_col6\" class=\"data row44 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row45\" class=\"row_heading level0 row45\" >3</th>\n",
       "      <td id=\"T_3f12d_row45_col0\" class=\"data row45 col0\" >68.711118</td>\n",
       "      <td id=\"T_3f12d_row45_col1\" class=\"data row45 col1\" >0.45</td>\n",
       "      <td id=\"T_3f12d_row45_col2\" class=\"data row45 col2\" >0.740374</td>\n",
       "      <td id=\"T_3f12d_row45_col3\" class=\"data row45 col3\" >0.610641</td>\n",
       "      <td id=\"T_3f12d_row45_col4\" class=\"data row45 col4\" >175296</td>\n",
       "      <td id=\"T_3f12d_row45_col5\" class=\"data row45 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row45_col6\" class=\"data row45 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row46\" class=\"row_heading level0 row46\" >30</th>\n",
       "      <td id=\"T_3f12d_row46_col0\" class=\"data row46 col0\" >151.228725</td>\n",
       "      <td id=\"T_3f12d_row46_col1\" class=\"data row46 col1\" >0.44</td>\n",
       "      <td id=\"T_3f12d_row46_col2\" class=\"data row46 col2\" >0.740344</td>\n",
       "      <td id=\"T_3f12d_row46_col3\" class=\"data row46 col3\" >0.596362</td>\n",
       "      <td id=\"T_3f12d_row46_col4\" class=\"data row46 col4\" >176713</td>\n",
       "      <td id=\"T_3f12d_row46_col5\" class=\"data row46 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row46_col6\" class=\"data row46 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row47\" class=\"row_heading level0 row47\" >2</th>\n",
       "      <td id=\"T_3f12d_row47_col0\" class=\"data row47 col0\" >95.031841</td>\n",
       "      <td id=\"T_3f12d_row47_col1\" class=\"data row47 col1\" >0.42</td>\n",
       "      <td id=\"T_3f12d_row47_col2\" class=\"data row47 col2\" >0.740383</td>\n",
       "      <td id=\"T_3f12d_row47_col3\" class=\"data row47 col3\" >0.567012</td>\n",
       "      <td id=\"T_3f12d_row47_col4\" class=\"data row47 col4\" >179736</td>\n",
       "      <td id=\"T_3f12d_row47_col5\" class=\"data row47 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row47_col6\" class=\"data row47 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row48\" class=\"row_heading level0 row48\" >9</th>\n",
       "      <td id=\"T_3f12d_row48_col0\" class=\"data row48 col0\" >54.988890</td>\n",
       "      <td id=\"T_3f12d_row48_col1\" class=\"data row48 col1\" >0.4</td>\n",
       "      <td id=\"T_3f12d_row48_col2\" class=\"data row48 col2\" >0.740386</td>\n",
       "      <td id=\"T_3f12d_row48_col3\" class=\"data row48 col3\" >0.536130</td>\n",
       "      <td id=\"T_3f12d_row48_col4\" class=\"data row48 col4\" >183464</td>\n",
       "      <td id=\"T_3f12d_row48_col5\" class=\"data row48 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row48_col6\" class=\"data row48 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row49\" class=\"row_heading level0 row49\" >1</th>\n",
       "      <td id=\"T_3f12d_row49_col0\" class=\"data row49 col0\" >81.286270</td>\n",
       "      <td id=\"T_3f12d_row49_col1\" class=\"data row49 col1\" >0.4</td>\n",
       "      <td id=\"T_3f12d_row49_col2\" class=\"data row49 col2\" >0.740333</td>\n",
       "      <td id=\"T_3f12d_row49_col3\" class=\"data row49 col3\" >0.536149</td>\n",
       "      <td id=\"T_3f12d_row49_col4\" class=\"data row49 col4\" >183557</td>\n",
       "      <td id=\"T_3f12d_row49_col5\" class=\"data row49 col5\" >0.000000</td>\n",
       "      <td id=\"T_3f12d_row49_col6\" class=\"data row49 col6\" >2.165263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x239181e6450>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_Log_Reg  = (styled_df_Log_Reg.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score', 'Fit Time', 'Prediction Time'])\n",
    "                          .format({'Threshold': \"{:g}\", 'Business Score': \"{:.0f}\"}))\n",
    "\n",
    "styled_df_Log_Reg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75aa1a-0df2-44ae-9b44-bca78c6655b7",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8989c6bd-5fb1-4cad-be9b-0318c0692177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_val': 152.3230731177815, 'threshold': 0.52}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cd4b708-c10b-4fac-a0d6-0bac79ca77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Feature  Importance\n",
      "4                                     AMT_GOODS_PRICE    0.568068\n",
      "2                                          AMT_CREDIT    0.528479\n",
      "617                             CC_AMT_INSTALMENT_VAR    0.503845\n",
      "622                                CC_AMT_PAYMENT_VAR    0.489154\n",
      "604                            CC_DAYS_INSTALMENT_MAX    0.380201\n",
      "..                                                ...         ...\n",
      "109                                 FLAG_OWN_REALTY_N    0.001683\n",
      "419       PREV_NAME_CONTRACT_STATUS_Unused offer_MEAN    0.001542\n",
      "424               PREV_CODE_REJECT_REASON_CLIENT_MEAN    0.001542\n",
      "295                           BURO_STATUS_4_MEAN_MEAN    0.001504\n",
      "458  PREV_NAME_GOODS_CATEGORY_House Construction_MEAN    0.000000\n",
      "\n",
      "[624 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Supposition que df_classification_imputed est déjà défini\n",
    "nb_0 = len(application_train[application_train[\"TARGET\"] == 0])\n",
    "nb_1 = len(application_train[application_train[\"TARGET\"] == 1])\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "# Enregistrer les noms des colonnes avant l'imputation\n",
    "column_names = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"]).columns\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='mean')  # Vous pouvez utiliser 'median' ou 'constant' selon votre besoin\n",
    "X = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X = imputer.fit_transform(X)\n",
    "y = application_train[\"TARGET\"]\n",
    "\n",
    "# Convertir le tableau NumPy en DataFrame pandas\n",
    "X = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Stockage des noms des colonnes pour utilisation ultérieure\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Remplacer 'C_val' par 'C'\n",
    "params = study.best_params.copy()\n",
    "params['C'] = params.pop('C_val')\n",
    "\n",
    "# Suppression de 'threshold'\n",
    "params.pop('threshold', None)\n",
    "\n",
    "# Instanciation du modèle\n",
    "clf = LogisticRegression(**params, class_weight=class_weights, max_iter=1000, verbose=1)\n",
    "\n",
    "# Validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    clf.fit(X[train], y.iloc[train])\n",
    "    # Pour la régression logistique, l'importance des caractéristiques est donnée par les coefficients\n",
    "    feature_importances += np.abs(clf.coef_[0])\n",
    "\n",
    "# Moyenne des importances de caractéristiques sur les plis\n",
    "feature_importances /= 5\n",
    "\n",
    "# Afficher les importances des caractéristiques\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "997b1e81-795b-4ca5-b692-4a0876cab5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importances_log_reg = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "df_feature_importances_log_reg = df_feature_importances_log_reg.sort_values(by='importance', ascending=False)\n",
    "df_feature_importances_log_reg[\"importance\"] = (df_feature_importances_log_reg[\"importance\"]/ df_feature_importances_log_reg[\"importance\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f668829c-b136-443b-8c22-779c027c2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importances_log_reg.to_csv('df_feature_importances_log_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02998162-ba50-4a7b-bc66-00452b8e1920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMT_GOODS_PRICE</td>\n",
       "      <td>1.938372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMT_CREDIT</td>\n",
       "      <td>1.803285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>CC_AMT_INSTALMENT_VAR</td>\n",
       "      <td>1.719231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>CC_AMT_PAYMENT_VAR</td>\n",
       "      <td>1.669102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>CC_DAYS_INSTALMENT_MAX</td>\n",
       "      <td>1.297328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>FLAG_OWN_REALTY_N</td>\n",
       "      <td>0.005744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>PREV_NAME_CONTRACT_STATUS_Unused offer_MEAN</td>\n",
       "      <td>0.005261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>PREV_CODE_REJECT_REASON_CLIENT_MEAN</td>\n",
       "      <td>0.005261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>BURO_STATUS_4_MEAN_MEAN</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>PREV_NAME_GOODS_CATEGORY_House Construction_MEAN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  importance\n",
       "4                                     AMT_GOODS_PRICE    1.938372\n",
       "2                                          AMT_CREDIT    1.803285\n",
       "617                             CC_AMT_INSTALMENT_VAR    1.719231\n",
       "622                                CC_AMT_PAYMENT_VAR    1.669102\n",
       "604                            CC_DAYS_INSTALMENT_MAX    1.297328\n",
       "..                                                ...         ...\n",
       "109                                 FLAG_OWN_REALTY_N    0.005744\n",
       "419       PREV_NAME_CONTRACT_STATUS_Unused offer_MEAN    0.005261\n",
       "424               PREV_CODE_REJECT_REASON_CLIENT_MEAN    0.005261\n",
       "295                           BURO_STATUS_4_MEAN_MEAN    0.005133\n",
       "458  PREV_NAME_GOODS_CATEGORY_House Construction_MEAN    0.000000\n",
       "\n",
       "[624 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importances_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2738e790-8bd1-464b-9870-9cf101cda054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pourcentage_importance_totale = 95\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "df_sorted = df_feature_importances_log_reg.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "df_sorted[\"cumulative_importance\"] = df_sorted[\"importance\"].cumsum()\n",
    "\n",
    "# Find the number of features needed to reach 99% of the total importance\n",
    "num_features = df_sorted[df_sorted[\"cumulative_importance\"] <= df_sorted[\"importance\"].sum() * Pourcentage_importance_totale/100].shape[0]\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1aae17e-4af9-4328-8f03-28c4af41c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDcAAAK+CAYAAAC2IvX9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeVxO6f8/8Ndd6W4vqSRFkkQlDWOZxshaSZYha5SSQcJYh5kh2XczJIy7MjPWbB9jDLJmHULGknWEoezKGun8/vC7z9dx33fdEYnX8/E4j4eu8z7XeZ9zn47OdV/numSCIAggIiIiIiIiIiqldEo6ASIiIiIiIiKit8HGDSIiIiIiIiIq1di4QURERERERESlGhs3iIiIiIiIiKhUY+MGEREREREREZVqbNwgIiIiIiIiolKNjRtEREREREREVKqxcYOIiIiIiIiISjU2bhARERF9AObOnYvffvutpNOgT1xKSgpiYmKQnZ1d0qkQERUJGzeIiIioSDIyMiCTyZCYmFjSqXw05s6di5iYGDRo0EDrbXbt2gWZTIZdu3YVSw6JiYmQyWTIyMgolvpIe46OjggNDS3pNHD58mW0a9cOpqamMDc3L+l03ppMJkN0dHRJp/HeFPd19KmdPyr92LhBREQfDOXDVWpqakmn8sbmz5/Ph34STZo0CevXry8w5vDhwxgzZgz++OMPVKtW7YPIiUo3R0dHyGQycTE2Nka9evXw66+/atzm+fPn6Ny5M0JDQ/Htt9++x2w/PtHR0ZDJZLh9+3ZJp1KoTZs2sQGDPhp6JZ0AERHRx2T+/PmwsrL6IL6FpZI3adIkdOzYEe3atdMYc+rUKaxZs6ZIvTYA4KuvvsKTJ0+gr69fLDn16NEDXbp0gVwuL1J99GGqXbs2hg4dCgDIzMzE4sWLERISgtzcXERERKjEnzp1Cl26dMGgQYPed6rvzJMnT6Cn9+k87pw9exY6OkX77nrTpk2IjY1V28DxqZ0/Kv14tRIRERWDx48fw8jIqKTToDeUl5eH/Pz8IjcUFIeiNoQ9ffoU+vr60NHRgYGBQbHloaurC11d3WKrj0pWxYoVERwcLP4cGhoKJycnzJ49W23jRu3atVG7du13ksujR49gbGz8TuouSHH+fpQGxd0w+amdPyr9+FoKERF90EJDQ2FiYoIrV66gdevWMDExQcWKFREbGwsAOHHiBJo2bQpjY2NUrlwZy5Ytk2yvfNUlJSUF33zzDcqVKwczMzP07NkT9+7dU9nf/Pnz4ebmBrlcDjs7O0RGRuL+/fuSGB8fH7i7u+PIkSP46quvYGRkhNGjR8PR0RGnTp3C7t27xe7gPj4+AIC7d+9i2LBh8PDwgImJCczMzODv74/jx49L6laOo7Bq1SpMnDgR9vb2MDAwQLNmzXDhwgWVfP/++2+0atUKZcuWhbGxMWrVqoWffvpJEnPmzBl07NgRlpaWMDAwQN26dbFhwwatzv/9+/cRGhoKc3NzWFhYICQkROV8FGU/z58/x7hx41CtWjUYGBigXLly+PLLL5GcnKxVLt9++y0cHR0hl8thb2+Pnj17il2/nz17hjFjxqBOnTowNzeHsbExGjVqhJ07d0rqUY4ZMmPGDMyZMwdVq1aFXC7H6dOnta4DAPLz8/HTTz/Bw8MDBgYGsLa2hp+fn/halUwmw6NHj7BkyRLxeni1IePatWsICwtD+fLlIZfL4ebmhvj4eMk+lNfDihUr8MMPP6BixYowMjJCTk6O2jE3zp8/jw4dOsDW1hYGBgawt7dHly5dxMEhC8pJ3ZgbgiBgwoQJsLe3h5GREZo0aYJTp06pvNuv7Ib/Ok3jePz1119o1KgRjI2NYWpqioCAAJw6dUoSk5WVhV69esHe3h5yuRwVKlRA27ZtCx0T5J9//hEf5A0MDGBra4uwsDDcuXNHjFm9ejVkMhl2796tsv3ChQshk8lw8uRJsUzb36HCrlFtaXu/KApra2u4urri4sWLkvL8/HzMmTMHbm5uMDAwQPny5fHNN9+o3B/z8/MRHR0NOzs78Vo4ffq0yrWg/Mx3796N/v37w8bGBvb29uL64vrsU1NT4evrCysrKxgaGqJKlSoICwuT1KNuzIhjx47B398fZmZmMDExQbNmzXDw4EFJjPIY9u3bhyFDhsDa2hrGxsZo3749bt26pe0pL9SOHTvEc2FhYYG2bdsiPT1dJW7Xrl2oW7cuDAwMULVqVSxcuFDt79zrn0Vh99vQ0FDx/9JXX2NSUnf+9u7di88//7zAXAoal0ldndrcC4m0wZ4bRET0wXvx4gX8/f3x1VdfYdq0aVi6dCkGDBgAY2NjfP/99+jevTu+/vprLFiwAD179kTDhg1RpUoVSR0DBgyAhYUFoqOjcfbsWcTFxeHy5cviAyLw8gFt3LhxaN68Ofr16yfGHT58GPv27UOZMmXE+u7cuQN/f3906dIFwcHBKF++PHx8fBAVFQUTExN8//33AIDy5csDAP7991+sX78eQUFBqFKlCm7cuIGFCxeicePGOH36NOzs7CT5TpkyBTo6Ohg2bBiys7Mxbdo0dO/eHX///bcYk5ycjNatW6NChQoYNGgQbG1tkZ6ejo0bN4pdy0+dOgVvb29UrFgR3333HYyNjbFq1Sq0a9cOa9asQfv27TWed0EQ0LZtW+zduxd9+/ZFjRo1sG7dOoSEhKjEaruf6OhoTJ48Gb1790a9evWQk5OD1NRUHD16FC1atNCYy8OHD9GoUSOkp6cjLCwMn332GW7fvo0NGzbgv//+g5WVFXJycrB48WJ07doVERERePDgARQKBXx9fXHo0CGVb6UTEhLw9OlT9OnTB3K5HJaWlkWqIzw8HImJifD390fv3r2Rl5eHPXv24ODBg6hbty5+++038Tj79OkDAKhatSoA4MaNG2jQoAFkMhkGDBgAa2tr/PXXXwgPD0dOTg4GDx4syXX8+PHQ19fHsGHDkJubq7aHybNnz+Dr64vc3FxERUXB1tYW165dw8aNG3H//n2Ym5sXmJM6Y8aMwYQJE9CqVSu0atUKR48eRcuWLfHs2TON2xTmt99+Q0hICHx9fTF16lQ8fvwYcXFx+PLLL3Hs2DE4OjoCADp06IBTp04hKioKjo6OuHnzJpKTk3HlyhUxRp3k5GT8+++/6NWrF2xtbXHq1CksWrQIp06dwsGDByGTyRAQEAATExOsWrUKjRs3lmy/cuVKuLm5wd3dHYD217Y216i2inq/0EZeXh7+++8/lC1bVlL+zTffIDExEb169cLAgQNx6dIlzJs3D8eOHZPc90aNGoVp06YhMDAQvr6+OH78OHx9ffH06VO1++vfvz+sra0xZswYPHr0CEDxffY3b95Ey5YtYW1tje+++w4WFhbIyMjA2rVrCzwHp06dQqNGjWBmZoYRI0agTJkyWLhwIXx8fLB7927Ur19fEh8VFYWyZcti7NixyMjIwJw5czBgwACsXLmyyOf/ddu2bYO/vz+cnJwQHR2NJ0+eYO7cufD29sbRo0fFc3Hs2DH4+fmhQoUKGDduHF68eIGYmBhYW1sXuo/C7rfffPMNrl+/juTkZK1majpx4oR43qOjo5GXl4exY8eK/8+9iaLeC4kKJBAREX0gEhISBADC4cOHxbKQkBABgDBp0iSx7N69e4KhoaEgk8mEFStWiOVnzpwRAAhjx45VqbNOnTrCs2fPxPJp06YJAIT//e9/giAIws2bNwV9fX2hZcuWwosXL8S4efPmCQCE+Ph4saxx48YCAGHBggUqx+Dm5iY0btxYpfzp06eSegVBEC5duiTI5XIhJiZGLNu5c6cAQKhRo4aQm5srlv/0008CAOHEiROCIAhCXl6eUKVKFaFy5crCvXv3JPXm5+eL/27WrJng4eEhPH36VLL+iy++EKpVq6aS56vWr18vABCmTZsmluXl5QmNGjUSAAgJCQlF3o+np6cQEBBQ4H7VGTNmjABAWLt2rco65fHm5eVJzpkgvLxWypcvL4SFhYllly5dEgAIZmZmws2bNyXx2taxY8cOAYAwcOBAjfkIgiAYGxsLISEhKjHh4eFChQoVhNu3b0vKu3TpIpibmwuPHz8WBOH/rgcnJyexTEm5bufOnYIgCMKxY8cEAEJSUpLK/l6lKSfl78qlS5cEQfi/34mAgADJMY0ePVoAIKlj7Nixgro/K1+v88GDB4KFhYUQEREhicvKyhLMzc3F8nv37gkAhOnTpxd4LOq8fp4EQRCWL18uABBSUlLEsq5duwo2NjZCXl6eWJaZmSno6OhIfie1vba1uUY1qVy5suR8anu/KKi+li1bCrdu3RJu3bolnDhxQujRo4cAQIiMjBTj9uzZIwAQli5dKtl+8+bNkvKsrCxBT09PaNeunSQuOjpa5VpQfuZffvml5NwW52e/bt06lf8r1Hn9/4N27doJ+vr6wsWLF8Wy69evC6ampsJXX32lcgzNmzeXfHbffvutoKurK9y/f7/A/Sp/H27duqUxpnbt2oKNjY1w584dsez48eOCjo6O0LNnT7EsMDBQMDIyEq5duyaWnT9/XtDT01P5nXv9OtLmfhsZGan2d1cQ1J8/AwMD4fLly2LZ6dOnBV1dXUkdynvsq/9HaKpT23shkTb4WgoREZUKvXv3Fv9tYWGB6tWrw9jYGJ06dRLLq1evDgsLC/z7778q2/fp00fS86Jfv37Q09PDpk2bALz8Fu3Zs2cYPHiwZEC2iIgImJmZ4c8//5TUJ5fL0atXL63zl8vlYr0vXrzAnTt3YGJigurVq+Po0aMq8b169ZJ8O9+oUSMAEI/t2LFjuHTpEgYPHgwLCwvJtsqeKHfv3sWOHTvQqVMnPHjwALdv38bt27dx584d+Pr64vz587h27ZrGnDdt2gQ9PT3069dPLNPV1UVUVJQkrij7sbCwwKlTp3D+/HltTx0AYM2aNfD09FTb00R5vLq6uuI5y8/Px927d5GXl4e6deuqPccdOnRQ+fZT2zrWrFkDmUyGsWPHasxHE0EQsGbNGgQGBkIQBPF83b59G76+vsjOzlbJNyQkBIaGhgXWq5y6c8uWLXj8+HGBsdpQ/k5ERUVJjultvklNTk7G/fv30bVrV8lx6+rqon79+uLrP4aGhtDX18euXbvUvj5WkFfP09OnT3H79m1xsNZXz2vnzp1x8+ZNyWs9q1evRn5+Pjp37gygaNe2Nteotop6v1Bn69atsLa2hrW1NTw8PPDbb7+hV69emD59uhiTlJQEc3NztGjRQvJ51KlTByYmJuLnsX37duTl5aF///6Sfbx+L3hVRESEZAyX4vzslfe8jRs34vnz51qdjxcvXmDr1q1o164dnJycxPIKFSqgW7du2Lt3L3JyciTb9OnTR/LZNWrUCC9evMDly5e12qcmmZmZSEtLQ2hoKCwtLcXyWrVqoUWLFuL/Sy9evMC2bdvQrl07SW8dZ2dn+Pv7F7qfN73fqvPixQts2bIF7dq1Q6VKlcTyGjVqwNfX943qfJN7IVFB2LhBREQfPOV4Bq8yNzeHvb29ykODubm52j+IX59i08TEBBUqVBDf4Vb+sVq9enVJnL6+PpycnFT+mK1YsWKRBp/Mz8/H7NmzUa1aNcjlclhZWcHa2hr//POPOB7Cq1794xGA2JVceWzK9+aVXefVuXDhAgRBwI8//ig+5CgX5UP5zZs3NW5/+fJlVKhQASYmJpLy189RUfYTExOD+/fvw8XFBR4eHhg+fDj++ecfjTkoXbx4scBjVVqyZAlq1aolvl9ubW2NP//8U+05fv3VpaLUcfHiRdjZ2UkeTLR169Yt3L9/H4sWLVI5X8oGs9c/F025vh4zZMgQLF68GFZWVvD19UVsbKzaY9eG8pp//XfH2tpa5dUGbSkfspo2bapy7Fu3bhWPWy6XY+rUqfjrr79Qvnx58ZW0rKysQvdx9+5dDBo0COXLl4ehoSGsra3F8/fqufDz84O5ubnkFYOVK1eidu3acHFxAVC0a1vba1QbRb1fqFO/fn0kJydj8+bNmDFjBiwsLHDv3j3Jfev8+fPIzs6GjY2NyvE9fPhQPDblteDs7CzZh6WlpcZr4fVrtjg/+8aNG6NDhw4YN24crKys0LZtWyQkJCA3N1fj+bh16xYeP36scv8CXj6g5+fn4+rVq5Lywu7Db0rT/zfKXG7fvo1Hjx7h5s2bePLkicp5B1Q/C3Xe9H6rzq1bt/DkyRO101WrOw5t6yzqvZCoIBxzg4iIPniaZnDQVC4IwrtMBwAK/Rb9dZMmTcKPP/6IsLAwjB8/HpaWltDR0cHgwYORn5+vEl8cx6asd9iwYRq/WdPmD+Ti3M9XX32Fixcv4n//+x+2bt2KxYsXY/bs2ViwYIGkd86b+P333xEaGop27dph+PDhsLGxga6uLiZPnqwyiCKg/jMsah1vQnm+goOD1Y5fArz8BrewXNWZOXMmQkNDxfM7cOBATJ48GQcPHpQM6ljcNPVMePHiheRn5bH/9ttvsLW1VYl/ddrJwYMHIzAwEOvXr8eWLVvw448/YvLkydixYwe8vLw05tKpUyfs378fw4cPR+3atWFiYoL8/Hz4+flJftfkcjnatWuHdevWYf78+bhx4wb27duHSZMmqeT7rn+HXlfU+4U6VlZWaN68OQDA19cXrq6uaN26NX766ScMGTIEwMvjs7GxwdKlS9XWoc24Dpq8fs0W52cvk8mwevVqHDx4EH/88Qe2bNmCsLAwzJw5EwcPHlRpkH1TJfl/THF4l/fbghT1flCUeyFRQdi4QUREn4Tz58+jSZMm4s8PHz5EZmYmWrVqBQCoXLkyAODs2bOSLsvPnj3DpUuXxIeEwmj6o2716tVo0qQJFAqFpPz+/ftFGmhQSTkI5MmTJzXmpjyOMmXKaJ3/qypXrozt27fj4cOHkoeFs2fPvtV+LC0t0atXL/Tq1QsPHz7EV199hejo6AL/2K5atapk9gp1Vq9eDScnJ6xdu1byOah7deRt66hatSq2bNmCu3fvFth7Q931YG1tDVNTU7x48eKNPpfCeHh4wMPDAz/88AP2798Pb29vLFiwABMmTNCYkzrK34nz589Lfidu3bql8s218hvt+/fvS16Ter3Hk/K6tbGx0erYq1atiqFDh2Lo0KE4f/48ateujZkzZ+L3339XG3/v3j1s374d48aNw5gxY8RyTd3yO3fujCVLlmD79u1IT0+HIAjiKylA0a5tba5RbRX3/QIAAgIC0LhxY0yaNAnffPMNjI2NUbVqVWzbtg3e3t4FNqApr4ULFy5IemTcuXNH614M7+Kzb9CgARo0aICJEydi2bJl6N69O1asWKH2XmJtbQ0jIyOV+xfwcjYcHR0dODg4aHUsb+vV/2/U5WJlZQVjY2MYGBjAwMBA7UxZ6srUKex+q+39wNraGoaGhmp/l14/jlfvB696/X7wru+F9OnhaylERPRJWLRokeTd7Li4OOTl5YnvLTdv3hz6+vr4+eefJd/KKRQKZGdnIyAgQKv9GBsbq50qVVdXV+XbvqSkpALHvCjIZ599hipVqmDOnDkq+1Pux8bGBj4+Pli4cCEyMzNV6ihsSsNWrVohLy8PcXFxYtmLFy8wd+5cSVxR9vPqdJzAy9eDnJ2dC+xODrwcH+P48eNYt26dyjrl8Sq/ZX31PP/99984cOBAgXW/Sts6OnToAEEQMG7cOI35AOqvB11dXXTo0AFr1qxR+zD8plNN5uTkIC8vT1Lm4eEBHR0dyfnVdI2+rnnz5ihTpgzmzp0rOaY5c+aoxCofXFNSUsQy5ZSzr/L19YWZmRkmTZqkdqwE5bE/fvxYZRaOqlWrwtTUtMBrRd3npyln4OUxWlpaYuXKlVi5ciXq1asneXgvyrWtzTWqreK+XyiNHDkSd+7cwS+//ALgZS+XFy9eYPz48SqxeXl54nXSrFkz6OnpSe4FADBv3jyt912cn/29e/dUzo9yJiNN14euri5atmyJ//3vf5IpZW/cuIFly5bhyy+/hJmZmdbH8zYqVKiA2rVrY8mSJZLfxZMnT2Lr1q1io7uuri6aN2+O9evX4/r162LchQsX8NdffxW6H23ut8bGxgBUGyJep6urC19fX6xfvx5XrlwRy9PT07FlyxZJrJmZGaysrCT3A+DlVOuv1/ku7oX06WLPDSIi+iQ8e/YMzZo1Q6dOnXD27FnMnz8fX375Jdq0aQPg5TdIo0aNwrhx4+Dn54c2bdqIcZ9//jmCg4O12k+dOnUQFxeHCRMmwNnZGTY2NmjatClat26NmJgY9OrVC1988QVOnDiBpUuXSr4RLwodHR3ExcUhMDAQtWvXRq9evVChQgWcOXMGp06dEv/YjI2NxZdffgkPDw9ERETAyckJN27cwIEDB/Dff//h+PHjGvcRGBgIb29vfPfdd8jIyEDNmjWxdu1ate/8a7ufmjVrwsfHB3Xq1IGlpSVSU1OxevVqDBgwoMDjHT58OFavXo2goCCEhYWhTp06uHv3LjZs2IAFCxbA09MTrVu3xtq1a9G+fXsEBATg0qVLWLBgAWrWrImHDx9qdV61raNJkybo0aMHfv75Z5w/f1585WHPnj1o0qSJeDx16tTBtm3bMGvWLNjZ2aFKlSqoX78+pkyZgp07d6J+/fqIiIhAzZo1cffuXRw9ehTbtm3D3bt3tcr3VTt27MCAAQMQFBQEFxcX5OXl4bfffhMfIJQ05fQ6a2trDBs2DJMnT0br1q3RqlUrHDt2DH/99ZdK74GWLVuiUqVKCA8Px/Dhw6Grq4v4+HhYW1tLHoTMzMwQFxeHHj164LPPPkOXLl3EmD///BPe3t6YN28ezp07J/6+1qxZE3p6eli3bh1u3LiBLl26aDwHZmZm4hgNz58/R8WKFbF161ZcunRJbXyZMmXw9ddfY8WKFXj06BFmzJihEqPtta3NNaqt4r5fKPn7+8Pd3R2zZs1CZGQkGjdujG+++QaTJ09GWloaWrZsiTJlyuD8+fNISkrCTz/9hI4dO6J8+fIYNGgQZs6ciTZt2sDPzw/Hjx8XrwVtvv0vzs9+yZIlmD9/Ptq3b4+qVaviwYMH+OWXX2BmZiY2DKgzYcIEJCcn48svv0T//v2hp6eHhQsXIjc3F9OmTXurc6vOrFmzYGRkJCnT0dHB6NGjMX36dPj7+6Nhw4YIDw8Xp4I1NzdHdHS0GB8dHY2tW7fC29sb/fr1w4sXLzBv3jy4u7sjLS2twP1rc7+tU6cOAGDgwIHw9fWFrq6uxt+xcePGYfPmzWjUqBH69++PvLw8zJ07F25ubipjefTu3RtTpkxB7969UbduXaSkpODcuXMqdb6LeyF9wt7fxCxEREQF0zQVrLGxsUps48aNBTc3N5XyypUrS6a+U9a5e/duoU+fPkLZsmUFExMToXv37pIp+JTmzZsnuLq6CmXKlBHKly8v9OvXT2WqVU37FoSX0xoGBAQIpqamAgBxWtinT58KQ4cOFSpUqCAYGhoK3t7ewoEDB4TGjRtLpo5VTu/5+nSemqbW27t3r9CiRQvB1NRUMDY2FmrVqiXMnTtXEnPx4kWhZ8+egq2trVCmTBmhYsWKQuvWrYXVq1erPYZX3blzR+jRo4dgZmYmmJubCz169BCnHH09F232M2HCBKFevXqChYWFYGhoKLi6ugoTJ06UTNNbUC4DBgwQKlasKOjr6wv29vZCSEiIOIVgfn6+MGnSJKFy5cqCXC4XvLy8hI0bNwohISFC5cqVVc6luqkmta1DEF5OGzt9+nTB1dVV0NfXF6ytrQV/f3/hyJEjYsyZM2eEr776SjA0NFSZMvPGjRtCZGSk4ODgIJQpU0awtbUVmjVrJixatEiM0XQ9vLpOORXsv//+K4SFhQlVq1YVDAwMBEtLS6FJkybCtm3bJNtpyun1aVsFQRBevHghjBs3TrxufXx8hJMnT6pMOSkIgnDkyBGhfv36gr6+vlCpUiVh1qxZautU5u7r6yuYm5sLBgYGQtWqVYXQ0FAhNTVVEARBuH37thAZGSm4uroKxsbGgrm5uVC/fn1h1apVKufhdf/995/Qvn17wcLCQjA3NxeCgoKE69evq0xBqZScnCwAEGQymXD16lW1dWr7O1TYNaqJuqlgtblfFFSfpilAExMTVX5/Fy1aJNSpU0cwNDQUTE1NBQ8PD2HEiBHC9evXxZi8vDzhxx9/FGxtbQVDQ0OhadOmQnp6ulCuXDmhb9++Ypy6+/iriuOzP3r0qNC1a1ehUqVKglwuF2xsbITWrVuLdSip+8yPHj0q+Pr6CiYmJoKRkZHQpEkTYf/+/ZIYTcfw+u+cJsqpYNUturq6Yty2bdsEb29vwdDQUDAzMxMCAwOF06dPq9S3fft2wcvLS9DX1xeqVq0qLF68WBg6dKhgYGAgiXv9OtLmfpuXlydERUUJ1tbWgkwmk0zpqu787d69W6hTp46gr68vODk5CQsWLFA7FfTjx4+F8PBwwdzcXDA1NRU6deok3Lx5U22d2twLibQhE4RSMiIOERHRG0hMTESvXr1w+PBh1K1bt6TTIfooODo6wsfHB4mJiSWdCpWg+/fvo2zZspgwYQK+//77kk7nk9KuXbtim+b1bUVHR2PcuHGlZqBV+nhxzA0iIiIiIirQkydPVMqUY5n4+Pi832Q+Ma+f+/Pnz2PTpk0870Sv4ZgbRERERERUoJUrVyIxMRGtWrWCiYkJ9u7di+XLl6Nly5bw9vYu6fQ+ak5OTggNDYWTkxMuX76MuLg46OvrY8SIESWdGtEHhY0bRERERERUoFq1akFPTw/Tpk1DTk6OOMiocophenf8/PywfPlyZGVlQS6Xo2HDhpg0aRKqVatW0qkRfVA45gYRERERERERlWocc4OIiIiIiIiISjU2bhARERERERFRqcYxN4jog5Cfn4/r16/D1NQUMpmspNMhIiIiIqISIggCHjx4ADs7O+joaNcng40bRPRBuH79OhwcHEo6DSIiIiIi+kBcvXoV9vb2WsWycYOIPgimpqYAXt7AzMzMSjgbIiIiIiIqKTk5OXBwcBCfEbTBxg0i+iAoX0UxMzNj4wYRERERERXpdXU2bhDRB8V97BboyI1KOg0iIiIiok9GxpSAkk7hrXG2FCIiIiIiIiIq1di4QURERERERESlGhs3iIiIiIiIiKhUY+MGEREREREREZVqbNwgIiIiIiIiolKNjRtEREREREREVKqxcYOIiIiIiIiICpSSkoLAwEDY2dlBJpNh/fr1hW4TGxuLGjVqwNDQENWrV8evv/6qMXbFihWQyWRo167dG+Wn90ZbEREREREREdEn49GjR/D09ERYWBi+/vrrQuPj4uIwatQo/PLLL/j8889x6NAhREREoGzZsggMDJTEZmRkYNiwYWjUqNEb58eeG1TsDhw4AF1dXQQEBEjKMzIyIJPJoKuri2vXrknWZWZmQk9PDzKZDBkZGYiOjoZMJitw0UZWVhYGDRoEZ2dnGBgYoHz58vD29kZcXBweP34sid2/fz9atWqFsmXLwsDAAB4eHpg1axZevHihUu/GjRvRuHFjmJqawsjICJ9//jkSExPVHq9yMTU1hZubGyIjI3H+/HlJ7IsXLzBlyhS4urrC0NAQlpaWqF+/PhYvXqzVcYaGhor70dfXh7OzM2JiYpCXlwcA2LVrlyQXa2trtGrVCidOnFCp5/WW0qysLERFRcHJyQlyuRwODg4IDAzE9u3bxRhHR0e1n9GUKVO0yp+IiIiIiD5s/v7+mDBhAtq3b69V/G+//YZvvvkGnTt3hpOTE7p06YI+ffpg6tSpkrgXL16ge/fuGDduHJycnN44PzZuULFTKBSIiopCSkoKrl+/rrK+YsWKKt2RlixZgooVK4o/Dxs2DJmZmeJib2+PmJgYSVlh/v33X3h5eWHr1q2YNGkSjh07hgMHDmDEiBHYuHEjtm3bJsauW7cOjRs3hr29PXbu3IkzZ85g0KBBmDBhArp06QJBEMTYuXPnom3btvD29sbff/+Nf/75B126dEHfvn0xbNgwlTy2bduGzMxMHD9+HJMmTUJ6ejo8PT0ljQPjxo3D7NmzMX78eJw+fRo7d+5Enz59cP/+/UKPU8nPzw+ZmZk4f/48hg4diujoaEyfPl0Sc/bsWWRmZmLLli3Izc1FQEAAnj17prHOjIwM1KlTBzt27MD06dNx4sQJbN68GU2aNEFkZKQk9vXPJzMzE1FRUVrnT0REREREH4/c3FwYGBhIygwNDXHo0CE8f/5cLIuJiYGNjQ3Cw8Pfan98LYWK1cOHD7Fy5UqkpqYiKysLiYmJGD16tCQmJCQECQkJGDVqlFiWkJCAkJAQjB8/HgBgYmICExMTcb2uri5MTU1ha2urdS79+/eHnp4eUlNTYWxsLJY7OTmhbdu2YoPFo0ePEBERgTZt2mDRokViXO/evVG+fHm0adMGq1atQufOnXH16lUMHToUgwcPxqRJk8TYoUOHQl9fHwMHDkRQUBDq168vritXrpyYt5OTEwIDA9GsWTOEh4fj4sWL0NXVxYYNG9C/f38EBQWJ23l6emp9rAAgl8vF/fTr1w/r1q3Dhg0bJOfZxsYGFhYWsLW1xeDBg9GmTRucOXMGtWrV0ngOZTIZDh06JDmHbm5uCAsLk8QW9fMhIiIiIqKPl6+vLxYvXox27drhs88+w5EjR7B48WI8f/4ct2/fRoUKFbB3714oFAqkpaW99f7Yc4OK1apVq+Dq6orq1asjODgY8fHxkl4PANCmTRvcu3cPe/fuBQDs3bsX9+7dU3nv6m3cuXMHW7duRWRkpOSh/FXKV1u2bt2KO3fuqO11ERgYCBcXFyxfvhwAsHr1ajx//lxt7DfffAMTExMxVhMdHR0MGjQIly9fxpEjRwAAtra22LFjB27dulWk4yyIoaGhxl4Z2dnZWLFiBQBAX19fbczdu3exefNmjefQwsLirfLLzc1FTk6OZCEiIiIioo/Djz/+CH9/fzRo0ABlypRB27ZtERISAuDlM9GDBw/Qo0cP/PLLL7Cysnrr/bFxg4qVQqFAcHAwgJevSWRnZ2P37t2SmDJlyogNHwAQHx+P4OBglClTptjyuHDhAgRBQPXq1SXlVlZWYq+QkSNHAgDOnTsHAKhRo4baulxdXcWYc+fOwdzcHBUqVFCJ09fXh5OTkxhbEFdXVwAvX/sAgFmzZuHWrVuwtbVFrVq10LdvX/z111/aHexrBEHAtm3bsGXLFjRt2lSyzt7eHiYmJrCwsMCyZcvQpk0bMZfXKc+hpvWvGzlypHhulcuePXs0xk+ePBnm5ubi4uDgoP1BEhERERHRB83Q0BDx8fF4/PgxMjIycOXKFTg6OsLU1BTW1ta4ePEiMjIyEBgYCD09Pejp6eHXX3/Fhg0bYGlpWeT9sXGDis3Zs2dx6NAhdO3aFQCgp6eHzp07Q6FQqMSGhYUhKSkJWVlZSEpKUnnF4V05dOgQ0tLS4ObmhtzcXMm613uYvEvKfSl7j9SsWRMnT57EwYMHERYWhps3byIwMBC9e/fWus6NGzfCxMQEBgYG8Pf3R+fOnREdHS2J2bNnD44cOYLExES4uLhgwYIFheaoreHDhyMtLU2y1K1bV2P8qFGjkJ2dLS5Xr14t0v6IiIiIiOjDV6ZMGdjb20NXVxcrVqxA69atoaOjA1dXV5w4cULy/NCmTRs0adJE7OVfFBxzg4qNQqFAXl4e7OzsxDJBECCXyzFv3jxJrIeHB1xdXdG1a1fUqFED7u7uxfKelZKzszNkMhnOnj0rKVeOvmtoaCiWubi4AADS09PxxRdfqNSVnp6OmjVrirHZ2dm4fv265DgB4NmzZ7h48SKaNGlSaH7p6ekAgCpVqohlOjo6+Pzzz/H5559j8ODB+P3339GjRw98//33kjhNmjRpgri4OOjr68POzg56eqq/3lWqVIGFhQWqV6+OmzdvonPnzkhJSVFbX7Vq1SCTyXDmzJlC9w287BXj7OysVSzwcowQuVyudTwREREREZWchw8f4sKFC+LPly5dQlpaGiwtLVGpUiWMGjUK165dEyePOHfuHA4dOoT69evj3r17mDVrFk6ePIklS5YAAAwMDODu7i7Zh/LVd+XzV1Gw5wYVi7y8PPz666+YOXOmpOXt+PHjsLOzUzsORVhYGHbt2vVOem2UK1cOLVq0wLx58/Do0aMCY1u2bAlLS0vMnDlTZd2GDRtw/vx5sTdKhw4dUKZMGbWxCxYswKNHj8RYTfLz8/Hzzz+jSpUq8PLy0hin/IUuLH8lY2NjODs7o1KlSmobNl4XGRmJkydPYt26dWrXW1pawtfXF7GxsWpzKMpMLkREREREVLqlpqbCy8tLfIYZMmQIvLy8MGbMGABAZmYmrly5Isa/ePECM2fOhKenJ1q0aIGnT59i//79cHR0fCf5secGFYuNGzfi3r17CA8Ph7m5uWRdhw4doFAo4OfnJymPiIhAUFDQWw9Mqcn8+fPh7e2NunXrIjo6GrVq1YKOjg4OHz6MM2fOoE6dOgBeNgosXLhQnHd5wIABMDMzw/bt2zF8+HB07NgRnTp1AgBUqlQJ06ZNw9ChQ2FgYIAePXqgTJky+N///ofRo0dj6NChkplSgJeDm2ZlZeHx48c4efIk5syZg0OHDuHPP/+Erq4uAKBjx47w9vbGF198AVtbW1y6dAmjRo2Ci4uL1mNeFJWRkREiIiIwduxYtGvXTnxF5lWxsbHw9vZGvXr1EBMTg1q1aiEvLw/JycmIi4sTe6AAwIMHD5CVlaWyDzMzs3eSPxERERERvT8+Pj4FvrqemJgo+blGjRo4duxYkfahrONNJhtgzw0qFgqFAs2bN1dp2ABeNm6kpqaqXKB6enqwsrLSqpfBm6hatSqOHTuG5s2bY9SoUfD09ETdunUxd+5cDBs2TJx2FnjZuLBz505cuXIFjRo1QvXq1TF79mx8//33WLFiheTBf/DgwVi3bh327NmDunXrwt3dHcuWLUNcXBxmzJihkkfz5s1RoUIFeHh44LvvvkONGjXwzz//SF5f8fX1xR9//CHOzhISEgJXV1ds3br1nZ0fABgwYADS09ORlJSkdr2TkxOOHj2KJk2aYOjQoXB3d0eLFi2wfft2xMXFSWLHjBmDChUqSJYRI0a8s9yJiIiIiIiUZML7HEWRiEiDnJycl7OmDF4FHblRSadDRERERPTJyJgSUNIpSCifDbKzs7XuCc6eG0RERERERERUqrFxg0qlK1euwMTEROPy6kA2pd2ndKxERERERERvggOKUqlkZ2dX4NSxr0/TWpp9SsdKRERERET0Jti4QaWSnp4enJ2dSzqN9+JTOlYiIiIiIqI3wddSiIiIiIiIiKhUY88NIvqgnBznq/WIyERERERERAB7bhARERERERFRKcfGDSIiIiIiIiIq1di4QURERERERESlGhs3iIiIiIiIiKhUY+MGEREREREREZVqnC2FiD4o7mO3QEduVNJpEBEREREVKGNKQEmnQK9gzw0iIiIiIiIiKtXYuEFEREREREREpRobN4iIiIiIiIioVGPjBhERERERERGVamzcICIiIiIiIqJSjY0bRERERERERFSqsXGDiIiIiIiIqJilpKQgMDAQdnZ2kMlkWL9+faHbLF26FJ6enjAyMkKFChUQFhaGO3fuSGKSkpLg6uoKAwMDeHh4YNOmTe/oCEoXNm4QERERERERFbNHjx7B09MTsbGxWsXv27cPPXv2RHh4OE6dOoWkpCQcOnQIERERYsz+/fvRtWtXhIeH49ixY2jXrh3atWuHkydPvqvDKDXYuEH0mgMHDkBXVxcBAQGS8oyMDMhkMujq6uLatWuSdZmZmdDT04NMJkNGRgaio6Mhk8kKXLSRlZWFqKgoODk5QS6Xw8HBAYGBgdi+fbsY4+joKNZpZGQEDw8PLF68WFLPrl27NOaRlZUFAJKc9fT0YGVlha+++gpz5sxBbm6upD4fHx8MHjxYPCcFLYmJidqeeiIiIiKij4a/vz8mTJiA9u3baxV/4MABODo6YuDAgahSpQq+/PJLfPPNNzh06JAY89NPP8HPzw/Dhw9HjRo1MH78eHz22WeYN2/euzqMUoONG0SvUSgUiIqKQkpKCq5fv66yvmLFivj1118lZUuWLEHFihXFn4cNG4bMzExxsbe3R0xMjKSsMBkZGahTpw527NiB6dOn48SJE9i8eTOaNGmCyMhISayy7pMnTyI4OBgRERH466+/VOo8e/asJIfMzEzY2NiI693c3JCZmYkrV65g586dCAoKwuTJk/HFF1/gwYMHKvU5ODhI6ho6dKhYh3Lp3LlzocdKRERERPSpa9iwIa5evYpNmzZBEATcuHEDq1evRqtWrcSYAwcOoHnz5pLtfH19ceDAgfed7gdHr6QTIPqQPHz4ECtXrkRqaiqysrKQmJiI0aNHS2JCQkKQkJCAUaNGiWUJCQkICQnB+PHjAQAmJiYwMTER1+vq6sLU1BS2trZa59K/f3/IZDIcOnQIxsbGYrmbmxvCwsIksa/WPXLkSEybNg3Jycnw9/eXxNnY2MDCwkLjPvX09MR67Ozs4OHhgRYtWsDT0xNTp07FhAkTJPG6urqSYzIxMZHUUZDc3FxJj5CcnJxCtyEiIiIi+lh5e3tj6dKl6Ny5M54+fYq8vDwEBgZKXmvJyspC+fLlJduVL19e7I39KWPPDaJXrFq1Cq6urqhevTqCg4MRHx8PQRAkMW3atMG9e/ewd+9eAMDevXtx7949BAYGFlsed+/exebNmxEZGSlp2FDS1ECRn5+PNWvW4N69e9DX1y+WXFxdXeHv74+1a9cWS31KkydPhrm5ubg4ODgUa/1ERERERKXJ6dOnMWjQIIwZMwZHjhzB5s2bkZGRgb59+5Z0aqUCGzeIXqFQKBAcHAwA8PPzQ3Z2Nnbv3i2JKVOmjNjwAQDx8fEIDg5GmTJlii2PCxcuQBAEuLq6ahU/cuRImJiYQC6Xo2PHjihbtix69+6tEmdvby/2KjExMYGbm5tW9bu6uiIjI6Moh1CoUaNGITs7W1yuXr1arPUTEREREZUmkydPhre3N4YPH45atWrB19cX8+fPR3x8vPhau62tLW7cuCHZ7saNG0XqIf6xYuMG0f939uxZHDp0CF27dgXw8hWNzp07Q6FQqMSGhYUhKSkJWVlZSEpKUnlN5G293lukMMOHD0daWhp27NiB+vXrY/bs2XB2dlaJ27NnD9LS0sRF22mjBEHQehBUbcnlcpiZmUkWIiIiIqJP1ePHj6GjI31E19XVBfB/zwcNGzaUTC4AAMnJyWjYsOH7SfIDxjE3iP4/hUKBvLw82NnZiWWCIEAul6uMPuzh4QFXV1d07doVNWrUgLu7O9LS0ootl2rVqkEmk+HMmTNaxVtZWcHZ2RnOzs5ISkqCh4cH6tati5o1a0riqlSpUuCYG5qkp6ejSpUqRd6OiIiIiOhT9fDhQ1y4cEH8+dKlS0hLS4OlpSUqVaqEUaNG4dq1a+JkBYGBgYiIiEBcXBx8fX2RmZmJwYMHo169euIzyqBBg9C4cWPMnDkTAQEBWLFiBVJTU7Fo0aISOcYPCXtuEAHIy8vDr7/+ipkzZ0p6Nhw/fhx2dnZYvny5yjZhYWHYtWtXsffaAABLS0v4+voiNjYWjx49Ull///59jds6ODigc+fOkgFP38aZM2ewefNmdOjQoVjqIyIiIiL6FKSmpsLLywteXl4AgCFDhsDLywtjxowBAHGWQqXQ0FDMmjUL8+bNg7u7O4KCglC9enXJ2HdffPEFli1bhkWLFsHT0xOrV6/G+vXr4e7u/n4P7gPEnhtEADZu3Ih79+4hPDwc5ubmknUdOnSAQqGAn5+fpDwiIgJBQUFv1BNCG7GxsfD29ka9evUQExODWrVqIS8vD8nJyYiLi0N6errGbQcNGgR3d3ekpqaibt26YvnNmzfx9OlTSWy5cuXE8ULy8vKQlZWF/Px83LlzB7t27cKECRNQu3ZtDB8+/J0cJxERERHRx8jHx6fA180TExNVyqKiohAVFVVgvUFBQQgKCnrb9D46bNwgwstXUpo3b67SsAG8bNyYNm2aylSlenp6sLKyemc5OTk54ejRo5g4cSKGDh2KzMxMWFtbo06dOoiLiytw25o1a6Jly5YYM2aMZFyN6tWrq8QeOHAADRo0AACcOnUKFSpUgK6uLszNzVGzZk2MGjUK/fr1g1wuL94DJCIiIiIiKiYyoagjFxIRvQM5OTkvp4QdvAo6cqOSToeIiIiIqEAZUwJKOoWPlvLZIDs7W+uJBzjmBhERERERERGVamzcICoBV65cgYmJicbl1YGFiIiIiIiIqGAcc4OoBNjZ2RU4deyr09ESERERERFRwdi4QVQC9PT04OzsXNJpEBERERERfRT4WgoRERERERERlWrsuUFEH5ST43y1HhGZiIiIiIgIYM8NIiIiIiIiIirl2LhBRERERERERKUaGzeIiIiIiIiIqFRj4wYRERERERERlWps3CAiIiIiIiKiUo2zpRDRB8V97BboyI1KOg0iIiIi+sBlTAko6RToA8KeG0RERERERERUqrFxg4iIiIiIiIhKNTZuEBEREREREVGpxsYNIiIiIiIiIirV2LhBRERERERERKUaGzeIiIiIiIiIqFRj4wYRERERERF9dFJSUhAYGAg7OzvIZDKsX7++wPjQ0FDIZDKVxc3NTYyZPHkyPv/8c5iamsLGxgbt2rXD2bNn3/GRkDbYuPGBycrKQlRUFJycnCCXy+Hg4IDAwEBs375djDl27BiCgoJQvnx5GBgYoFq1aoiIiMC5c+eKtC9fX1/o6uri8OHDKuuUv9h9+/ZVWRcZGQmZTIbQ0FAAUHsDeHWJjo4uMI+MjAzIZDKkpaVJfraxscGDBw8ksbVr15bUd+nSJXTr1g12dnYwMDCAvb092rZtizNnziAxMbHQ3DIyMgAABw4cgK6uLgICVOfKfj2/1yn3U6NGDZV1SUlJkMlkcHR0VIl/fTEwMBBjlOd/ypQpkvrWr18PmUwmidG0vLpPdTw8PNR+vgDw22+/QS6X4/bt22KZNteLTCZDmTJlUKVKFYwYMQJPnz4tMAciIiIionfl0aNH8PT0RGxsrFbxP/30EzIzM8Xl6tWrsLS0RFBQkBize/duREZG4uDBg0hOTsbz58/RsmVLPHr06F0dBmmJjRsfkIyMDNSpUwc7duzA9OnTceLECWzevBlNmjRBZGQkAGDjxo1o0KABcnNzsXTpUqSnp+P333+Hubk5fvzxR633deXKFezfvx8DBgxAfHy82hgHBwesWLECT548EcuePn2KZcuWoVKlSmLZqzeAOXPmwMzMTFI2bNiwNzofDx48wIwZMzSuf/78OVq0aIHs7GysXbsWZ8+excqVK+Hh4YH79++jc+fOkjwaNmyIiIgISZmDgwMAQKFQICoqCikpKbh+/XqRczU2NsbNmzdx4MABSblCoZCcK6XXz1FmZiYuX74siTEwMMDUqVNx7949tft8/eYLAAkJCeLP6hohXhUeHq7y+SolJCSgTZs2sLKyAqDd9eLn54fMzEz8+++/mD17NhYuXIixY8cWmAMRERER0bvi7++PCRMmoH379lrFm5ubw9bWVlxSU1Nx79499OrVS4zZvHkzQkND4ebmBk9PTyQmJuLKlSs4cuTIuzoM0pJeSSdA/6d///6QyWQ4dOgQjI2NxXI3NzeEhYXh8ePH6NWrF1q1aoV169aJ66tUqYL69evj/v37Wu8rISEBrVu3Rr9+/dCgQQPMmjULhoaGkpjPPvsMFy9exNq1a9G9e3cAwNq1a1GpUiVUqVJFjLO1tRX/bW5uDplMJil7U1FRUZg1axYiIyNhY2Ojsv7UqVO4ePEitm/fjsqVKwMAKleuDG9vbzHm1WPS19eHkZGRSm4PHz7EypUrkZqaiqysLCQmJmL06NFFylVPTw/dunVDfHw8GjZsCAD477//sGvXLnz77bdYvny5JF6bc9S8eXNcuHABkydPxrRp01TWm5ubw9zcXFJmYWGh9bkPDg7GyJEjsWbNGgQHB4vlly5dwq5du7Bp0yaxTJvrRS6Xi/t2cHBA8+bNkZycjKlTp2qVDxERERHRh0ShUKB58+bis4Y62dnZAABLS8v3lRZpwJ4bH4i7d+9i8+bNiIyMlDRsKFlYWGDLli24ffs2RowYobYOCwsLrfYlCAISEhIQHBwMV1dXODs7Y/Xq1Wpjw8LCkJCQIP4cHx8vabl8l7p27QpnZ2fExMSoXW9tbQ0dHR2sXr0aL168eOP9rFq1Cq6urqhevTqCg4MRHx8PQRCKXE9YWBhWrVqFx48fA3j5+omfnx/Kly//Rnnp6upi0qRJmDt3Lv777783qqMgVlZWaNu2rUpPjMTERNjb26Nly5YAina9KJ08eRL79++Hvr6+xpjc3Fzk5ORIFiIiIiKiD8H169fx119/oXfv3hpj8vPzMXjwYHh7e8Pd3f09ZkfqsHHjA3HhwgUIggBXV1eNMefPnweAAmO0sW3bNjx+/Bi+vr4AXn6Dr1Ao1MYGBwdj7969uHz5Mi5fvox9+/ZJvuV/l5RjTixatAgXL15UWV+xYkX8/PPPGDNmDMqWLYumTZti/Pjx+Pfff4u0H4VCIR6Tn58fsrOzsXv37iLn6+XlBScnJ6xevRqCICAxMRFhYWFqY7Ozs2FiYiJZ/P39VeLat2+P2rVrv7PXO8LDw7Fr1y5cunQJwMuGjCVLliAkJAQ6Oi9vD9peLxs3boSJiQkMDAzg4eGBmzdvYvjw4Rr3PXnyZLH3ibm5ufiKEBERERFRSVuyZAksLCzQrl07jTGRkZE4efIkVqxY8f4SI43YuPGB0KanwJv0JlAnPj4enTt3hp7ey7eSunbtin379qltQLC2tkZAQAASExORkJCAgIAAcRyG98HX1xdffvmlxvFEIiMjkZWVhaVLl6Jhw4ZISkqCm5sbkpOTtar/7NmzOHToELp27Qrg5eslnTt31tjYUxhlT5fdu3fj0aNHaNWqldo4U1NTpKWlSZbFixerjZ06dSqWLFmC9PT0N8qpIC1atIC9vb3YO2f79u24cuWKpHeOttdLkyZNkJaWhr///hshISHo1asXOnTooHHfo0aNQnZ2trhcvXq12I+PiIiIiKioBEFAfHw8evToobEn8oABA7Bx40bs3LkT9vb27zlDUoeNGx+IatWqQSaT4cyZMxpjXFxcAKDAmMLcvXsX69atw/z586Gnpwc9PT1UrFgReXl5GgeKDAsLQ2JiIpYsWaKxJ8K7NGXKFKxcuRLHjh1Tu97U1BSBgYGYOHEijh8/jkaNGmHChAla1a1QKJCXlwc7OzvxfMTFxWHNmjXi+3NF0b17dxw8eBDR0dHo0aOH2CDwOh0dHTg7O0uWihUrqo396quv4Ovri1GjRhU5n8Lo6OggNDQUS5YsQX5+PhISEtCkSRM4OTkBKNr1YmxsDGdnZ3h6eiI+Ph5///13gY1EcrkcZmZmkoWIiIiIqKTt3r0bFy5cQHh4uMo6QRAwYMAArFu3Djt27JCMRUgli40bHwhLS0v4+voiNjZW7TRC9+/fR8uWLWFlZaV2cEllTGGWLl0Ke3t7HD9+XNJrYObMmUhMTFQ7doWfnx+ePXuG58+fi68mvE/16tXD119/je+++67QWJlMBldXV62mYsrLy8Ovv/6KmTNnSs7F8ePHYWdnpzIIqDYsLS3Rpk0b7N69u1gbgqZMmYI//vhDZTaW4tCrVy9cvXoVa9euxbp16yQ38Te5XoCXjSajR4/GDz/8oHY2FiIiIiKid+3hw4fi36/Ay4Hz09LScOXKFQAvexL37NlTZTuFQoH69eurHUcjMjISv//+O5YtWwZTU1NkZWUhKyuLf/N+ANi48QGJjY3FixcvUK9ePaxZswbnz59Heno6fv75ZzRs2BDGxsZYvHgx/vzzT7Rp0wbbtm1DRkYGUlNTMWLECPTt27fQfSgUCnTs2BHu7u6SJTw8HLdv38bmzZtVttHV1UV6ejpOnz4NXV3dd3HohZo4cSJ27NiBs2fPimVpaWlo27YtVq9ejdOnT+PChQtQKBSIj49H27ZtC61z48aNuHfvHsLDw1XOR4cOHVR6HZw9e1blVZLnz5+r1JuYmIjbt28XODaKIAjijfDVJT8/X228h4cHunfvjp9//rnQ4yqqKlWqoGnTpujTpw/kcjm+/vprcd2bXC9KQUFB0NXV1XpecSIiIiKi4pSamgovLy94eXkBAIYMGQIvLy+MGTMGAJCZmSk2dChlZ2djzZo1anttAEBcXByys7Ph4+ODChUqiMvKlSvf7cFQoTgV7AfEyckJR48excSJEzF06FBkZmbC2toaderUQVxcHACgbdu22L9/PyZPnoxu3bohJycHDg4OaNq0aaGvYhw5cgTHjx/HL7/8orLO3NwczZo1g0KhQEBAgMr6kn5lwMXFBWFhYVi0aJFYZm9vD0dHR4wbNw4ZGRmQyWTiz99++22hdSqndnp9OlUA6NChA6ZNm4Z//vlHPPYuXbqoxKkbJ8LQ0FBlmtTX5eTkoEKFCirlmZmZGqdyjYmJeWc3zfDwcGzfvh39+/eHgYEBgLe7XoCX45cMGDAA06ZNQ79+/dTOAkRERERE9K74+PgUOG5hYmKiSpm5ubk4+6E6xTUOIhU/mcBPh4g+ADk5OS9nTRm8Cjpyo5JOh4iIiIg+cBlT1H/JRqWf8tkgOztb6y/a+VoKEREREREREZVqbNz4yPTt2xcmJiZqF23G5PhUcvoU7NmzR+N5NzExKen0iIiIiIiIig1fS/nI3Lx5Ezk5OWrXmZmZwcbG5j1n9GHm9Cl48uQJrl27pnG9s7Pze8ymcHwthYiIiIiKgq+lfLze5LUUDij6kbGxsfngGgs+xJw+BYaGhh9cAwYREREREdG7wNdSiIiIiIiIiKhUY+MGEREREREREZVqfC2FiD4oJ8f5av1eHREREREREcCeG0RERERERERUyrFxg4iIiIiIiIhKNTZuEBEREREREVGpxsYNIiIiIiIiIirV2LhBRERERERERKUaZ0shog+K+9gt0JEblXQaRERERPQOZEwJKOkU6CPFnhtEREREREREVKqxcYOIiIiIiIiISjU2bhARERERERFRqcbGDSIiIiIiIiIq1di4QURERERERESlGhs3iIiIiIiIiKhUY+MGERERERERfRBSUlIQGBgIOzs7yGQyrF+/vtBtcnNz8f3336Ny5cqQy+VwdHREfHy8uN7HxwcymUxlCQjgtLQfEzZufEKysrIQFRUFJycnyOVyODg4IDAwENu3bxdjjh07hqCgIJQvXx4GBgaoVq0aIiIicO7cuSLty9fXF7q6ujh8+LDKutDQUMhkMvTt21dlXWRkJGQyGUJDQwFA7U3o1SU6OrrAPDIyMiTx5cqVQ8uWLXHs2DFJ3IEDB6Crqyu5wf32228wNjbGhQsXJLHXr19H2bJlMW/ePACAo6MjZDIZVqxYobJ/Nzc3yGQyJCYmimXK+NeXKVOmSHK2sbHBgwcPJPXVrl0b0dHRKselbnl1n69bs2YNdHV1ce3aNbXrq1WrhiFDhhR4fpRez8XS0hKNGzfGnj17NO6fiIiIiEidR48ewdPTE7GxsVpv06lTJ2zfvh0KhQJnz57F8uXLUb16dXH92rVrkZmZKS4nT56Erq4ugoKC3sUhUAlh48YnIiMjA3Xq1MGOHTswffp0nDhxAps3b0aTJk0QGRkJANi4cSMaNGiA3NxcLF26FOnp6fj9999hbm6OH3/8Uet9XblyBfv378eAAQMkLaavcnBwwIoVK/DkyROx7OnTp1i2bBkqVaoklr16E5ozZw7MzMwkZcOGDdMqp23btiEzMxNbtmzBw4cP4e/vj/v374vrFQoFoqKikJKSguvXrwMAevToAV9fX4SGhiI/P1+MjYiIQJ06dcTzpjyehIQEyT4PHjyIrKwsGBsbq+QTExMjOY7MzExERUVJYh48eIAZM2aoPR4HBwfJtkOHDoWbm5ukrHPnzhrPR5s2bVCuXDksWbJEZV1KSgouXLiA8PDwAs/P65TnOCUlBXZ2dmjdujVu3LihMQciIiIiotf5+/tjwoQJaN++vVbxmzdvxu7du7Fp0yY0b94cjo6OaNiwIby9vcUYS0tL2NraiktycjKMjIzYuPGRYePGJ6J///6QyWQ4dOgQOnToABcXF7i5uWHIkCE4ePAgHj9+jF69eqFVq1bYsGEDmjdvjipVqqB+/fqYMWMGFi5cqPW+EhIS0Lp1a/Tr1w/Lly+XNGAoffbZZ3BwcMDatWvFsrVr16JSpUrw8vISy169CZmbm0Mmk0nKTExMtMqpXLlysLW1Rd26dTFjxgzcuHEDf//9NwDg4cOHWLlyJfr164eAgABJj4eFCxfi3LlzmDVrFgAgMTER+/btQ0JCAmQymRjXvXt37N69G1evXhXL4uPj0b17d+jp6ankY2pqKjkOW1tblUaQqKgozJo1Czdv3lTZXldXV+U86OnpScoMDQ01no8yZcqgR48eant3xMfHo379+nBzcyv0/LxKeY7d3d0xevRo5OTkiOeYiIiIiOhd2LBhA+rWrYtp06ahYsWKcHFxwbBhw9Q+gygpFAp06dJF7ZeQVHqxceMTcPfuXWzevBmRkZFqf4EtLCywZcsW3L59GyNGjFBbh4WFhVb7EgQBCQkJCA4OhqurK5ydnbF69Wq1sWFhYZLeDvHx8ejVq5dW+3kbyof+Z8+eAQBWrVoFV1dXVK9eHcHBwYiPj4cgCAAAa2trLFq0CD/++COSk5Px7bff4qeffoKDg4OkzvLly8PX11fsCfH48WOsXLkSYWFhb5xn165d4ezsjJiYmDeuoyDh4eE4f/48UlJSxLKHDx9i9erVkl4bBZ0fdZ48eYJff/0VAKCvr68xLjc3Fzk5OZKFiIiIiKgo/v33X+zduxcnT57EunXrMGfOHKxevRr9+/dXG3/o0CGcPHkSvXv3fs+Z0rvGxo1PwIULFyAIAlxdXTXGnD9/HgAKjNHGtm3b8PjxY/j6+gIAgoODoVAo1MYGBwdj7969uHz5Mi5fvox9+/YhODj4rfZfmPv372P8+PEwMTFBvXr1ALxsuVXu18/PD9nZ2di9e7e4Tbt27dCpUyf4+fmhcePGCAkJUVt3WFgYEhMTIQgCVq9ejapVq6J27dpqY0eOHAkTExPJ8voYFcpxOBYtWoSLFy8Ww9FL1axZEw0aNJC8OrRq1SoIgoAuXbqIZYWdH6UvvvgCJiYmMDY2xowZM1CnTh00a9ZM4/4nT54Mc3NzcXm9wYiIiIiIqDD5+fmQyWRYunQp6tWrh1atWmHWrFlYsmSJ2t4bCoUCHh4e4rMAfTzYuPEJKOhb9qLEaCM+Ph6dO3cWX8Xo2rUr9u3bp/bh3NraWnzNISEhAQEBAbCysiqWPF6nfPAuW7Ysjh8/jpUrV6J8+fI4e/YsDh06hK5duwIA9PT00LlzZ5UGmR9//BH5+fn44YcfNO4jICAADx8+REpKCuLj4wvstTF8+HCkpaVJlrp166rE+fr64ssvvyzSmCdFERYWhtWrV4sDl8bHxyMoKAimpqYAoPX5AYCVK1fi2LFjWLNmDZydnZGYmIgyZcpo3PeoUaOQnZ0tLq++0kNEREREpI0KFSqgYsWKMDc3F8tq1KgBQRDw33//SWIfPXqEFStWSHop08dDdTAA+uhUq1YNMpkMZ86c0Rjj4uICADhz5gwaNmz4Rvu5e/cu1q1bh+fPnyMuLk4sf/HiBeLj4zFx4kSVbcLCwjBgwAAAKNKIyEW1cuVK1KxZE+XKlZO8YqNQKJCXlwc7OzuxTBAEyOVyzJs3T7xJKhtr1I2foaSnp4cePXpg7Nix+Pvvv7Fu3TqNsVZWVnB2dtYq9ylTpqBhw4YYPny4VvFF0aVLF3z77bdYtWoVvvrqK+zbtw+TJ08W12t7foCXg5xWq1YN1apVQ15eHtq3b4+TJ09CLper3bdcLte4joiIiIhIG97e3khKSsLDhw/F8fjOnTsHHR0d2NvbS2KTkpKQm5v7znuLU8lgz41PgKWlJXx9fREbG4tHjx6prL9//z5atmwJKysrTJs2TW0dr84sosnSpUthb2+P48ePS3okzJw5E4mJiXjx4oXKNn5+fnj27BmeP38uvsryLjg4OKBq1aqSho28vDz8+uuvmDlzpiTf48ePw87ODsuXLy/yfsLCwrB79260bdsWZcuWLZbc69Wrh6+//hrfffddsdT3KlNTUwQFBSE+Ph4JCQlwcXFBo0aNALzd+enYsSP09PQwf/78Ys+ZiIiIiD5eDx8+FP/uBIBLly4hLS0NV65cAfCy92/Pnj3F+G7duqFcuXLo1asXTp8+jZSUFAwfPhxhYWEqA+wrFAq0a9cO5cqVe2/HQ+8Pe258ImJjY+Ht7Y169eohJiYGtWrVQl5eHpKTkxEXF4f09HQsXrwYQUFBaNOmDQYOHAhnZ2fcvn0bq1atwpUrV7BixYoC96FQKNCxY0e4u7tLyh0cHDBq1Chs3rwZAQEBknW6urpIT08X//0+bdy4Effu3UN4eLikBwIAdOjQAQqFAn379i1SnTVq1MDt27dhZGRUYNyDBw+QlZUlKTMyMoKZmZna+IkTJ8LNza3AniNvKjw8HI0aNUJ6ejpGjhwplr/N+ZHJZBg4cCCio6PxzTffFHo+iIiIiIgAIDU1FU2aNBF/HjJkCAAgJCQEiYmJyMzMFBs6AMDExATJycmIiopC3bp1Ua5cOXTq1AkTJkyQ1Hv27Fns3bsXW7dufT8HQu8de258IpycnHD06FE0adIEQ4cOhbu7O1q0aIHt27eLr5C0bdsW+/fvR5kyZdCtWze4urqia9euyM7OVrk5vO7IkSM4fvw4OnTooLLO3NwczZo10ziwqJmZmcaH+ndJoVCgefPmKg/uwMuH99TUVPzzzz9FrrdcuXIFTsMKAGPGjEGFChUki6aZaoCXrw2FhYXh6dOnRc6nMF9++SWqV6+OnJwcSSv4256fkJAQPH/+HPPmzSv2nImIiIjo4+Tj4wNBEFSWxMREAEBiYiJ27dol2cbV1RXJycl4/Pgxrl69ipkzZ6r8PV69enUIgoAWLVq8pyOh900mFNdIkkREbyEnJ+flrCmDV0FHzp4eRERERB+jjCkBhQfRJ0/5bJCdna31F+HsuUFEREREREREpRobN0hrffv2hYmJidqlqGNTfMw5fSj8/f01nptJkyaVdHpERERERETFhq+lkNZu3ryJnJwctevMzMxgY2PznjP6MHP6UFy7dg1PnjxRu87S0hKWlpbvOaOC8bUUIiIioo8fX0shbbzJaymcLYW0ZmNj88E1FnyIOX0oKlasWNIpEBERERERvRd8LYWIiIiIiIiISjX23CCiD8rJcb4lMjUwERERERGVXuy5QURERERERESlGhs3iIiIiIiIiKhUY+MGEREREREREZVqbNwgIiIiIiIiolKNjRtEREREREREVKqxcYOIiIiIiIiISjVOBUtEHxT3sVugIzcq6TSIiIioEBlTAko6BSIiEXtuEBEREREREVGpxsYNIiIiIiIiIirV2LhBRERERERERKUaGzeIiIiIiIiIqFRj4wYRERERERERlWps3CAiIiIiomKVkpKCwMBA2NnZQSaTYf369QXG7927F97e3ihXrhwMDQ3h6uqK2bNnq8TFxsbC0dERBgYGqF+/Pg4dOvSOjoCIShs2bhARERERUbF69OgRPD09ERsbq1W8sbExBgwYgJSUFKSnp+OHH37ADz/8gEWLFokxK1euxJAhQzB27FgcPXoUnp6e8PX1xc2bN9/VYRBRKcLGjfckKysLUVFRcHJyglwuh4ODAwIDA7F9+3Yx5tixYwgKCkL58uVhYGCAatWqISIiAufOnSu0/oyMDMhkMnExNTWFm5sbIiMjcf78ebXbHDhwALq6uggI+L85yn/77TcYGxvjwoULktjr16+jbNmymDdvHgDg+PHjaNOmDWxsbGBgYABHR0d07txZq/9clLmmpaVJfraxscGDBw8ksbVr10Z0dLT486VLl9CtWzfY2dnBwMAA9vb2aNu2Lc6cOYPExETJOVC3ZGRkaDx2Tfm9TrmfGjVqqKxLSkqCTCaDo6OjSvzri4GBgRgTGhoKmUyGKVOmSOpbv349ZDKZJEbT8uo+NfHx8VG7HwAICAiATCaTnG+l5cuXQ1dXF5GRkSrr4uLiYGFhgatXr0rKo6Ki4OLigsePHxeaFxEREX1c/P39MWHCBLRv316reC8vL3Tt2hVubm5wdHREcHAwfH19sWfPHjFm1qxZiIiIQK9evVCzZk0sWLAARkZGiI+Pf1eHQUSlCBs33oOMjAzUqVMHO3bswPTp03HixAls3rwZTZo0ER8WN27ciAYNGiA3NxdLly5Feno6fv/9d5ibm+PHH3/Uel/btm1DZmYmjh8/jkmTJiE9PR2enp6SRhQlhUKBqKgopKSk4Pr16wCAHj16wNfXF6GhocjPzxdjIyIiUKdOHURGRuLWrVto1qwZLC0tsWXLFqSnpyMhIQF2dnZ49OjRG5+nBw8eYMaMGRrXP3/+HC1atEB2djbWrl2Ls2fPYuXKlfDw8MD9+/fRuXNnZGZmikvDhg0REREhKXNwcNB47EVhbGyMmzdv4sCBA5JyhUKBSpUqqcSbmZlJ8sjMzMTly5clMQYGBpg6dSru3bundp8//fSTZHsASEhIEH8+fPiwVrk7ODggMTFRUnbt2jVs374dFSpUULuNQqHAiBEjsHz5cjx9+lSyrm/fvqhXrx7Cw8PFsu3btyMuLg6JiYkwMjLSKi8iIiIipWPHjmH//v1o3LgxAODZs2c4cuQImjdvLsbo6OigefPmKn+PEdGnSa+kE/gU9O/fHzKZDIcOHYKxsbFY7ubmhrCwMDx+/Bi9evVCq1atsG7dOnF9lSpVUL9+fdy/f1/rfZUrVw62trYAACcnJwQGBqJZs2YIDw/HxYsXoaurCwB4+PAhVq5cidTUVGRlZSExMRGjR48GACxcuBBubm6YNWsWhg0bhsTEROzbtw8nTpyATCbDvn37kJ2djcWLF0NPT0/MtUmTJm91nqKiojBr1ixERkbCxsZGZf2pU6dw8eJFbN++HZUrVwYAVK5cGd7e3mKMoaGh+G99fX0YGRmJ50OpoGPXlp6eHrp164b4+Hg0bNgQAPDff/9h165d+Pbbb7F8+XJJvEwmU8njdc2bN8eFCxcwefJkTJs2TWW9ubk5zM3NJWUWFhaF1vu61q1bY9WqVdi3b5947pYsWYKWLVviypUrKvGXLl3C/v37sWbNGuzcuRNr165Ft27dJMemUCjg7u6OBQsWoFu3bggLC8OQIUPwxRdfFCk3IiIi+rTZ29vj1q1byMvLQ3R0NHr37g0AuH37Nl68eIHy5ctL4suXL48zZ86URKpE9IFhz4137O7du9i8eTMiIyMlDRtKFhYW2LJlC27fvo0RI0aorcPCwuKN96+jo4NBgwbh8uXLOHLkiFi+atUquLq6onr16ggODkZ8fDwEQQAAWFtbY9GiRfjxxx+RnJyMb7/9Fj/99JPY68HW1hZ5eXlYt26duE1x6Nq1K5ydnRETE6N2vbW1NXR0dLB69Wq8ePHijfdT0LEXRVhYGFatWiW+dpGYmAg/Pz+V/3S1pauri0mTJmHu3Ln477//3qgObejr66N79+5ISEgQyxITExEWFqY2PiEhAQEBATA3N0dwcDAUCoVKjIODA+bMmYPhw4cjODgYJiYmGD9+fIF55ObmIicnR7IQERHRp23Pnj1ITU3FggULMGfOHJUvjIiINGHjxjt24cIFCIIAV1dXjTHKMTEKinkbynqV400AL18zCA4OBgD4+fkhOzsbu3fvFte3a9cOnTp1gp+fHxo3boyQkBBxXYMGDTB69Gh069YNVlZW8Pf3x/Tp03Hjxo23ylM5FsSiRYtw8eJFlfUVK1bEzz//jDFjxqBs2bJo2rQpxo8fj3///bdI+yns2LXl5eUFJycnrF69GoIgFNhAkJ2dDRMTE8ni7++vEte+fXvUrl0bY8eOLXI+RaFsmHn06BFSUlKQnZ2N1q1bq8Tl5+cjMTFRPF9dunTB3r17cenSJZXYXr16wd3dHX/88QcSEhIgl8sLzGHy5MlibxRzc3Ox8YyIiIg+XVWqVIGHhwciIiLw7bffimOBWVlZQVdXV+XvzRs3bhS5FysRfZzYuPGOadMjoDh7PxRUv3JgyrNnz+LQoUPo2rUrgJevWHTu3FnlG/kff/wR+fn5+OGHH1TqnDhxIrKysrBgwQK4ublhwYIFcHV1xYkTJ94qV19fX3z55ZcaxxmJjIxEVlYWli5dioYNGyIpKQlubm5ITk7Wqn5tj11bYWFhSEhIwO7du/Ho0SO0atVKbZypqSnS0tIky+LFi9XGTp06FUuWLEF6evob5aQNT09PVKtWDatXr0Z8fDx69OghvmL0quTkZMlxWVlZoUWLFmoH7jp+/DiOHj0KIyMjyeBfmowaNQrZ2dni8vqApERERPRpy8/PR25uLoCXPU/r1KkjGUcuPz8f27dvF18RJqJPG8fceMeqVasGmUxW4LuALi4uAIAzZ868k5uz8iG5SpUqAF72XMjLy4OdnZ0YIwgC5HI55s2bJ47roHzYVffQC7wc3yMoKAhBQUGYNGkSvLy8MGPGDCxZsuSt8p0yZQoaNmyI4cOHq11vamqKwMBABAYGYsKECfD19cWECRPQokWLQuvW9ti11b17d4wYMQLR0dEaGwiAl68HOTs7a1XnV199BV9fX4waNQqhoaFFyqcowsLCEBsbi9OnT2ucI16hUODu3buSsUzy8/Pxzz//YNy4cdDRedk++uzZM/Ts2RPdu3dH48aN0bdvX7Ru3RrVq1fXuH+5XF5o7w4iIiIqnR4+fCiZfe/SpUtIS0uDpaUlKlWqhFGjRuHatWv49ddfAQCxsbGoVKmS2OM4JSUFM2bMwMCBA8U6hgwZgpCQENStWxf16tXDnDlz8OjRI/Tq1ev9HhwRfZDYc+Mds7S0hK+vL2JjY9XOJHL//n20bNkSVlZWageRVMa8qfz8fPz888+oUqUKvLy8kJeXh19//RUzZ86U9CI4fvw47Ozs3vi9Rn19fVStWvWtZktRqlevHr7++mt89913hcbKZDK4urpqtd93ceyWlpZo06YNdu/erfGVlDcxZcoU/PHHH+909O9u3brhxIkTcHd3R82aNVXW37lzB//73/+wYsUKyfk6duwY7t27h61bt4qxMTExuHv3LmbPno2QkBC0aNECvXr1ksy4Q0RERJ+O1NRUeHl5wcvLC8DLhgkvLy+MGTMGAJCZmSkZyDw/Px+jRo1C7dq1UbduXcTGxmLq1KmSsdg6d+6MGTNmYMyYMahduzbS0tKwefPmNx7vjIg+Luy58R7ExsbC29sb9erVQ0xMDGrVqoW8vDwkJycjLi4O6enpWLx4MYKCgtCmTRsMHDgQzs7OuH37NlatWoUrV65gxYoVWu3rzp07yMrKwuPHj3Hy5EnMmTMHhw4dwp9//gldXV2sX78e9+7dQ3h4uEovhQ4dOkChUKBv374F7mPjxo1YsWIFunTpAhcXFwiCgD/++AObNm2SDFL5NiZOnAg3NzdJT4i0tDSMHTsWPXr0QM2aNaGvr4/du3cjPj4eI0eOLLTOjRs3FunYz549q1KHm5ubSlliYiLmz5+PcuXKady3IAjIyspSKbexsRF7P7zKw8MD3bt3x88//1zgMb2NsmXLIjMzE2XKlFG7/rfffkO5cuXQqVMn8ZUmpVatWkGhUMDPzw+HDx/G1KlT8eeff4rndeHChXB3d8fs2bMxdOjQd3YMRERE9GHy8fEp8NXr16elj4qKQlRUVKH1DhgwAAMGDHjb9IjoI8TGjffAyckJR48excSJEzF06FBkZmbC2toaderUQVxcHACgbdu22L9/PyZPnoxu3bohJycHDg4OaNq0KSZMmKD1vpRzfxsZGaFy5cpo0qQJFi1aJL4SoVAo0Lx5c7WvX3To0AHTpk3DP//8g1q1amncR82aNWFkZIShQ4fi6tWrkMvlqFatGhYvXowePXoU5dRo5OLigrCwMCxatEgss7e3h6OjI8aNG4eMjAzIZDLx52+//bbQOrU9djMzMwAvB898nbpxIQwNDSWvbaiTk5ODChUqqJRnZmZqHAQrJiYGK1euLLDet1XQTDzx8fFo3769SsMG8PJ89ejRAzdu3EBISAh69eqFli1biusrVKiAuXPnIjw8vNDXU4iIiIiIiN6WTHjXo1kSEWkhJyfn5awpg1dBR25U0ukQERFRITKmBJR0CkT0kVI+G2RnZ4tfPheGY24QERERERERUanGxo1Som/fvjAxMVG7FDZGxvtWmnL9mOzZs0fjeTcxMSnp9IiIiIiIiN4ZvpZSSty8eRM5OTlq15mZmcHGxuY9Z6RZacr1Y/LkyRNcu3ZN43ptp6ItKXwthYiIqHThaylE9K68yWspHFC0lLCxsSk1jQKlKdePiaGh4QffgEFERERERPQu8LUUIiIiIiIiIirV2HODiD4oJ8f5at31jIiIiIiICGDPDSIiIiIiIiIq5di4QURERERERESlGhs3iIiIiIiIiKhUY+MGEREREREREZVqbNwgIiIiIiIiolKNs6UQ0QfFfewW6MiNSjoNIiL6BGVMCSjpFIiI6A2x5wYRERERERERlWps3CAiIiIiIiKiUo2NG0RERERERERUqrFxg4iIiIiIiIhKNTZuEBEREREREVGpxsYNIiIiIiIiIirV2LhBRERERKRBSkoKAgMDYWdnB5lMhvXr1xcYn5mZiW7dusHFxQU6OjoYPHiw2rikpCS4urrCwMAAHh4e2LRpU/EnT0T0CWHjBhERERGRBo8ePYKnpydiY2O1is/NzYW1tTV++OEHeHp6qo3Zv38/unbtivDwcBw7dgzt2rVDu3btcPLkyeJMnYjok/JeGzdCQ0Mhk8nEpVy5cvDz88M///wDAMjIyIBMJkNaWprKtj4+PpKWb0dHR7EeIyMjeHh4YPHixSrbvXjxArNnz4aHhwcMDAxQtmxZ+Pv7Y9++fVrnnZiYCJlMhho1aqisS0pKgkwmg6Ojo6T8yZMnGDt2LFxcXCCXy2FlZYWgoCCcOnVKEhcdHQ2ZTIa+fftKytPS0iCTyZCRkSHGFLQAL89vu3btVHLctWsXZDIZ7t+/L5b98ssv8PT0hImJCSwsLODl5YXJkydrdT5ez8fc3ByNGjXC7t271cZPnjwZurq6mD59usq6xMREWFhYaLVfV1dXyOVyZGVlqazz8fGBTCbDihUrJOVz5sxR+WyePXuGadOmwdPTE0ZGRrCysoK3tzcSEhLw/PlzAKrXqnLx8/PTKtfjx4+jTZs2sLGxgYGBARwdHdG5c2fcvHkTgOq1rsxf06I8t+86L3XXipKjoyPmzJkj/qzc98GDByVxubm5KFeuHGQyGXbt2qVVXkRERB8qf39/TJgwAe3bt9cq3tHRET/99BN69uwJc3NztTE//fQT/Pz8MHz4cNSoUQPjx4/HZ599hnnz5hVn6kREn5T33nPDz88PmZmZyMzMxPbt26Gnp4fWrVu/UV0xMTHIzMzEyZMnERwcjIiICPz111/iekEQ0KVLF8TExGDQoEFIT0/Hrl274ODgAB8fn0K7Fb7K2NgYN2/exIEDByTlCoUClSpVkpTl5uaiefPmiI+Px4QJE3Du3Dls2rQJeXl5qF+/vsrDoIGBARQKBc6fP69238OGDRPPWWZmJuzt7cVjVy5FER8fj8GDB2PgwIFIS0vDvn37MGLECDx8+FDrOtzc3MR9HzhwANWqVUPr1q2RnZ2tdn8jRoxAfHx8kfJ81d69e/HkyRN07NgRS5YsURtjYGCAH374QWygUOfZs2fw9fXFlClT0KdPH+zfvx+HDh1CZGQk5s6dK2l8evVaVS7Lly8vNNdbt26hWbNmsLS0xJYtW5Ceno6EhATY2dnh0aNHardZu3atyr4uX74Md3d31K1bF/Xr1y+RvArj4OCAhIQESdm6detgYmLyRvURERF9Cg4cOIDmzZtLynx9fVX+ziQiIu3pve8dyuVy2NraAgBsbW3x3XffoVGjRrh161aR6zI1NRXrGjlyJKZNm4bk5GT4+/sDAFatWoXVq1djw4YNCAwMFLdbtGgR7ty5g969e6NFixYwNjYudF96enro1q0b4uPj0bBhQwDAf//9h127duHbb7+VPFzOmTMHBw4cwLFjx8TuiJUrV8aaNWtQv359hIeH4+TJk2KPi+rVq8PGxgbff/89Vq1apbJvExMTycOirq6u5NiLasOGDejUqRPCw8PFMjc3tyLVoaenJ/kcY2JikJCQgHPnzuHzzz8X43bv3o0nT54gJiYGv/76K/bv348vvviiyDkrFAp069YNjRs3xqBBgzBy5EiVmK5du2LDhg345Zdf0L9/f7X1zJkzBykpKUhNTYWXl5dY7uTkhKCgIDx79kwse/VaLYp9+/YhOzsbixcvhp7ey1+xKlWqoEmTJhq3sbS0VCmLiIjA7du3cfjwYRgYGJRIXoUJCQnBzz//jDlz5sDQ0BDAy8askJAQjB8//o3rJSIi+phlZWWhfPnykrLy5cur7Z1KRETaKdExNx4+fIjff/8dzs7OKFeu3BvXk5+fjzVr1uDevXvQ19cXy5ctWwYXFxdJw4bS0KFDcefOHSQnJ2u9n7CwMKxatQqPHz8G8PKVCj8/P5X/nJYtW4YWLVqovGepo6ODb7/9FqdPn8bx48cl66ZMmYI1a9YgNTVV63zelK2tLQ4ePIjLly8XS325ublISEiAhYUFqlevLlmnUCjQtWtXlClTBl27doVCoShy/Q8ePEBSUhKCg4PRokULZGdnY8+ePSpxZmZm+P777xETE6OxJ8LSpUvRvHlzScOGUpkyZbRq6CqMra0t8vLysG7dOgiC8EZ1zJ8/H7/++ivWrFkDe3v7t86puPJ6XZ06deDo6Ig1a9YAAK5cuYKUlBT06NGj0G1zc3ORk5MjWYiIiIiIiN7Ee2/c2Lhxo9gTwdTUFBs2bMDKlSuho1P0VEaOHAkTExPI5XJ07NgRZcuWRe/evcX1586dUztOBgCx/Ny5c1rvz8vLC05OTli9ejUEQUBiYiLCwsJU4t5kv5999hk6deqktkdCUbx6fpWLsieL0tixY2FhYQFHR0dUr14doaGhWLVqFfLz87Xez4kTJ8T6DQ0NMWPGDCxfvhxmZmZiTE5ODlavXo3g4GAAQHBwMFatWlWk118AYMWKFahWrRrc3Nygq6uLLl26aGwk6d+/PwwMDDBr1iy168+fPw9XV1et9qvuXE6aNKnQ7Ro0aIDRo0ejW7dusLKygr+/P6ZPn44bN25otd+UlBQMHjwYsbGxanu5lFRemoSFhYmvHCUmJqJVq1awtrYudLvJkyfD3NxcXBwcHN4qDyIiotLC1tZW5f/fGzduvHGvXCIiKoHGjSZNmiAtLQ1paWk4dOgQfH194e/v/0a9CIYPH460tDTs2LED9evXx+zZs+Hs7CyJKa5vqJXCwsKQkJCA3bt349GjR2jVqpXauDfZ74QJE7Bnzx5s3br1jfN79fwql9cHWq1QoQIOHDiAEydOYNCgQcjLy0NISAj8/Py0buCoXr26WP+RI0fQr18/BAUFSXqeLF++HFWrVhV7sNSuXRuVK1fGypUri3RM8fHxYgMJ8LKRJCkpCQ8ePFCJlcvliImJwYwZM3D79m2V9UX5XNSdy9cHftVk4sSJyMrKwoIFC+Dm5oYFCxbA1dUVJ06cKHC7K1euoGPHjujTp4+koa6k8ypIcHAwDhw4gH///Vdjg586o0aNQnZ2trhcvXr1jXMgIiIqTRo2bIjt27dLypKTk8VXn4mIqOjee+OGsbExnJ2d4ezsjM8//xyLFy/Go0eP8Msvv4jf+qsblPL+/fsqI05bWVnB2dkZjRo1QlJSEgYOHIjTp0+L611cXJCenq42D2W5i4tLkfLv3r07Dh48iOjoaPTo0UMcu+BVb7rfqlWrIiIiAt99990bN8q8en6VS8WKFdXGuru7o3///vj999+RnJyM5ORkjTOevE5fX1+s38vLC1OmTEHFihUls2koFAqcOnUKenp64nL69OkiDSx6+vRpHDx4ECNGjBDraNCgAR4/fqwyM4pScHAwKleujAkTJqisc3FxwZkzZ7Tat7pzqW5sDE3KlSuHoKAgzJgxA+np6bCzs8OMGTM0xj958gTt27eHm5ub5Dy+z7yK+juorK9169YIDw/H06dPVXoKaSKXy2FmZiZZiIiIPjQPHz4Uv0wAgEuXLiEtLQ1XrlwB8LKxvmfPnpJtlPEPHz7ErVu3kJaWJvkbddCgQdi8eTNmzpyJM2fOIDo6GqmpqRgwYMB7Oy4ioo9NiY65AbycTlJHRwdPnjyBpaUlrKyscOTIEUlMTk4OLly4UGBDhIODAzp37oxRo0aJZV26dMH58+fxxx9/qMTPnDkT5cqVQ4sWLYqUr6WlJdq0aYPdu3dr/Ia6S5cu2LZtm8q4Gvn5+Zg9ezZq1qypcd7zMWPG4Ny5cxof3N+VmjVrAsAbz5oBvBzo9MmTJwBevraSmpqKXbt2SXoY7Nq1CwcOHNC6gUGhUOCrr77C8ePHJfUMGTJE46spOjo6mDx5MuLi4pCRkSFZ161bN2zbtg3Hjh1T2e758+dvdfwF0dfXR9WqVQusv3fv3rh79y6SkpLUNpq9j7yqVasGHR0dld/Bf//9F9nZ2Rp/B8PCwrBr1y707NkTurq67zxvIiKi90U5CLlyvK4hQ4bAy8sLY8aMAQBkZmaKDR1KyvgjR45g2bJl8PLykvT2/eKLL7Bs2TIsWrQInp6eWL16NdavXw93d/f3d2BERB+Z9z5bSm5urjgS9L179zBv3jw8fPhQHPRzyJAhmDRpEsqXL48GDRrgzp07GD9+PKytrfH1118XWPegQYPg7u6O1NRU1K1bF126dEFSUhJCQkIwffp0NGvWDDk5OYiNjcWGDRuQlJT0RgNIJiYmYv78+RoHQf3222/xv//9D4GBgZg5cybq16+PGzduYNKkSUhPT8e2bdvEmVJeV758eQwZMgTTp08vcl7a6tevH+zs7NC0aVPY29sjMzMTEyZMgLW1tdbdIfPy8sTP8cGDB1i5ciVOnz4tjhmiUChQr149fPXVVyrbfv7551AoFOIxvnjxQvw2REkul8PZ2Rm//fYbYmJiVP6z7927N2bNmoVTp06pneklICAA9evXx8KFCyUDvg4ePBh//vknmjVrhvHjx+PLL7+EqakpUlNTMXXqVCgUCtSuXRuA9FpV0tPTg5WVVYHnZuPGjVixYgW6dOkCFxcXCIKAP/74A5s2bVKZNlVp+vTpSEpKwh9//CE5t0rm5ubibCTvMi9TU1P07t0bQ4cOhZ6eHjw8PHD16lWMHDkSDRo00DjTjZ+fH27dusXeF0RE9NHx8fEpsEdtYmKiSpk2PXCDgoIQFBT0NqkREdEr3nvjxubNm1GhQgUALx+kXF1dkZSUBB8fHwDAiBEjYGJigqlTp+LixYuwtLSEt7c3du7cKT7caVKzZk20bNkSY8aMwaZNmyCTybBq1SrMmTMHs2fPFgebbNiwIXbt2gVvb+83OgZDQ8MCczEwMMCOHTswadIkjB49GpcvX4apqSmaNGmCgwcPFtoqP2zYMMTFxeHp06dvlF9hmjdvjvj4eMTFxeHOnTuwsrIS3/3UdtaaU6dOiZ+jkZERqlatiri4OPTs2RPPnj3D77//rnFw1A4dOmDmzJniIJgPHz5Umb2katWqmDp1Ku7cuYP27dur1FGjRg3UqFEDCoVC4+ChU6dOVXkYl8vlSE5OxuzZs7Fw4UIMGzYMRkZGqFGjBgYOHCj5bF69VpWqV69eaK+TmjVrwsjICEOHDsXVq1chl8tRrVo1LF68WOMsIvPnz8fz58/h5+endn1CQgJCQ0PfS14//fQTpkyZgpEjR+Ly5cuwtbVFixYtMHHiRI2NcjKZrNDGFSIiIiIiondFJhT3iJtERG8gJyfn5awpg1dBR25U0ukQEdEnKGNKQEmnQERE+L9ng+zsbK17h5f4mBtERERERERERG+DjRsA3NzcYGJionZZunRpSaf33mk6FyYmJtizZ09Jp1fili5dqvH8qBv/41PPi4iIiIiI6F1772NufIg2bdqE58+fq1336mCUn4rXB/d8laZpZT8lbdq0Qf369dWuK1OmzHvO5v98qHkRERERERG9a2zcAFC5cuWSTuGD4uzsXNIpfNBMTU1hampa0mmo+FDzIiIiIiIietf4WgoRERERERERlWrsuUFEH5ST43y1HhGZiIiIiIgIYM8NIiIiIiIiIirl2LhBRERERERERKUaGzeIiIiIiIiIqFRj4wYRERERERERlWps3CAiIiIiIiKiUo2zpRDRB8V97BboyI1KOg0iIirFMqYElHQKRET0nrHnBhERERERERGVamzcICIiIiIiIqJSjY0bRERERERERFSqsXGDiIiIiIiIiEo1Nm4QERERERERUanGxg0iIiIiIiIiKtXYuEFEREREn4SUlBQEBgbCzs4OMpkM69evL3SbXbt24bPPPoNcLoezszMSExNVYq5du4bg4GCUK1cOhoaG8PDwQGpqavEfABERacTGDSIiIiL6JDx69Aienp6IjY3VKv7SpUsICAhAkyZNkJaWhsGDB6N3797YsmWLGHPv3j14e3ujTJky+Ouvv3D69GnMnDkTZcuWfVeHQUREarx140ZoaChkMhlkMhn09fXh7OyMmJgY5OXlAQBevHiB2bNnw8PDAwYGBihbtiz8/f2xb98+ST0vXrzAlClT4OrqCkNDQ1haWqJ+/fpYvHix1rlkZWUhKioKTk5OkMvlcHBwQGBgILZv3y7GODo6QiaT4eDBg5JtBw8eDB8fH0mMpiU0NLTQXF7/NkAmk8HAwACXL1+WxLVr105S361bt9CvXz9UqlQJcrkctra28PX1xb59+7Br164C85LJZNi1axcA4L///oO+vj7c3d21yu9Vyv2ULVsWT58+law7fPiwuK/X49UtWVlZAIDo6GjIZDL07dtXUl9aWhpkMhkyMjLEmIKWwrx6Pb66XLhwAQDg4+ODwYMHq2yXmJgICwsLANp9/hkZGZDJZEhLS1Op6/V9vFqfkZERPDw8VK5rbc5hQZTnzs/PT2Xd9OnTIZPJxOv71fjXF1dXV5Xtly9fDl1dXURGRqqsU+bt5uaGFy9eSNZZWFio/XaLiIiopPj7+2PChAlo3769VvELFixAlSpVMHPmTNSoUQMDBgxAx44dMXv2bDFm6tSpcHBwQEJCAurVq4cqVaqgZcuWqFq16rs6DCIiUqNYem74+fkhMzMT58+fx9ChQxEdHY3p06dDEAR06dIFMTExGDRoENLT07Fr1y44ODjAx8dH8nA9btw4zJ49G+PHj8fp06exc+dO9OnTB/fv39cqh4yMDNSpUwc7duzA9OnTceLECWzevBlNmjRReSgzMDDAyJEjNdZ1+PBhZGZmIjMzE2vWrAEAnD17Viz76aefinyOgJcNCmPGjCkwpkOHDjh27BiWLFmCc+fOYcOGDfDx8cGdO3fwxRdfiDlkZmaiU6dO4rlXLl988QWAlw/rnTp1Qk5ODv7+++83ytfU1BTr1q2TlCkUClSqVElt/KvnSLnY2NiI6w0MDKBQKHD+/Hm12w8bNkyyrb29PWJiYiRl2nj9nGRmZqJKlSpaHvW7+fyVx3Hy5EkEBwcjIiICf/31l0pcYeewIBUqVMDOnTvx33//Scrj4+PVfmZubm4q+9q7d69KnEKhwIgRI7B8+XKVxi6lf//9F7/++qtWeRIREZUWBw4cQPPmzSVlvr6+OHDggPjzhg0bULduXQQFBcHGxgZeXl745Zdf3neqRESfPL3iqETZwwAA+vXrh3Xr1mHDhg1wcnLC6tWrsWHDBgQGBorxixYtwp07d9C7d2+0aNECxsbG2LBhA/r374+goCAxztPTU+sc+vfvD5lMhkOHDsHY2Fgsd3NzQ1hYmCS2T58+WLBgATZt2oRWrVqp1GVtbS3+29LSEgBgY2MjfrP/pgYMGIBZs2Zh+PDhantU3L9/H3v27MGuXbvQuHFjAEDlypVRr149MUZ5ngHA0NAQubm5kjIAEAQBCQkJmD9/Puzt7aFQKFC/fv0i5xsSEoL4+Hh07doVAPDkyROsWLECAwcOxPjx41XiCztH1atXh42NDb7//nusWrVKZb2JiQlMTEzEn3V1dWFqaqpyfIV59Xp8E9p8/vfu3StSna8ex8iRIzFt2jQkJyfD399fEvc215mNjQ3q1KmDJUuW4PvvvwcA7N+/H7dv30ZQUBBOnz4tidfT0yv0PF26dAn79+/HmjVrsHPnTqxduxbdunVTiYuKisLYsWPRrVs3yOVyrfLNzc1Fbm6u+HNOTo5W2xEREb0vWVlZKF++vKSsfPnyyMnJwZMnT2BoaIh///0XcXFxGDJkCEaPHo3Dhw9j4MCB0NfXR0hISAllTkT06XknY24YGhri2bNnWLZsGVxcXCQNG0pDhw7FnTt3kJycDODlQ/uOHTtw69atIu/v7t272Lx5MyIjIyUNG0qvPyxWqVIFffv2xahRo5Cfn1/k/b0pb29vtG7dGt99953a9cqH+/Xr10se+opq586dePz4MZo3b47g4GCsWLECjx49KnI9PXr0wJ49e3DlyhUAwJo1a+Do6IjPPvvsjXObMmUK1qxZ88kOspWfn481a9bg3r170NfXL/b6w8LCJK+CxMfHo3v37m+8r4SEBAQEBMDc3BzBwcFQKBRq4wYPHoy8vDzMnTtX67onT54Mc3NzcXFwcHijHImIiEpSfn4+PvvsM0yaNAleXl7o06cPIiIisGDBgpJOjYjok1KsjRuCIGDbtm3YsmULmjZtinPnzqFGjRpqY5Xl586dAwDMmjULt27dgq2tLWrVqoW+ffuq7bavzoULFyAIgtrxAjT54YcfcOnSJSxdulTrbYrD5MmTsXnzZuzZs0dlnZ6eHhITE7FkyRJYWFjA29sbo0ePxj///FOkfSgUCnTp0gW6urpwd3eHk5MTkpKSipyrjY0N/P39xYfl+Ph4lV4wr7K3txcbaExMTODm5qYS89lnn6FTp04Fvhb0tjZu3CjJ49XeQCVl5MiRMDExgVwuR8eOHVG2bFn07t1bJU6bc1iQ1q1bIycnBykpKXj06BFWrVql8TM7ceKEZF8mJiaSMVHy8/ORmJiI4OBgAECXLl2wd+9eXLp0SaUuIyMjjB07FpMnT0Z2drZWuY4aNQrZ2dnicvXq1SIdKxER0btma2uLGzduSMpu3LgBMzMzGBoaAnj5WmjNmjUlMTVq1BC/HCIiovejWBo3lA+TBgYG8Pf3R+fOnREdHQ3gZYOHNmrWrImTJ0/i4MGDCAsLw82bNxEYGKj2AfB12u7jVdbW1hg2bBjGjBmDZ8+eFXn7N1WzZk307NlTY++NDh064Pr169iwYQP8/PzE6ce0HZjx/v37WLt2rfhACqDAb9wLo+wJ8O+//+LAgQPo3r27xtg9e/YgLS1NXDZt2qQ2bsKECdizZw+2bt36RjkVRjmiuXL5+eef38l+imL48OFIS0vDjh07UL9+fcyePRvOzs4qcdqeQ03KlCmD4OBgJCQkICkpCS4uLqhVq5ba2OrVq0v2lZaWhpiYGHF9cnIyHj16JL66ZWVlhRYtWiA+Pl5tfeHh4ShXrhymTp2qVa5yuRxmZmaShYiI6EPSsGFDycD0wMv/Hxs2bCj+7O3tjbNnz0pizp07h8qVK7+XHImI6KViGXOjSZMmiIuLg76+Puzs7KCn97JaFxcXpKenq91GWe7i4iKW6ejo4PPPP8fnn3+OwYMH4/fff0ePHj3w/fffFzggZLVq1SCTyXDmzJki5T1kyBDMnz8f8+fPL9J2b2vcuHFwcXHROFuJgYEBWrRogRYtWuDHH39E7969MXbsWK1maVm2bBmePn0qGWNDEATk5+fj3LlzkvOtDX9/f/Tp0wfh4eEIDAxEuXLlNMZWqVJFq/EiqlatioiICHz33Xdv3OhSEGNjY7UNBwBgZmamtmfB/fv3YW5urvU+lA/i2tZlZWUFZ2dnODs7IykpCR4eHqhbt67KNz3ansOChIWFoX79+jh58mSBPW2UsxtpolAocPfuXfGbKeBlb45//vkH48aNg46OtG1UT08PEydORGhoKAYMGPBWx0BERPQuPHz4UJxBDXg5tlRaWhosLS1RqVIljBo1CteuXRMHye7bty/mzZuHESNGICwsDDt27MCqVavw559/inV8++23+OKLLzBp0iR06tQJhw4dwqJFi7Bo0aL3fnxERJ+yYum5oXyYrFSpktiwAbzsxn7+/Hn88ccfKtvMnDkT5cqVQ4sWLTTWq3zwK2y8CEtLS/j6+iI2NlZtrKYZV0xMTPDjjz9i4sSJePDgQYH7KE4ODg4YMGAARo8erTJ9pjo1a9bUeswMhUKBoUOHSr6NP378OBo1aqTxG/eC6OnpoWfPnti1a1eBD8pFNWbMGJw7dw4rVqwotjq1Ub16dRw9elSl/OjRo0Vq+LG0tISVlRWOHDkiKc/JycGFCxcKrMvBwQGdO3fGqFGjtE+8CNzc3ODm5oaTJ0+qHfxTG3fu3MH//vc/rFixQnItHTt2DPfu3dPY6yYoKAhubm4YN27c2xwCERHRO5GamgovLy94eXkBePlFl5eXlzibXWZmpuR1kipVquDPP/9EcnIyPD09MXPmTCxevBi+vr5izOeff45169Zh+fLlcHd3x/jx4zFnzpwCe7sSEVHxK5aeG5p06dIFSUlJCAkJwfTp09GsWTPk5OQgNjYWGzZsQFJSkjgAaMeOHeHt7Y0vvvgCtra2uHTpEkaNGgUXFxetxtKIjY2Ft7c36tWrh5iYGNSqVQt5eXlITk5GXFycxh4kffr0wezZs7Fs2bI3mlHkTY0aNQq//PILLl26hM6dOwN4+UAZFBSEsLAw1KpVC6ampkhNTcW0adPQtm3bQutMS0vD0aNHsXTpUpVz1rVrV8TExGDChAliA5Ty24pXVatWTaXe8ePHY/jw4QX22gCAmzdvqkwVWq5cOZQpU0Yltnz58hgyZAimT59e6HEVp379+mHevHkYOHAgevfuDblcjj///BPLly9X2whXkCFDhmDSpEkoX748GjRogDt37mD8+PGwtrbG119/XeC2gwYNgru7O1JTU1G3bl2xvCjnsCA7duzA8+fPC+wFkpeXh6ysLEmZTCZD+fLl8dtvv6FcuXLo1KkTZDKZJKZVq1ZQKBTw8/NTW++UKVMkf/QRERF9KHx8fAp8nVnda8A+Pj44duxYgfW2bt0arVu3ftv0iIjoLbzTxg2ZTIZVq1Zhzpw5mD17Nvr37w8DAwM0bNgQu3btgre3txjr6+uL5cuXiwMS2traomnTpoiOjpb0BtHEyckJR48excSJEzF06FBkZmbC2toaderUQVxcnMbtypQpg/Hjx7/xN9xvytLSEiNHjsTo0aPFMhMTE3E8hosXL+L58+dwcHBARESEJE4ThUKBmjVrqm0Mat++PQYMGIBNmzahTZs2AF4+nL9O3UCn+vr6sLKyKnT/1atXVyk7cOAAGjRooDZ+2LBhiIuLU3mYf5ecnJyQkpKC77//Hs2bN8ezZ8/g6uqKpKQkjQ/rmowYMQImJiaYOnUqLl68CEtLS3h7e2Pnzp2SVznUqVmzJlq2bIkxY8ZIxtUo6jnURN2sQa87deoUKlSoICmTy+V4+vQp4uPj0b59e5WGDeDluDA9evTA7du31dbbtGlTNG3a9J2NqUJERERERPQ6mfAmo3ESERWznJycl1PCDl4FHblRSadDRESlWMaUgJJOgYiI3oLy2SA7O1vriQeKdSpYIiIiIiIiIqL3rVQ0bly5cgUmJiYal/c9j/ikSZM05uLv7/9ec/mUfGjXwftU0HGre5WIiIiIiIjoU/JOx9woLnZ2dioDX76+/n3q27cvOnXqpHZdYWMt0Jv70K6D96mg465YseL7S4SIiIiIiOgDVCoaN/T09ODs7FzSaYgsLS1haWlZ0ml8cj606+B9+lSPm4iIiIiISBul4rUUIiIiIiIiIiJNSkXPDSL6dJwc56v1iMhEREREREQAe24QERERERERUSnHxg0iIiIiIiIiKtXYuEFEREREREREpRobN4iIiIiIiIioVGPjBhERERERERGVapwthYg+KO5jt0BHblTSaRARfdIypgSUdApERERFwp4bRERERERERFSqsXGDiIiIiIiIiEo1Nm4QERERERERUanGxg0iIiIiIiIiKtXYuEFEREREREREpRobN4iIiIiIiIioVGPjBhERERFpLTY2Fo6OjjAwMED9+vVx6NAhjbHPnz9HTEwMqlatCgMDA3h6emLz5s0a46dMmQKZTIbBgwe/g8yJiOhjxsaN9yQ0NBTt2rUT/y2TyTBlyhRJzPr16yGTySRlv/zyCzw9PWFiYgILCwt4eXlh8uTJAABHR0fIZDKNS2hoqFiPr68vdHV1cfjw4QJze1OTJ0+Grq4upk+frrIuMTERMpkMNWrUUFmXlJQEmUwGR0dHAICPj0+Bx+Tj41NoLq+eF2NjY3z22WdISkqSxDx58gSWlpawsrJCbm4uAODcuXMwMjLCsmXLJLH5+fn44osv0LFjRwD/9/n17dtXZd+RkZEq514Z//ri5+enkvPBgwcl9Q0ePFg85qJ83prs3r0bTZs2haWlJYyMjFCtWjWEhITg2bNnAF5+VhYWFmq3lclkWL9+PQAgIyMDMpkMurq6uHbtmiQuMzMTenp6kMlkyMjIKDQnIiIqPVauXIkhQ4Zg7NixOHr0KDw9PeHr64ubN2+qjf/hhx+wcOFCzJ07F6dPn0bfvn3Rvn17HDt2TCX28OHDWLhwIWrVqvWuD4OIiD5CbNwoIQYGBpg6dSru3bunMSY+Ph6DBw/GwIEDkZaWhn379mHEiBF4+PAhgJd/BGRmZiIzMxNr1qwBAJw9e1Ys++mnnwAAV65cwf79+zFgwADEx8e/k+OJj4/HiBEjNNZvbGyMmzdv4sCBA5JyhUKBSpUqiT+vXbtWzF/5TdC2bdvEsrVr12qVT0xMDDIzM3Hs2DF8/vnn6Ny5M/bv3y+uX7NmDdzc3ODq6io+sLu4uGDKlCmIiopCZmamGDtz5kz8+++/WLBggVjm4OCAFStW4MmTJ2LZ06dPsWzZMsnxKPn5+YnHoFyWL18uiTEwMMDIkSM1HpO2n7cmp0+fhp+fH+rWrYuUlBScOHECc+fOhb6+Pl68eFHgtppUrFgRv/76q6RsyZIlqFix4hvVR0REH7ZZs2YhIiICvXr1Qs2aNbFgwQIYGRlp/P//t99+w+jRo9GqVSs4OTmhX79+aNWqFWbOnCmJe/j/2Lv7uJrv/3/gj1NHV7rSNYkQIlZmY9jMRXMiV81ICpXxibLRNtaGLBfFMGZhs3M6mYuKxcdsnyxt5TKM5YNhc1G5KPSJohKp3x9+vb/ezjl1SlQ87rfb+/ZxXq/X+/V+vt/nfNY5z/fr9XrfvQtfX1+sX78ezZo1ex6nQkRELxgmN+qJu7s77OzshFEY6uzcuRNjxozBpEmT4OTkBBcXF/j4+GDRokUAAGtra9jZ2cHOzg4WFhYAABsbG6HMzMwMABATE4OhQ4di6tSp2LJli+gHeV1IS0tDSUkJIiIiUFhYKEoiVJJKpRg3bpzoy8+VK1eQmpqKcePGCWUWFhZC/NbW1gAAS0tLlfOsjomJCezs7NChQwdER0fD0NAQP/30k1Avl8vh5+cHPz8/yOVyoXz69OlwdXXF5MmTAQBnz57FvHnz8N1338HKykpo9+qrr8LBwUGUbElMTESrVq3QrVs3lXj09fWFc6jcnvzyNmXKFKSnp+OXX35Re07avt+a/Prrr7Czs8PSpUvRpUsXtGvXDh4eHli/fj0MDQ2r3FeTiRMnIiYmRlQWExODiRMn1qo/IiJquO7fv49jx47B3d1dKNPR0YG7u7vKzYtKpaWlMDAwEJUZGhpi//79orLg4GB4enqK+iYiIqoJJjfqia6uLhYvXozVq1fjypUratvY2dkhPT0dWVlZtT5ORUUFYmJi4OfnB2dnZzg5OWHbtm217k8duVwOHx8fNGnSBD4+PqJkweMCAwORkJCA4uJiAI+mQHh4eMDW1rZO43mSVCpFkyZNhKkXFy5cwKFDhzBmzBiMGTMG+/btE66xRCJBTEwM9u3bh/Xr18Pf3x9jx47F8OHD1Z7P4z/sFQoFAgICah1nmzZtEBQUhLCwMJSXl9e6H03s7OyQk5ODvXv31lmfw4cPx61bt4Qvqfv378etW7cwbNiwavctLS1FYWGhaCMiooYrLy8PDx8+VPm7bWtri9zcXLX7yGQyrFixAv/88w/Ky8uRnJwsjNKsFBcXh+PHj1d5w4eIiKg6TG7UIy8vL7i5uSE8PFxtfXh4OMzNzeHo6IiOHTvC398fCQkJNfrhu2fPHhQXF0MmkwGAykiFp1VYWIht27bBz89P6D8hIUGYOvO4bt26oW3btti2bRsqKiqgVCoRGBhYZ7Goc//+fURGRqKgoAADBgwA8CgJMXjwYDRr1gwWFhaQyWSiJEXr1q2xcuVKBAUFVTndw8/PD/v370dWVhaysrJw4MAB4To8adeuXTA2NhZtixcvVmk3Z84cXLp0CZs2baqDsxcbPXo0fHx88Pbbb6N58+bw8vLCN99881RJhSZNmsDPz08YkaNQKODn54cmTZpUu29kZCTMzMyEzcHBodZxEBFRw7Rq1Sq0b98ezs7O0NPTQ0hICAICAqCj8+gr6OXLl/Hhhx9i06ZNKiM8iIiIaoLJjXq2ZMkSxMbG4syZMyp1zZs3x6FDh3Dy5El8+OGHKCsrw8SJE+Hh4aF1gkOhUMDb2xtSqRQA4OPjgwMHDuDChQt1Ev+WLVvQrl07uLq6AgDc3NzQunVrxMfHq21fOdohLS0NRUVFGDJkSJ3E8aTZs2fD2NgYRkZGWLJkCaKiouDp6YmHDx8iNjZWlITw8/ODUqkUXdOAgAA0b94c06dPh6mpqdpjWFtbw9PTE0qlEjExMfD09BRNXXlc//79kZGRIdrULUhqbW2Njz/+GPPmzRNGmtQVXV1dxMTE4MqVK1i6dCns7e2xePFiuLi4iO6g1VRgYCC2bt2K3NxcbN26VeuEVVhYGAoKCoTt8uXLtY6BiIiePSsrK+jq6uL69eui8uvXr8POzk7tPtbW1tixYweKioqQlZWFs2fPwtjYGG3btgUAHDt2DDdu3MCrr74KqVQKqVSKtLQ0fP3115BKpbVeE4qIiF4+TG7Us759+0ImkyEsLExjmy5dumDatGnYuHEjkpOTkZycjLS0tGr7zs/Px/bt27FmzRrhC4O9vT3KysrqbGFRuVyO06dPC/1LpVL89ddfGvv39fVFeno65s+fj/HjxwtJl7r2ySefICMjA1euXMGtW7eEhTp3796Nq1evCgkfqVSKsWPHIisrCykpKaI+KuurEhgYCKVSidjY2Cp/1Ddt2hROTk6iTdP6IaGhoSgpKcGaNWtqeNbasbe3x/jx4/HNN9/g9OnTuHfvnrBYqqmpKYqKilSSZ7dv3wYAtet6dO3aFc7OzvDx8UGnTp3QpUsXreLQ19eHqampaCMiooZLT08P3bt3F/29LC8vR0pKCnr16lXlvgYGBsJ3kB9//BEjRowAAAwcOBAnT54UJf9fe+01+Pr6IiMjA7q6us/0nIiI6MXxbH5ZUo1ERUXBzc0NHTt2rLZt586dAQBFRUXVtt20aRNatmwpPA2k0q+//orly5cjIiLiqb40nDx5En/88QdSU1NFP9Tz8/PRr18/nD17Fs7OzqJ9LCwsMHz4cCQkJIiePlLXrKys4OTkpFIul8sxduxYfP7556LyRYsWQS6X45133qnRcTw8PHD//n1IJBJh6s/TMjY2xty5czF//ny1a33UpWbNmqF58+bC56ljx44oKytDRkYGXn31VaHd8ePHATx6oow6gYGBmDZtGtauXftM4yUiovoVGhqKiRMn4rXXXkOPHj2wcuVKFBUVCWtOTZgwAfb29sL6GYcPH8bVq1fh5uaGq1evYv78+SgvL8esWbMAPFoA/MmkeNOmTWFpaal1spyIiAhgcqNB6Nq1K3x9ffH111+LyqdOnYoWLVpgwIABaNmyJXJycrBw4UJYW1tXe4cEePRD/r333lP5cuDg4ICwsDAkJSXB09MTAFBQUICMjAxRO0tLyyrXQZDL5ejRowf69u2rUvf6669DLpfjyy+/VKlTKpVYs2YNLC0tqz2HunTz5k389NNP2Llzp8o1mTBhAry8vJCfn6/1E1mAR1M9KqcUVZUoKi0tVVlsTSqVapzGMmXKFHz11VfYvHkzevbsqXU8Vfn222+RkZEBLy8vtGvXDvfu3cOGDRtw+vRprF69GgDg4uKCQYMGITAwEMuXL0fbtm1x7tw5zJgxA97e3hof8Tp58mSMHj0a5ubmdRIrERE1TN7e3rh58ybmzZuH3NxcuLm5ISkpSVhkNDs7W1hPA3j0mPQ5c+bg4sWLMDY2xpAhQ/DDDz/w7wUREdU5JjcaiIiICJV1Ktzd3aFQKLB27Vr873//g5WVFXr16oWUlJRqEwPHjh3DiRMnsH79epU6MzMzDBw4EHK5XEhupKamqjzCdNKkSfj+++/V9n///n1s3LhRmO7xpFGjRmH58uVqF800NDSs9aNHn8aGDRvQtGlTDBw4UKVu4MCBMDQ0xMaNG/HBBx/UqF9tplMkJSWhefPmorKOHTvi7Nmzats3adIECxYsED0m92n16NED+/fvR1BQEK5duwZjY2O4uLhgx44dePvtt4V28fHxCA8Px7/+9S9cu3YNLVu2hJeXF+bOnaux76oSNURE9GIJCQlBSEiI2rrU1FTR67fffht//fVXjfp/sg8iIiJtSCoqKirqOwgiosLCwkdPTZmRAB19o/oOh4jopZYZ5VnfIRAR0Uus8rdBQUGB1mvzcUFRIiIiIiIiImrUmNwgjTZt2gRjY2O1m4uLC2NqYBYvXqzx2gwePLi+wyMiIiIiInpmOC2FNLpz547Ks+wrNWnSBK1bt37OETXMmBqK/Px85Ofnq60zNDTUuBhoQ8FpKUREDQenpRARUX2qzbQULihKGpmYmMDExKS+wxBpiDE1FBYWFjV60gsREREREdGLgtNSiIiIiIiIiKhRY3KDiIiIiIiIiBo1Tkshogbl1BcyrefVERERERERARy5QURERERERESNHJMbRERERERERNSoMblBRERERERERI0akxtERERERERE1KgxuUFEREREREREjRqflkJEDUqX8N3Q0Teq7zCIiF5qmVGe9R0CERFRjXDkBhERERERERE1akxuEBEREREREVGjxuQGERERERERETVqTG4QERERERERUaPG5AYRERERERERNWpMbhARERERERFRo8bkBhERERFpLTo6Go6OjjAwMEDPnj1x5MgRjW0fPHiAiIgItGvXDgYGBnB1dUVSUpLG9lFRUZBIJJgxY8YziJyIiF5kTG684HJzczF9+nS0bdsW+vr6cHBwwLBhw5CSkiK0+fPPPzF69GjY2trCwMAA7du3x+TJk/H333/X6FgymQy6uro4evSoSp2/vz8kEgmCgoJU6oKDgyGRSODv7w8AkEgkVW7z58+vMo7MzExRe0tLSwwaNAh//vmnqN2hQ4egq6sLT09PoeyHH35A06ZNcf78eVHba9euoVmzZvjmm28AAI6OjpBIJIiLi1M5vouLCyQSCZRKpVBW2f7JLSoqShSzjY0N7ty5I+rPzc0N8+fPVzkvddvjx9Rk/fr1cHV1hbGxMczNzdGtWzdERkYK9f7+/hg5cqTKfqmpqZBIJLh9+zYAQKlUQiKRoFOnTiptt27dColEAkdHx2rjISKixiM+Ph6hoaEIDw/H8ePH4erqCplMhhs3bqhtP2fOHHz77bdYvXo1/vrrLwQFBcHLy0vlbzIAHD16FN9++y1eeeWVZ30aRET0AmJy4wWWmZmJ7t2747fffsOXX36JkydPIikpCf3790dwcDAAYNeuXXjjjTdQWlqKTZs24cyZM9i4cSPMzMwwd+5crY+VnZ2NgwcPIiQkBAqFQm0bBwcHxMXFoaSkRCi7d+8eNm/ejFatWgllOTk5wrZy5UqYmpqKyj7++GOtYtqzZw9ycnKwe/du3L17F4MHDxZ+mAOAXC7H9OnTsXfvXly7dg0AMH78eMhkMvj7+6O8vFxoO3nyZHTv3l24bpXnExMTIzpmeno6cnNz0bRpU5V4IiIiROeRk5OD6dOni9rcuXMHy5YtU3s+Dg4Oon0/+ugjuLi4iMq8vb2rvCYKhQIzZszABx98gIyMDBw4cACzZs3C3bt3q9xPk6ZNm+LGjRs4dOiQqFwul4veUyIiejGsWLECkydPRkBAADp37ox169bByMhI49/+H374AZ999hmGDBmCtm3bYurUqRgyZAiWL18uanf37l34+vpi/fr1aNas2fM4FSIiesFI6zsAenamTZsGiUSCI0eOiH5su7i4IDAwEMXFxQgICMCQIUOwfft2ob5Nmzbo2bOnKBFQnZiYGAwdOhRTp07FG2+8gRUrVsDQ0FDU5tVXX8WFCxeQmJgIX19fAEBiYiJatWqFNm3aCO3s7OyEf5uZmUEikYjKtGVpaQk7OzvY2dlh2bJl6NOnDw4fPgyZTIa7d+8iPj4ef/zxB3Jzc6FUKvHZZ58BAL799lu4uLhgxYoV+Pjjj6FUKnHgwAGcPHkSEolE6N/X1xdfffUVLl++DAcHBwCPkge+vr7YsGGDSjwmJibVnsf06dOxYsUKBAcHw8bGRlSnq6sr2t/Y2BhSqbRG12bnzp0YM2YMJk2aJJS5uLhovf+TpFIpxo0bB4VCgV69egEArly5gtTUVMycORNbtmypdd9ERNSw3L9/H8eOHUNYWJhQpqOjA3d3d5Ukd6XS0lIYGBiIygwNDbF//35RWXBwMDw9PeHu7o6FCxfWffBERPTC48iNF1R+fj6SkpIQHBysdhSBubk5du/ejby8PMyaNUttH+bm5lodq6KiAjExMfDz84OzszOcnJywbds2tW0DAwNFox0UCgUCAgK0Os7TqEy03L9/HwCQkJAAZ2dndOzYEX5+flAoFKioqAAAWFtb47vvvsPcuXORnJyMmTNnYtWqVUICo5KtrS1kMhliY2MBAMXFxYiPj0dgYGCt4/Tx8YGTkxMiIiJq3UdV7OzskJ6ejqysrDrrMzAwEAkJCSguLgbwaLqKh4cHbG1tq9yvtLQUhYWFoo2IiBquvLw8PHz4UOW/77a2tsjNzVW7j0wmw4oVK/DPP/+gvLwcycnJSExMRE5OjtAmLi4Ox48fF02RJCIiqikmN15Q58+fR0VFBZydnTW2+eeffwCgyjba2LNnD4qLiyGTyQAAfn5+kMvlatv6+flh//79yMrKQlZWFg4cOAA/P7+nOn51bt++jQULFsDY2Bg9evQA8GjaROVxPTw8UFBQgLS0NGGfkSNHYsyYMfDw8MDbb7+NiRMnqu07MDAQSqUSFRUV2LZtG9q1awc3Nze1bWfPng1jY2PRtm/fPlGbynU4vvvuO1y4cKEOzl4sPDwc5ubmcHR0RMeOHeHv74+EhATRFJya6tatG9q2bYtt27ahoqICSqVSqwRPZGQkzMzMhO3J5BERETV+q1atQvv27eHs7Aw9PT2EhIQgICAAOjqPvoJevnwZH374ITZt2qQywoOIiKgmmNx4QVWOQnjaNtpQKBTw9vaGVPpolpOPjw8OHDig9se5tbU1PD09oVQqERMTA09PT1hZWdVJHE/q3bs3jI2N0axZM5w4cQLx8fGwtbXFuXPncOTIEfj4+AB4NLXC29tbJSEzd+5clJeXY86cORqP4enpibt372Lv3r1QKBRV/qj/5JNPkJGRIdpee+01lXYymQxvvvlmjdY80Vbz5s1x6NAhnDx5Eh9++CHKysowceJEeHh4PFWCo3JETlpaGoqKijBkyJBq9wkLC0NBQYGwXb58udbHJyKiZ8/Kygq6urq4fv26qPz69esap0haW1tjx44dKCoqQlZWFs6ePQtjY2O0bdsWAHDs2DHcuHEDr776KqRSKaRSKdLS0vD1119DKpXi4cOHz/y8iIjoxcA1N15Q7du3h0QiwdmzZzW26dChAwDg7NmzwnoJNZWfn4/t27fjwYMHWLt2rVD+8OFDKBQKLFq0SGWfwMBAhISEAHj0OLlnJT4+Hp07d4alpaVoio1cLkdZWRlatGghlFVUVEBfXx/ffPMNzMzMAEBI1lT+rzpSqRTjx49HeHg4Dh8+LFq75ElWVlZwcnLSKvaoqCj06tULn3zyiVbta6pLly7o0qULpk2bhqCgILz11ltIS0tD//79YWpqqnbayu3bt6Grq6t2mpOvry9mzZqF+fPnY/z48VVes0r6+vrQ19evk/MhIqJnT09PD927d0dKSorwVK3y8nKkpKQIf9c1MTAwgL29PR48eIAff/wRY8aMAQAMHDgQJ0+eFLUNCAiAs7MzZs+eDV1d3WdyLkRE9OLhyI0XlIWFBWQyGaKjo1FUVKRSf/v2bQwaNAhWVlZYunSp2j60WVB006ZNaNmyJU6cOCEakbB8+XIolUq1d1w8PDxw//59PHjwQJjK8iw4ODigXbt2osRGWVkZNmzYgOXLl4viPXHiBFq0aFGrBTADAwORlpaGESNG1NkK7z169MC7776LTz/9tE76q0rnzp0BQPicdOzYEadPn0Zpaamo3fHjx9GmTRs0adJEpQ8LCwsMHz4caWlpT7XmCBERNWyhoaFYv349YmNjcebMGUydOhVFRUXC+lkTJkwQLTh6+PBhJCYm4uLFi9i3b58wUrByvS8TExMh4V65NW3aFJaWlujSpUu9nCMRETVOHLnxAouOjkafPn3Qo0cPRERE4JVXXkFZWRmSk5Oxdu1anDlzBt9//z1Gjx6N4cOH44MPPoCTkxPy8vKQkJCA7OxsxMXFVXkMuVyO9957T+ULiIODA8LCwpCUlARPT09Rna6uLs6cOSP8+3natWsXbt26hUmTJgkjNCqNGjUKcrkcQUFBNeqzU6dOyMvLg5GRUZXt7ty5o7LgmpGREUxNTdW2X7RoEVxcXLQaBaGtqVOnokWLFhgwYABatmyJnJwcLFy4ENbW1sLoHV9fX0RERGDChAmYNWsWzMzMsHfvXqxcuVJjIgx4tJDomjVrYGlpWWfxEhFRw+Lt7Y2bN29i3rx5yM3NhZubG5KSkoRFRrOzs4X1NIBHj3yfM2cOLl68CGNjYwwZMgQ//PCD1ouWExERaYsjN15gbdu2xfHjx9G/f3989NFH6NKlC9555x2kpKQIU0hGjBiBgwcPokmTJhg3bhycnZ3h4+ODgoKCah/FduzYMZw4cQKjRo1SqTMzM8PAgQM1Lixqamqq8Uf9sySXy+Hu7q6S2AAeJTf++OMP/Pe//61xv5aWliqPvn3SvHnz0Lx5c9Gm6Uk1wKNpQ4GBgbh3716N49HE3d0d6enpGD16NDp06IBRo0bBwMAAKSkpQlLC3Nwc+/btw4MHDzB8+HC4ubnh66+/xooVK/Cvf/1LY9+GhoZMbBARvQRCQkKQlZWF0tJSHD58GD179hTqUlNToVQqhddvv/02/vrrL9y7dw95eXnYsGGDaFqoOqmpqVi5cuUzip6IiF5Ukoq6WlWSiOgpFBYWPnpqyowE6OhXPQqGiIiercwoz+obERERPSOVvw0KCgq0vinOkRtERERERERE1KgxuUFVCgoKgrGxsdqtpmtTvMgxNRSDBw/WeG0WL15c3+ERERERERE9E5yWQlW6ceMGCgsL1daZmprCxsbmOUfUMGNqKK5evYqSkhK1dRYWFrCwsHjOEWmP01KIiBoOTkshIqL6VJtpKXxaClXJxsamwSULGmJMDYW9vX19h0BERERERPTccVoKERERERERETVqHLlBRA3KqS9k9fKYYCIiIiIiarw4coOIiIiIiIiIGjUmN4iIiIiIiIioUWNyg4iIiIiIiIgaNSY3iIiIiIiIiKhRY3KDiIiIiIiIiBo1JjeIiIiIiIiIqFHjo2CJqEHpEr4bOvpG9R0GEdELLzPKs75DICIiqjMcuUFEREREREREjRqTG0RERERERETUqDG5QURERERERESNGpMbRERERERERNSoMblBRERERERERI0akxtEREREJIiOjoajoyMMDAzQs2dPHDlyRGPbBw8eICIiAu3atYOBgQFcXV2RlJQkarN3714MGzYMLVq0gEQiwY4dO57xGRAR0cuIyQ0iIiIiAgDEx8cjNDQU4eHhOH78OFxdXSGTyXDjxg217efMmYNvv/0Wq1evxl9//YWgoCB4eXnhzz//FNoUFRXB1dUV0dHRz+s0iIjoJVSr5EZubi6mT5+Otm3bQl9fHw4ODhg2bBhSUlIAAI6Ojli5cqXG/S9fvozAwEC0aNECenp6aN26NT788EP873//E7W7dOkSxo0bhxYtWsDAwAAtW7bEiBEjcPbsWaGNRCJRu8XFxQEAUlNThTIdHR2YmZmhW7dumDVrFnJycrQ+5/nz50MikcDDw0Ol7ssvv4REIkG/fv1E5fn5+ZgxYwZat24NPT09tGjRAoGBgcjOzha18/f3h0QiQVRUlKh8x44dkEgkojaaNkdHRwBAv379MGPGDJUYlUolzM3NhdcPHz5EVFQUnJ2dYWhoCAsLC/Ts2RPff/+9VtfjyXgsLS3h4eGB//73v2rb/+tf/4Kuri62bt2qUjd//ny4ublVe8ySkhJYWFjAysoKpaWlKvWOjo6QSCRIT08Xlc+YMUPlvSksLMTnn38OZ2dnGBgYwM7ODu7u7khMTERFRQWAR9dS3bUOCgqqNlYASEtLw4ABA2BhYQEjIyO0b98eEydOxP379wH832fz9u3bovg1bVlZWc8lric/K497/I5bZmYmJBIJdHV1cfXqVVG7nJwcSKVSSCQSZGZmahUXERHVvxUrVmDy5MkICAhA586dsW7dOhgZGUGhUKht/8MPP+Czzz7DkCFD0LZtW0ydOhVDhgzB8uXLhTaDBw/GwoUL4eXl9bxOg4iIXkI1Tm5kZmaie/fu+O233/Dll1/i5MmTSEpKQv/+/REcHFzt/hcvXsRrr72Gf/75B1u2bMH58+exbt06pKSkoFevXsjPzwfwaJjjO++8g4KCAiQmJuLcuXOIj49H165dhR+DlWJiYpCTkyPaRo4cKWpz7tw5XLt2DUePHsXs2bOxZ88edOnSBSdPntT63Js3b47ff/8dV65cEZUrFAq0atVKVJafn4833ngDe/bswbp163D+/HnExcXh/PnzeP3113Hx4kVRewMDAyxZsgS3bt1Se+xVq1aJzu/J8z569KjW5wEAX3zxBb766issWLAAf/31F37//XdMmTJF5dpWxcPDQzh+SkoKpFIphg4dqtKuuLgYcXFxmDVrlsYvR9r48ccf4eLiAmdnZ41DWg0MDDB79uwq+7l9+zZ69+6NDRs2ICwsDMePH8fevXvh7e2NWbNmoaCgQGg7efJklc/W0qVLq431r7/+goeHB1577TXs3bsXJ0+exOrVq6Gnp4eHDx+q3efo0aMqxzpz5gxatGiBYcOGiT5jzzOu6tjb22PDhg2istjYWNjb29eqPyIiqh/379/HsWPH4O7uLpTp6OjA3d0dhw4dUrtPaWkpDAwMRGWGhobYv3//M42ViIjoSdKa7jBt2jRIJBIcOXIETZs2FcpdXFwQGBhY7f7BwcHQ09PDr7/+CkNDQwBAq1at0K1bN7Rr1w6ff/451q5di9OnT+PChQtISUlB69atAQCtW7dGnz59VPo0NzeHnZ1dlce1sbER2nXo0AEjRoxAt27dMHXqVK3/ANvY2KB79+6IjY3F559/DgA4ePAg8vLyMHr0aPz1119C288//xzXrl3D+fPnhdhatWqF3bt3o3379ggODsZ//vMfob27uzvOnz+PyMhItT9SzczMYGZmVuPz1mTnzp2YNm0aRo8eLZS5urrWqA99fX3h+HZ2dvj000/x1ltv4ebNm7C2thbabd26FZ07d8ann36KFi1a4PLly3BwcKhxzHK5HH5+fqioqIBcLoe3t7dKmylTpmDdunX45ZdfMGTIELX9fPbZZ8jMzMTff/+NFi1aCOUdOnSAj4+P6EuakZFRra7xr7/+Cjs7O9F72a5dO7Ujfyo9fs0AoLy8HP7+/jAzM8OmTZuEUTzPO67qTJw4ETExMQgLCxPKYmJiMHHiRCxYsKDW/RIR0fOVl5eHhw8fwtbWVlRua2srGjX7OJlMhhUrVqBv375o164dUlJSkJiYWOuEORERUW3VaORGfn4+kpKSEBwcLEpsVNI0lP3x/Xfv3o1p06YJiY1KdnZ28PX1RXx8PCoqKmBtbQ0dHR1s27btmfyBNDQ0RFBQEA4cOKBxHqk6gYGBUCqVwmuFQgFfX1/o6ekJZeXl5YiLi4Ovr6/KD1BDQ0NMmzYNu3fvFkapAICuri4WL16M1atXq4wMeRbs7Ozw22+/4ebNm3XS3927d7Fx40Y4OTnB0tJSVFeZlDAzM8PgwYNF109bFy5cwKFDhzBmzBiMGTMG+/btE6ZpPK5NmzYICgpCWFgYysvLVeoff28eT2xUMjY2hlRa45yfCjs7O+Tk5GDv3r217uPTTz/F4cOH8e9//xsmJiZPHVNdxfWk4cOH49atW0KScP/+/bh16xaGDRtW5X6lpaUoLCwUbURE1LisWrUK7du3h7OzM/T09BASEoKAgADo6HBZNyIier5q9Jfn/PnzqKiogLOzc60O9s8//6CiogKdOnVSW9+pUyfcunULN2/ehL29Pb7++mvMmzcPzZo1w4ABA7BgwQKV6RwA4OPjA2NjY9H25LoW6lSeR03WBBg6dCgKCwuxd+9eFBUVISEhQWXEys2bN3H79u0qz7OiogLnz58XlXt5ecHNzQ3h4eFax6POmjVrVK7Hk+sxrFixAjdv3oSdnR1eeeUVBAUFiUaSaGPXrl1C/yYmJti5cyfi4+NFX2j++ecfpKenC6Ms/Pz8EBMTI6xroS2FQoHBgwejWbNmsLCwgEwmQ0xMjNq2c+bMwaVLl7Bp0yaVury8PNy6dUvrz7C6a6mu3yeNHj0aPj4+ePvtt9G8eXN4eXnhm2++0foH/JYtW7BixQrExcWhffv2DSYudZo0aQI/Pz9hypFCoYCfnx+aNGlS5X6RkZHCiCQzM7NajeYhIqK6Y2VlBV1dXVy/fl1Ufv36dY2jBa2trbFjxw4UFRUhKysLZ8+ehbGxMdq2bfs8QiYiIhLUKLlR0x+kT9tPcHAwcnNzsWnTJvTq1Qtbt26Fi4sLkpOTRe2++uorZGRkiDZ1d+U1xfH4cP/qVP6Qi4mJwdatW9GhQwe88sorVfZfE0uWLEFsbCzOnDlT430r+fr6qlyPiIgIUZvOnTvj1KlTSE9PR2BgIG7cuIFhw4bh/fff1/o4/fv3F/o/cuQIZDIZBg8eLBpRoVAoIJPJYGVlBQAYMmQICgoK8Ntvv2l9nIcPHyI2NhZ+fn5CmZ+fH5RKpdrRGdbW1vj4448xb948YZHMSjV9T9Rdy+HDh1e7n66uLmJiYnDlyhUsXboU9vb2WLx4MVxcXKpdyPb48eOYNGkSoqKiIJPJGkxcVQkMDMTWrVuRm5uLrVu3ajVFLSwsDAUFBcJ2+fLlWh+fiIienp6eHrp37y4sEA88GvFYuS5aVQwMDGBvb4+ysjL8+OOPGDFixLMOl4iISKRGyY327dtDIpFonHdZHScnJ0gkEo0/3M+cOYNmzZqJ1h4wMTHBsGHDsGjRIpw4cQJvvfUWFi5cKNrPzs4OTk5Ook2bqQWVcVQ+aURblT/koqOj1f6Is7a2hrm5eZXnKZFI4OTkpFLXt29fyGQy0foFNWVmZqZyPWxsbFTa6ejo4PXXX8eMGTOQmJgIpVIJuVyOS5cuaXWcpk2bCv2//vrr+P7771FUVIT169cD+L+kxM8//wypVAqpVAojIyPk5+fXaGHR3bt34+rVq/D29hb6GTt2LLKyskRfwB4XGhqKkpISrFmzRlRe+d5o+xlWdy1rMkXE3t4e48ePxzfffIPTp0/j3r17WLduncb2N2/ehJeXF0aNGoWPP/64XuIyNTVFUVGRSuKocrHZJ9d+AYCuXbvC2dkZPj4+6NSpE7p06VJtDPr6+jA1NRVtRERUv0JDQ7F+/XrhRsvUqVNRVFSEgIAAAMCECRNE31EOHz6MxMREXLx4Efv27YOHhwfKy8sxa9Ysoc3du3eFRDzw6Gl4GRkZWo2yJSIi0laNkhuV0wGio6NRVFSkUl/dkzYsLS3xzjvvYM2aNSgpKRHVVY7Q8Pb21jiSQiKRwNnZWe2xa6qkpATfffcd+vbtq7KQY3VcXFzg4uKCU6dOYdy4cSr1Ojo6GDNmDDZv3ozc3FyV465ZswYymQwWFhZq+4+KisJPP/2kcWXyZ6Vz584AUOvrW/m43cr39pdffsGdO3fw559/ikYYbNmyBYmJiVo/mUUul2Ps2LEqIxXGjh0LuVyudh9jY2PMnTsXixYtwp07d4RyHR0djB07Fps2bcK1a9dU9rt79y7KyspqfvJaaNasGZo3b67x+j548ADvvfcebGxshATR8/BkXB07dkRZWZnwJbTS8ePHATxaeFWdwMBApKamajVqg4iIGiZvb28sW7YM8+bNg5ubGzIyMpCUlCQsMpqdnS0a6Xfv3j3MmTMHnTt3hpeXF+zt7bF//37ROmx//PEHunXrhm7dugF4lEDp1q0b5s2b91zPjYiIXmw1XjkxOjoaffr0QY8ePRAREYFXXnkFZWVlSE5Oxtq1a4XRClevXlX5cdS6dWt888036N27N2QyGRYuXIg2bdrg9OnT+OSTT2Bvb49FixYBADIyMhAeHo7x48ejc+fO0NPTQ1paGhQKhcqjPm/fvq2SRDAxMREtenrjxg3cu3cPd+7cwbFjx7B06VLk5eUhMTGxppcAAPDbb7/hwYMHGhdRXbx4MVJSUvDOO+9g6dKl6NKlCy5duoQ5c+bgwYMHiI6O1th3165d4evri6+//rpWsWnjvffeQ58+fdC7d2/Y2dnh0qVLCAsLQ4cOHbRej6K0tFS47rdu3cI333yDu3fvCgtJyuVyeHp6qjyFpXPnzpg5cyY2bdokPD64pKRE5fNiYmICU1NT/PTTT9i5c6fKaIAJEybAy8sL+fn5ahNFU6ZMwVdffYXNmzejZ8+eQvmiRYuQmpqKnj17YtGiRXjttdfQpEkT7Nu3D5GRkTh69KjwvhYXF6t8tvT19dGsWbMqr823336LjIwMeHl5oV27drh37x42bNiA06dPY/Xq1Wr3mTFjBk6cOIE9e/aoTfxYWFgIC9c+y7hcXFwwaNAgBAYGYvny5Wjbti3OnTuHGTNmwNvbW+MjXidPnozRo0dXu7AwERE1bCEhIQgJCVFbl5qaKnr99ttvi54Wp06/fv3qbGozERGRJjVObrRt2xbHjx/HokWL8NFHHyEnJwfW1tbo3r071q5dK7RbtmwZli1bJtr3hx9+gJ+fH/744w+Eh4djzJgxyM/Ph52dHUaOHInw8HDhR2rLli3h6OiIL774ApmZmZBIJMLrmTNnivqtHCr5uMjISHz66afC644dO0IikQiLXA0aNAihoaG1fpSquqfFPM7S0hLp6emIiIjAv/71L+Tm5sLCwgKDBw/Gxo0b0apVqyr3j4iIQHx8fK1i04ZMJsOWLVsQGRmJgoIC2NnZYcCAAZg/f77WTwtJSkpC8+bNATxKRDg7O2Pr1q3o168frl+/jp9//hmbN29W2U9HRwdeXl6Qy+VCcuPvv/8W7uhUGjhwIAYPHoymTZti4MCBKv0MHDgQhoaG2LhxIz744AOV+iZNmmDBggUqo2ssLCyQnp6OqKgoLFy4EFlZWWjWrBm6du2KL7/8UjTtYv369SqjKGQyGZKSkqq8Nj169MD+/fsRFBSEa9euwdjYGC4uLtixYwfefvtttftUTqF5/fXX1db//vvv6Nev33OJKz4+HuHh4fjXv/6Fa9euoWXLlvDy8sLcuXM19i2VSoW1VYiIiIiIiJ4nSQVT6UTUABQWFj56asqMBOjoG9V3OEREL7zMKM/6DoGIiEityt8GBQUFWq/Nx4eQExEREREREVGjxuTG/2dsbKxx27dvX32H91xlZ2dXeT24uvmjNVU0XZ/BgwczLiIiIiIioueI01L+v/Pnz2uss7e3h6Gh4XOMpn6VlZUhMzNTY72jo6PW63K8qPLz85Gfn6+2ztDQUOOim89aQ41LG5yWQkT0fHFaChERNVS1mZbycv9CfYyTk1N9h9BgSKVSXo9qWFhYaHyUb31qqHERERERERE9S5yWQkRERERERESNGkduEFGDcuoLmdZDz4iIiIiIiACO3CAiIiIiIiKiRo7JDSIiIiIiIiJq1JjcICIiIiIiIqJGjckNIiIiIiIiImrUmNwgIiIiIiIiokaNT0shogalS/hu6Ogb1XcYRETPRGaUZ32HQERE9ELiyA0iIiIiIiIiatSY3CAiIiIiIiKiRo3JDSIiIiIiIiJq1JjcICIiIiIiIqJGjckNIiIiIiIiImrUmNwgIiIiIiIiokaNyQ0iIiKiehYdHQ1HR0cYGBigZ8+eOHLkSJXtV65ciY4dO8LQ0BAODg6YOXMm7t27J9RHRkbi9ddfh4mJCWxsbDBy5EicO3fuWZ8GERFRvWFyg4iIiKgexcfHIzQ0FOHh4Th+/DhcXV0hk8lw48YNte03b96MTz/9FOHh4Thz5gzkcjni4+Px2WefCW3S0tIQHByM9PR0JCcn48GDBxg0aBCKioqe12kRERE9V0xuPMbf3x8jR44U/i2RSBAVFSVqs2PHDkgkElHZ+vXr4erqCmNjY5ibm6Nbt26IjIwEADg6OkIikWjc/P39hX5kMhl0dXVx9OjRKmOrrcjISOjq6uLLL79UqVMqlZBIJOjUqZNK3datWyGRSODo6AgA6NevX5Xn1K9fv2pjcXR0xMqVK0WvJRIJ0tPTRe1mzJgh6q+4uBhhYWFo164dDAwMYG1tjbfffhv//ve/kZmZWWVcEokESqUSAFBSUgILCwtYWVmhtLS02vgeV3kcXV1dXL16VVSXk5MDqVQKiUSCzMxMUXt1W+X5Vl5/Dw8PUX+3b9+GRCJBamqq0KaqrfKYmlR1/ao79/nz58PNzU14Xfn/kaCgIJW2wcHBKp9vIiJSb8WKFZg8eTICAgLQuXNnrFu3DkZGRlAoFGrbHzx4EH369MG4cePg6OiIQYMGwcfHRzTaIykpCf7+/nBxcYGrqyuUSiWys7Nx7Nix53VaREREzxWTG1UwMDDAkiVLcOvWLY1tFAoFZsyYgQ8++AAZGRk4cOAAZs2ahbt37wIAjh49ipycHOTk5ODHH38EAJw7d04oW7VqFQAgOzsbBw8eREhIiMYvM09LoVBg1qxZGvtv2rQpbty4gUOHDonK5XI5WrVqJbxOTEwU4q/8IrVnzx6hLDExsVbxGRgYYPbs2VW2CQoKQmJiIlavXo2zZ88iKSkJ7733Hv73v//BwcFBiCEnJwcfffQRXFxcRGXe3t4AgB9//BEuLi5wdnbGjh07ahWvvb09NmzYICqLjY2Fvb292vaPX6PKrXv37kK9VCrFnj178Pvvv6vd39vbW7Rvr169MHnyZFGZg4NDlTFXdf1qw8HBAXFxcSgpKRHK7t27h82bN4s+M0REpN79+/dx7NgxuLu7C2U6Ojpwd3dX+XtcqXfv3jh27JjwN/jixYv45ZdfMGTIEI3HKSgoAABYWFjUYfREREQNh7S+A2jI3N3dcf78eURGRmLp0qVq2+zcuRNjxozBpEmThDIXFxfh39bW1sK/K79Q2NjYwNzcXNRPTEwMhg4diqlTp+KNN97AihUrYGhoWGfnkpaWhpKSEkRERGDDhg04ePAgevfuLWojlUoxbtw4KBQK9OrVCwBw5coVpKamYubMmdiyZYvoPAAI83stLS1hZ2f3VDFOmTIF69atq/IL2s6dO7Fq1Sqh3tHRUZQgeDwGY2NjSKVStXHJ5XL4+fmhoqICcrlcSHrUxMSJExETE4OwsDChLCYmBhMnTsSCBQtU2ld3jZo2bYoxY8bg008/xeHDh1XqDQ0NRZ8JPT09GBkZ1ei6V3f9aurVV1/FhQsXkJiYCF9fXwCPkl+tWrVCmzZtat0vEdHLIi8vDw8fPoStra2o3NbWFmfPnlW7z7hx45CXl4c333wTFRUVKCsrQ1BQkGhayuPKy8sxY8YM9OnTB126dKnzcyAiImoIOHKjCrq6uli8eDFWr16NK1euqG1jZ2eH9PR0ZGVl1fo4FRUViImJgZ+fH5ydneHk5IRt27bVuj915HI5fHx80KRJE/j4+EAul6ttFxgYiISEBBQXFwN4NF3Cw8ND5UvXs9CmTRsEBQUhLCwM5eXlatvY2dnhl19+wZ07d2p9nAsXLuDQoUMYM2YMxowZg3379tXq/Rs+fDhu3bqF/fv3AwD279+PW7duYdiwYbWObf78+Th58mSdv/+V6uL6PSkwMBAxMTHCa4VCgYCAgGr3Ky0tRWFhoWgjIqLqpaamYvHixVizZg2OHz+OxMRE/Pzzz2oT68CjqYKnTp1CXFzcc46UiIjo+WFyoxpeXl5wc3NDeHi42vrw8HCYm5vD0dERHTt2hL+/PxISEjT+OFdnz549KC4uhkwmAwD4+flpTD7URmFhIbZt2wY/Pz+h/4SEBGHqzOO6deuGtm3bYtu2baioqIBSqURgYGCdxVKdOXPm4NKlS9i0aZPa+u+++w4HDx6EpaUlXn/9dcycORMHDhyo0TEUCgUGDx6MZs2awcLCAjKZTPTjXFtNmjSBn5+fMM1HoVDAz88PTZo0Udu+d+/eMDY2Fm1PatGiBT788EN8/vnnKCsrq3FM1amL6/ckPz8/7N+/H1lZWcjKysKBAweEz1pVIiMjYWZmJmzVTakhInoRWVlZQVdXF9evXxeVX79+XePIvLlz52L8+PF4//330bVrV3h5eWHx4sWIjIxU+f4REhKCXbt24ffff0fLli2f2XkQERHVNyY3tLBkyRLExsbizJkzKnXNmzfHoUOHcPLkSXz44YcoKyvDxIkT4eHhoXWCQ6FQwNvbG1Lpo1lCPj4+OHDgAC5cuFAn8W/ZsgXt2rWDq6srAMDNzQ2tW7dGfHy82vaVd+LT0tJQVFRU5RzeumZtbY2PP/4Y8+bNw/3791Xq+/bti4sXLyIlJQXvvfceTp8+jbfeekvj3aonPXz4ELGxsaIf335+flAqlTVKSFUKDAzE1q1bkZubi61bt1aZCIqPj0dGRoZoU2f27Nm4efPmM1l75WmvnzrW1tbw9PSEUqlETEwMPD09YWVlVe1+YWFhKCgoELbLly/XOgYiosZKT08P3bt3R0pKilBWXl6OlJQUYYrok4qLi6GjI/4Kp6urC+DRaNDK/w0JCcH27dvx22+/caogERG98Jjc0ELfvn0hk8lEays8qUuXLpg2bRo2btyI5ORkJCcnIy0trdq+8/PzsX37dqxZswZSqRRSqRT29vYoKyursx+3crkcp0+fFvqXSqX466+/NPbv6+uL9PR0zJ8/H+PHjxeSLs9LaGgoSkpKsGbNGrX1TZo0wVtvvYXZs2fj119/RUREBBYsWKA2GfKk3bt34+rVq0IySSqVYuzYscjKyhJ9sdRW165d4ezsDB8fH3Tq1KnKucwODg5wcnISbeqYm5sjLCwMX3zxhTA9qC5Vd/1MTU2Fheced/v2bZiZmantMzAwEEqlErGxsVqP9NHX14epqaloIyJ6GYWGhmL9+vXCjZSpU6eiqKhImOI3YcIE0XeQYcOGYe3atYiLi8OlS5eQnJyMuXPnYtiwYUKSIzg4GBs3bsTmzZthYmKC3Nxc5ObmihaAJiIiepFwQVEtRUVFwc3NDR07dqy2befOnQFAq2fJb9q0CS1btlR5Ysevv/6K5cuXIyIiQviiUhsnT57EH3/8gdTUVNFCoPn5+ejXrx/Onj0LZ2dn0T4WFhYYPnw4EhISsG7dulofu7aMjY0xd+5czJ8/H8OHD6+2fefOnVFWVoZ79+5BT0+vyrZyuRxjx47F559/LipftGgR5HI53nnnnRrHGxgYiGnTpmHt2rU13leT6dOn4+uvvxaepvMsPXn9OnbsqPZRgcePH9f4+ffw8MD9+/chkUiE6VVERKQdb29v3Lx5E/PmzUNubi7c3NyQlJQkrHeVnZ0tGqkxZ84cSCQSzJkzB1evXoW1tTWGDRuGRYsWCW0q/yY9+Xj2mJgYPqabiIheSExuaKlr167w9fXF119/LSqfOnUqWrRogQEDBqBly5bIycnBwoULYW1trXE46ePkcjnee+89lTv+Dg4OCAsLQ1JSEjw9PQE8eozbk1MZLC0tq1yrQC6Xo0ePHujbt69K3euvvw65XI4vv/xSpU6pVGLNmjWwtLSs9hyehSlTpuCrr77C5s2b0bNnT6G8X79+8PHxwWuvvQZLS0v89ddf+Oyzz9C/f/9q7/zfvHkTP/30E3bu3KlyvSdMmAAvLy/k5+cLSaCrV6+qXO/WrVur9Dt58mSMHj1a5Qk4T/rf//6H3NxcUZm5uTkMDAxU2hoYGOCLL75AcHBwlX3WlDbXb+bMmXjrrbewaNEivPvuu3j48CG2bNmCQ4cOaRxNo6urK0zbeppkHBHRyyokJAQhISFq61JTU0WvpVIpwsPDNa4HBvzf9BQiIqKXBael1EBERITKugzu7u5IT0/H6NGj0aFDB4waNQoGBgZISUmpNjFw7NgxnDhxAqNGjVKpMzMzw8CBA0ULi6ampqJbt26i7YsvvtDY//3797Fx40a1/QPAqFGjsGHDBjx48EClztDQsN4SG8CjqRMLFiwQHjVbSSaTITY2FoMGDUKnTp0wffp0yGQyJCQkVNvnhg0b0LRpUwwcOFClbuDAgTA0NMTGjRuFsmXLlqlc759//lllX6lUCisrq2qn77i7u6N58+ai7ckRO4+bOHEi2rZtW+151YQ216937974z3/+g//85z/o06cP+vXrh4MHDyIlJaXKaTecWkJERERERPVFUsHUPhE1AIWFhY+emjIjATr6RvUdDhHRM5EZ5VnfIRARETV4lb8NCgoKtL6BypEbRERERERERNSoMbnRyG3atAnGxsZqNxcXF8b0ktF03Y2NjbFv3776Do+IiIiIiOiZ4IKijdzw4cNFC24+rkmTJs85mkcaYkwviycXQH2cvb398wuEiIiIiIjoOWJyo5EzMTGBiYlJfYch0hBjelk4OTnVdwhERERERETPHaelEBEREREREVGjxpEbRNSgnPpCxkfKEhERERFRjXDkBhERERERERE1akxuEBEREREREVGjxuQGERERERERETVqTG4QERERERERUaPG5AYRERERERERNWp8WgoRNShdwndDR9+ovsMgInomMqM86zsEIiKiFxJHbhARERERERFRo8bkBhERERERERE1akxuEBEREREREVGjxuQGERERERERETVqTG4QERERERERUaPG5AYRERERERERNWpMbhARERHVs+joaDg6OsLAwAA9e/bEkSNHqmy/cuVKdOzYEYaGhnBwcMDMmTNx7949oT4yMhKvv/46TExMYGNjg5EjR+LcuXPP+jSIiIjqDZMbRERERPUoPj4eoaGhCA8Px/Hjx+Hq6gqZTIYbN26obb9582Z8+umnCA8Px5kzZyCXyxEfH4/PPvtMaJOWlobg4GCkp6cjOTkZDx48wKBBg1BUVPS8TouIiOi5YnKjnuXm5mL69Olo27Yt9PX14eDggGHDhiElJUVo8+eff2L06NGwtbWFgYEB2rdvj8mTJ+Pvv/+u0bFkMhl0dXVx9OhRlTp/f39IJBIEBQWp1AUHB0MikcDf3x8AIJFIqtzmz59fZRyZmZmQSCTIyMgQvbaxscGdO3dEbd3c3ET9Xbp0CePGjUOLFi1gYGCAli1bYsSIETh79iyUSmW1sWVmZgIADh06BF1dXXh6elYb35Mqj9OpUyeVuq1bt0IikcDR0VGl/ZObgYGB0Kby+kdFRYn627FjByQSiaiNpu3xY2pS1fWr7tz79euHGTNmCK8dHR0hkUgQFxen0tbFxQUSiQRKpbLamIiIXnYrVqzA5MmTERAQgM6dO2PdunUwMjKCQqFQ2/7gwYPo06cPxo0bB0dHRwwaNAg+Pj6i0R5JSUnw9/eHi4sLXF1doVQqkZ2djWPHjj2v0yIiInqumNyoR5mZmejevTt+++03fPnllzh58iSSkpLQv39/BAcHAwB27dqFN954A6Wlpdi0aRPOnDmDjRs3wszMDHPnztX6WNnZ2Th48CBCQkI0fllycHBAXFwcSkpKhLJ79+5h8+bNaNWqlVCWk5MjbCtXroSpqamo7OOPP67V9bhz5w6WLVumsf7Bgwd45513UFBQgMTERJw7dw7x8fHo2rUrbt++DW9vb1EcvXr1wuTJk0VlDg4OAAC5XI7p06dj7969uHbtWo1jbdq0KW7cuIFDhw6JyuVyuehaVXryGuXk5CArK0vUxsDAAEuWLMGtW7fUHnPVqlWi/QEgJiZGeK0uafW46q5fbTg4OCAmJkZUlp6ejtzcXDRt2rRWfRIRvUzu37+PY8eOwd3dXSjT0dGBu7u7yt+YSr1798axY8eEZMbFixfxyy+/YMiQIRqPU1BQAACwsLCow+iJiIgaDml9B/AymzZtGiQSCY4cOSL6Ieji4oLAwEAUFxcjICAAQ4YMwfbt24X6Nm3aoGfPnjX6QRoTE4OhQ4di6tSpeOONN7BixQoYGhqK2rz66qu4cOECEhMT4evrCwBITExEq1at0KZNG6GdnZ2d8G8zMzNIJBJRWW1Nnz4dK1asQHBwMGxsbFTqT58+jQsXLiAlJQWtW7cGALRu3Rp9+vQR2jx+Tnp6ejAyMlKJ7e7du4iPj8cff/yB3NxcKJVK0VBebUilUowbNw4KhQK9evUCAFy5cgWpqamYOXMmtmzZImqvzTVyd3fH+fPnERkZiaVLl6rUm5mZwczMTFRmbm6u9bXX5vrVlK+vL7766itcvnxZSBwpFAr4+vpiw4YNVe5bWlqK0tJS4XVhYWGt4yAiaqzy8vLw8OFD2NraisptbW2FUXVPGjduHPLy8vDmm2+ioqICZWVlCAoK0vi3rLy8HDNmzECfPn3QpUuXOj8HIiKihoAjN+pJfn4+kpKSEBwcrPYOt7m5OXbv3o28vDzMmjVLbR/m5uZaHauiogIxMTHw8/ODs7MznJycsG3bNrVtAwMDRXfiFQoFAgICtDrO0/Lx8YGTkxMiIiLU1ltbW0NHRwfbtm3Dw4cPa32chIQEODs7o2PHjvDz84NCoUBFRUWN+wkMDERCQgKKi4sBPJp+4uHhofIFVVu6urpYvHgxVq9ejStXrtSqj6rU1fV7nK2tLWQyGWJjYwEAxcXFiI+PR2BgYLX7RkZGCgkbMzMzITlCRERVS01NxeLFi7FmzRocP34ciYmJ+Pnnn7FgwQK17YODg3Hq1Cm10wiJiIheFExu1JPz58+joqICzs7OGtv8888/AFBlG23s2bMHxcXFkMlkAAA/Pz/I5XK1bf38/LB//35kZWUhKysLBw4cgJ+f31MdX1uVa0589913uHDhgkq9vb09vv76a8ybNw/NmjXDgAEDsGDBAly8eLFGx5HL5cI5eXh4oKCgAGlpaTWOt1u3bmjbti22bduGiooKKJVKjT/qCwoKYGxsLNoGDx6s0s7Lywtubm4IDw+vcTzVqavr96TAwEAolUpUVFRg27ZtaNeuHdzc3KrdLywsDAUFBcJ2+fLlp4qDiKgxsrKygq6uLq5fvy4qv379usaReXPnzsX48ePx/vvvo2vXrvDy8sLixYsRGRmJ8vJyUduQkBDs2rULv//+O1q2bPnMzoOIiKi+MblRT7QZKVCb0QTqKBQKeHt7Qyp9NAvJx8cHBw4cUJtAsLa2hqenJ5RKJWJiYuDp6QkrK6s6iUMbMpkMb775psb1RIKDg5Gbm4tNmzahV69e2Lp1K1xcXJCcnKxV/+fOncORI0fg4+MD4NH0Em9vb43JnupUjnRJS0tDUVGRxvnOJiYmyMjIEG3ff/+92rZLlixBbGwszpw5U6uYqvK0108dT09P3L17F3v37oVCodBq1AYA6Ovrw9TUVLQREb1s9PT00L17d9FC4uXl5UhJSRGmPT6puLgYOjrir3C6uroA/u+7Q0VFBUJCQrB9+3b89ttvoumlRERELyImN+pJ+/btIZFINM6nBYAOHToAQJVtqpOfn4/t27djzZo1kEqlkEqlsLe3R1lZmcaFRSvvxMfGxmr9Q7UuRUVFIT4+Hn/++afaehMTEwwbNgyLFi3CiRMn8NZbb2HhwoVa9S2Xy1FWVoYWLVoI12Pt2rX48ccfhcXWasLX1xfp6emYP38+xo8fLySQnqSjowMnJyfRZm9vr7Zt3759IZPJEBYWVuN4tFHV9atMMKi7Frdv31ZZ8wN4lCAaP348wsPDcfjwYWG9FiIi0k5oaCjWr18vJLanTp2KoqIiYVrohAkTRH8Thg0bhrVr1yIuLg6XLl1CcnIy5s6di2HDhglJjuDgYGzcuBGbN2+GiYkJcnNzkZubK1o0nIiI6EXC5EY9sbCwgEwmQ3R0tNpnzt++fRuDBg2ClZWV2sUlK9tUZ9OmTWjZsiVOnDghGjWwfPlyKJVKtWsveHh44P79+3jw4IEwleV56tGjB9599118+umn1baVSCRwdnZWew2fVFZWhg0bNmD58uWia3HixAm0aNFCZRFQbVhYWGD48OFIS0ur00RQVFQUfvrpJ40r5deVJ6+fhYUFrKysVB4VWFhYiPPnzwsJtycFBgYiLS0NI0aMQLNmzZ5pzERELxpvb28sW7YM8+bNg5ubGzIyMpCUlCSs4ZSdnS08JQsA5syZg48++ghz5sxB586dMWnSJMhkMnz77bdCm7Vr16KgoAD9+vVD8+bNhS0+Pv65nx8REdHzwKel1KPo6Gj06dMHPXr0QEREBF555RWUlZUhOTkZa9euxZkzZ/D9999j9OjRGD58OD744AM4OTkhLy8PCQkJyM7OrnZxMLlcjvfee09ldXQHBweEhYUhKSkJnp6eojpdXV1hSkTlHaDnbdGiRXBxcRGNhMjIyEB4eDjGjx+Pzp07Q09PD2lpaVAoFJg9e3a1fe7atQu3bt3CpEmTVEYgjBo1CnK5HEFBQULZuXPnVPpwcXFRKVMqlVizZg0sLS01HruiogK5ubkq5TY2NipDiwGga9eu8PX1xddff13lOdWEttcvNDQUixcvhq2tLd544w3873//w4IFC2BtbY13331Xbd+dOnVCXl4ejIyM6ixeIqKXSUhICEJCQtTWpaamil5LpVKEh4dXuT5TXU1tJSIiaiyY3KhHbdu2xfHjx7Fo0SJ89NFHyMnJgbW1Nbp37461a9cCAEaMGIGDBw8iMjIS48aNQ2FhIRwcHDBgwIBqp2IcO3YMJ06cwPr161XqzMzMMHDgQMjlcpXkBoB6X/+gQ4cOCAwMxHfffSeUtWzZEo6Ojvjiiy+QmZkJiUQivJ45c2a1fcrlcri7u6udWjFq1CgsXboU//3vf4VzHzt2rEo7dYteGhoaqjxW90mFhYVo3ry5SnlOTo7GBeMiIiLq9A6bttdv1qxZMDY2xpIlS3DhwgVYWFigT58++P3336s8z6qSO0RERERERM+SpIKpfSJqAAoLCx89EnZGAnT0OQKEiF5MmVGqNxSIiIhIrPK3QUFBgdY33rnmBhERERERERE1akxuNHJBQUEwNjZWuz2+fsTLHtPLYN++fRqvu7GxcX2HR0RERERE9MxwWkojd+PGDRQWFqqtMzU1hY2NzXOOqGHG9DIoKSnB1atXNdY7OTk9x2hqjtNSiOhlwGkpRERE1avNtBQuKNrI2djYNLhkQUOM6WVgaGjY4BMYREREREREzwKnpRARERERERFRo8aRG0TUoJz6QlbvjyImIiIiIqLGhSM3iIiIiIiIiKhRY3KDiIiIiIiIiBo1JjeIiIiIiIiIqFFjcoOIiIiIiIiIGjUmN4iIiIiIiIioUePTUoioQekSvhs6+kb1HQYRUZ3JjPKs7xCIiIheeBy5QURERERERESNGpMbRERERERERNSoMblBRERERERERI0akxtERERERERE1KgxuUFEREREREREjRqTG0RERERERETUqDG5QURERFQPoqOj4ejoCAMDA/Ts2RNHjhypsv3KlSvRsWNHGBoawsHBATNnzsS9e/eE+sjISLz++uswMTGBjY0NRo4ciXPnzj3r0yAiImoQ6i25kZubi+nTp6Nt27bQ19eHg4MDhg0bhpSUFKHNn3/+idGjR8PW1hYGBgZo3749Jk+ejL///rva/jMzMyGRSGBjY4M7d+6I6tzc3DB//nzhtaOjI1auXKnSx/z58+Hm5ia89vf3h0QiQVBQkErb4OBgSCQS+Pv7VxtbZV8jR45U6TsqKkrUbseOHZBIJKKy9evXw9XVFcbGxjA3N0e3bt0QGRkpnItEItG4PR6fTCaDrq4ujh49Wm18T6o8TlxcnEqdi4sLJBIJlEqlSvsnt8rz1eb9qmxT1fb4MdVJTU2FRCKBi4sLHj58KKozNzcX7S+RSLBjx45qr02/fv3UvncA4OnpCYlEIvq8Vbav3GxtbTF69GhkZWUJbao61/T0dACAUqkUynR0dNC8eXN4e3sjOzu7ymtQqWvXrmo/ywDwww8/QF9fH3l5eUJZdZ+XyliaNGmCNm3aYNasWaIv3URE9H/i4+MRGhqK8PBwHD9+HK6urpDJZLhx44ba9ps3b8ann36K8PBwnDlzBnK5HPHx8fjss8+ENmlpaQgODkZ6ejqSk5Px4MEDDBo0CEVFRc/rtIiIiOpNvSQ3MjMz0b17d/z222/48ssvcfLkSSQlJaF///4IDg4GAOzatQtvvPEGSktLsWnTJpw5cwYbN26EmZkZ5s6dq/Wx7ty5g2XLltVZ7A4ODoiLi0NJSYlQdu/ePWzevBmtWrV6qr4NDAywZMkS3Lp1S2MbhUKBGTNm4IMPPkBGRgYOHDiAWbNm4e7duwCAo0ePIicnBzk5Ofjxxx8BAOfOnRPKVq1aBQDIzs7GwYMHERISAoVCUat4HRwcEBMTIypLT09Hbm4umjZtqtI+IiJCiKNymz59uqhNVe+Xg4ODaN+PPvoILi4uojJvb2+tYr948SI2bNig5ZlWz8HBQSWxcvXqVaSkpKB58+Yq7SdPnoycnBxcu3YN//73v3H58mX4+fmptNuzZ4/KNevevbtQb2pqipycHFy9ehU//vgjzp07h9GjR2sV86RJk1Q+y5ViYmIwfPhwWFlZAdDu8+Lh4YGcnBxcvHgRX331Fb799luEh4drFQsR0ctmxYoVmDx5MgICAtC5c2esW7cORkZGGv8be/DgQfTp0wfjxo2Do6MjBg0aBB8fH9Foj6SkJPj7+8PFxQWurq5QKpXIzs7GsWPHntdpERER1Zt6SW5MmzYNEokER44cwahRo9ChQwe4uLggNDQU6enpKC4uRkBAAIYMGYKdO3fC3d0dbdq0Qc+ePbFs2TJ8++23Wh9r+vTpWLFihcY7ITX16quvwsHBAYmJiUJZYmIiWrVqhW7duj1V3+7u7rCzsxNGYaizc+dOjBkzBpMmTYKTkxNcXFzg4+ODRYsWAQCsra1hZ2cHOzs7WFhYAABsbGyEMjMzMwCPfrwOHToUU6dOxZYtW9T+wK2Or68v0tLScPnyZaFMoVDA19cXUqlUpb2JiYkQR+X2ZBKkqvdLV1dXtK+xsTGkUqmozNDQUKvYp0+fjvDwcJSWltbwrNUbOnQo8vLycODAAaEsNjYWgwYNgo2NjUp7IyMj2NnZoXnz5njjjTcQEhKC48ePq7SztLRUuWZNmjQR6iUSidBP7969MWnSJBw5cgSFhYXVxuzn54eSkhIhCVbp0qVLSE1NxaRJk4QybT4v+vr6sLOzg4ODA0aOHAl3d3ckJydXGwcR0cvm/v37OHbsGNzd3YUyHR0duLu749ChQ2r36d27N44dOyYkMy5evIhffvkFQ4YM0XicgoICABC+DxAREb3InntyIz8/H0lJSQgODlZ7d9/c3By7d+9GXl4eZs2apbYPc3NzrY/n4+MDJycnRERE1DZkFYGBgaIRCwqFAgEBAU/dr66uLhYvXozVq1fjypUratvY2dkhPT1dNIWhpioqKhATEwM/Pz84OzvDyckJ27Ztq3E/tra2kMlkiI2NBQAUFxcjPj4egYGBtY7tWbxf6syYMQNlZWVYvXp1nfSnp6cHX19f0edCqVRqdS3y8/ORkJCAnj17PlUMN27cwPbt26GrqwtdXd1q21tZWWHEiBEqdwmVSiVatmyJQYMGAajd5+XUqVM4ePAg9PT0NLYpLS1FYWGhaCMiehnk5eXh4cOHsLW1FZXb2toiNzdX7T7jxo1DREQE3nzzTTRp0gTt2rVDv379RNNSHldeXo4ZM2agT58+6NKlS52fAxERUUPz3JMb58+fR0VFBZydnTW2+eeffwCgyjbaqlwL4bvvvsOFCxeeuj/g0R3v/fv3IysrC1lZWThw4IDaKQW14eXlBTc3N43D+cPDw2Fubg5HR0d07NgR/v7+SEhIQHl5udbH2LNnD4qLiyGTyYTzkcvltYo3MDAQSqUSFRUV2LZtG9q1aydap+Rxs2fPhrGxsWjbt2+fqM2zeL/UMTIyQnh4OCIjI4U7W08rMDAQCQkJKCoqwt69e1FQUIChQ4eqbbtmzRoYGxujadOmsLS0xLlz59QORe7du7fKNXtcQUGB0I+trS1+//13jYlDdSZNmoTU1FRcunQJwKNERmxsLCZOnAgdnUf/edD287Jr1y4YGxvDwMAAXbt2xY0bN/DJJ59oPHZkZCTMzMyEzcHBQauYiYheRqmpqVi8eDHWrFmD48ePIzExET///DMWLFigtn1wcDBOnTqldm0sIiKiF9FzT25UVFTUSZuakMlkePPNN2u0VkdVrK2t4enpCaVSiZiYGHh6egprE9SFJUuWIDY2FmfOnFGpa968OQ4dOoSTJ0/iww8/RFlZGSZOnAgPDw+tExwKhQLe3t7C1BEfHx8cOHCgVskET09P3L17F3v37oVCoahypMInn3yCjIwM0fbaa6+ptKvr90uTSZMmwdLSEkuWLKmT/lxdXdG+fXts27YNCoUC48ePVzs9B3g0pScjIwMnTpzA/v374eTkhEGDBqksphofH69yzR5nYmKCjIwM/PHHH1i+fDleffVVYYqSNt555x20bNlSGHGSkpKC7Oxs0UgkbT8v/fv3R0ZGBg4fPoyJEyciICAAo0aN0njssLAwFBQUCNvj05uIiF5kVlZW0NXVxfXr10Xl169fh52dndp95s6di/Hjx+P9999H165d4eXlhcWLFyMyMlLl739ISAh27dqF33//HS1btnxm50FERNSQPPfkRvv27SGRSHD27FmNbTp06AAAVbapqaioKMTHx+PPP/9UqTM1NVV79/727dvCGhVPqhyxEBsb+1TTMNTp27cvZDIZwsLCNLbp0qULpk2bho0bNyI5ORnJyclIS0urtu/8/Hxs374da9asgVQqhVQqhb29PcrKymq1sKhUKsX48eMRHh6Ow4cPw9fXV2NbKysrODk5iTZNa2RU9X7VFalUikWLFmHVqlW4du2aSr2JiUmtPhfR0dHYtm1blZ8LMzMz4Rr06dMHcrkc//zzD+Lj40XtHBwcVK7Z43R0dODk5IROnTohNDQUb7zxBqZOnarN6Qv7+/v7IzY2FuXl5YiJiUH//v3Rtm1bADX7vDRt2hROTk5wdXWFQqHA4cOHqxwRpK+vD1NTU9FGRPQy0NPTQ/fu3UVPiCsvL0dKSgp69eqldp/i4mJhRF2lyimIlTeFKioqEBISgu3bt+O3335DmzZtntEZEBERNTzPPblhYWEBmUyG6OhotY8mu337NgYNGgQrKyssXbpUbR+3b9+u8XF79OiBd999F59++qlKXceOHdWuJH78+HEh0fIkDw8P3L9/Hw8ePBCG69elqKgo/PTTTxoXFntc586dAUCrR71t2rQJLVu2xIkTJ0SjAZYvXw6lUqnyeFRtBAYGIi0tDSNGjECzZs1qvL86Vb1fdWn06NFwcXHBF198oVKn7nPx8OFDnDhxQuPnYty4cTh58iS6dOkivC/aqPyCWpuFXR/36aefIj4+Xu3ipJoEBATg8uXLSExMxPbt20ULidb286Kjo4PPPvsMc+bMeepzIiJ6EYWGhmL9+vXCSM2pU6eiqKhIGDk3YcIE0U2OYcOGYe3atYiLi8OlS5eQnJyMuXPnYtiwYcLfkODgYGzcuBGbN2+GiYkJcnNzkZuby/8OExHRS0H9mPlnLDo6Gn369EGPHj0QERGBV155BWVlZUhOTsbatWtx5swZfP/99xg9ejSGDx+ODz74AE5OTsjLy0NCQgKys7NrNYd00aJFcHFxUZkqMHPmTLz11ltYtGgR3n33XTx8+BBbtmzBoUOHsGbNGrV96erqCtNGtFm8saa6du0KX19ffP3116LyqVOnokWLFhgwYABatmyJnJwcLFy4ENbW1hrv9jxOLpfjvffeU1lczMHBAWFhYUhKSoKnpyeAR+s5PDkNwtLSUmVthE6dOiEvLw9GRkZVHvvOnTsqC6UZGRlpvGOv6f2qa1FRUWoTVKGhoZg0aRKcnZ3xzjvvoKioCKtXr8atW7fw/vvvq+2rWbNmyMnJET3RRJ3i4mLhWly/fh0LFiyAgYGBsIhnpf/9738q18zc3BwGBgZq+3VwcICXlxfmzZuHXbt2VRlDpTZt2mDAgAGYMmUK9PX18e677wp1Nfm8PGn06NH45JNPEB0djY8//lirWIiIXhbe3t64efMm5s2bh9zcXLi5uSEpKUlYZDQ7O1s0UmPOnDmQSCSYM2cOrl69CmtrawwbNkw0FXHt2rUAgH79+omOFRMTA39//2d+TkRERPWpXh4F27ZtWxw/fhz9+/fHRx99hC5duuCdd95BSkqK8Id5xIgROHjwIJo0aYJx48bB2dkZPj4+KCgowMKFC2t13A4dOiAwMBD37t0Tlffu3Rv/+c9/8J///Ad9+vRBv379cPDgQaSkpFS5wvizHkofERGhMo/W3d0d6enpGD16NDp06IBRo0bBwMAAKSkpsLS0rLK/Y8eO4cSJE2rXQTAzM8PAgQNF0whSU1PRrVs30aZuhAPwKOlR3WNY582bh+bNm4s2TU/EATS/X3VtwIABGDBgAMrKykTlPj4++P7776FQKNC9e3d4eHggNzcXe/fuVVnh/nHm5ubVLui5fv164Rr0798feXl5+OWXX9CxY0dRO3d3d5VrtmPHjir7njlzJn7++WfhcYHamDRpEm7duoVx48YJiZOafl6eJJVKERISgqVLl2o1qoiI6GUTEhKCrKwslJaW4vDhw6KnZqWmpkKpVAqvpVIpwsPDcf78eZSUlCA7OxvR0dGiJ8hVVFSo3ZjYICKil4Gkoq5X7yQiqoXCwsJHT02ZkQAd/apHARERNSaZUepHuBEREZF6lb8NCgoKtB5QUC8jN4iIiIiIiIiI6kqjTW4EBQXB2NhY7RYUFFSvsWVnZ2uMzdjYGNnZ2fUa34ts8ODBGq/74sWL6zu852rfvn1Vfg6JiIiIiIheFI12WsqNGzdQWFiots7U1BQ2NjbPOaL/U1ZWhszMTI31jo6Oz3yRzJfV1atXNa4Kb2FhAQsLi+ccUf0pKSnB1atXNdY/+VjZ+sZpKUT0ouK0FCIiopqpzbSURvsL28bGpl4TGFWRSqUN7ofjy8Le3r6+Q2gwDA0N+TkkIiIiIqKXQqOdlkJEREREREREBDC5QURERERERESNXKOdlkJEL6ZTX8i0nldHREREREQEcOQGERERERERETVyTG4QERERERERUaPG5AYRERERERERNWpMbhARERERERFRo8bkBhERERERERE1anxaChE1KF3Cd0NH36i+wyAiUpEZ5VnfIRAREZEGHLlBRERERERERI0akxtERERERERE1KgxuUFEREREREREjRqTG0RERERERETUqDG5QURERERERESNGpMbRERERERERNSoMblBRERE9BSio6Ph6OgIAwMD9OzZE0eOHNHYtl+/fpBIJCqbp+f/PWb27t27CAkJQcuWLWFoaIjOnTtj3bp1z+NUiIiIGi0mNxqg3NxcTJ8+HW3btoW+vj4cHBwwbNgwpKSkCG3+/PNPjB49Gra2tjAwMED79u0xefJk/P333zU6lkwmg66uLo4ePapS5+/vD4lEgqCgIJW64OBgSCQS+Pv7A4DaL2qPb/Pnz68yjszMTEgkEmRkZIhe29jY4M6dO6K2bm5uov4uXbqEcePGoUWLFjAwMEDLli0xYsQInD17FkqlstrYMjMzAQCHDh2Crq6u6AumpvieVHmcTp06qdRt3boVEokEjo6OKu2f3AwMDIQ2ldc/KipK1N+OHTsgkUhEbTRtjx9Tk8ov2k8eBwA8PT1V3r9+/fphxowZKvvHxcWJ9l25cqVWxyciaszi4+MRGhqK8PBwHD9+HK6urpDJZLhx44ba9omJicjJyRG2U6dOQVdXF6NHjxbahIaGIikpCRs3bsSZM2cwY8YMhISEYOfOnc/rtIiIiBodJjcamMzMTHTv3h2//fYbvvzyS5w8eRJJSUno378/goODAQC7du3CG2+8gdLSUmzatAlnzpzBxo0bYWZmhrlz52p9rOzsbBw8eBAhISFQKBRq2zg4OCAuLg4lJSVC2b1797B582a0atVKKHv8i9rKlSthamoqKvv4449rdT3u3LmDZcuWaax/8OAB3nnnHRQUFCAxMRHnzp1DfHw8unbtitu3b8Pb21sUR69evTB58mRRmYODAwBALpdj+vTp2Lt3L65du1bjWJs2bYobN27g0KFDonK5XC66VpWevEY5OTnIysoStTEwMMCSJUtw69YttcdctWqVaH8AiImJEV6rS1qp4+DgAKVSKSq7evUqUlJS0Lx582r3NzAwwJw5c/DgwQOtjkdE9KJYsWIFJk+ejICAAGGEhZGRkca/qxYWFrCzsxO25ORkGBkZiZIbBw8exMSJE9GvXz84OjpiypQpcHV1rXJECBER0ctOWt8BkNi0adMgkUhw5MgRNG3aVCh3cXFBYGAgiouLERAQgCFDhmD79u1CfZs2bdCzZ0/cvn1b62PFxMRg6NChmDp1Kt544w2sWLEChoaGojavvvoqLly4gMTERPj6+gJ4dNepVatWaNOmjdDOzs5O+LeZmRkkEomorLamT5+OFStWIDg4GDY2Nir1p0+fxoULF5CSkoLWrVsDAFq3bo0+ffoIbR4/Jz09PRgZGanEdvfuXcTHx+OPP/5Abm4ulEolPvvssxrFKpVKMW7cOCgUCvTq1QsAcOXKFaSmpmLmzJnYsmWLqL0218jd3R3nz59HZGQkli5dqlJvZmYGMzMzUZm5uXmNr/3QoUORkJCAAwcOCNcuNjYWgwYNQnZ2drX7+/j4YOfOnVi/fj2mTZtWo2MTETVW9+/fx7FjxxAWFiaU6ejowN3dXSXRrYlcLsfYsWNFf/N79+6NnTt3IjAwEC1atEBqair+/vtvfPXVV3V+DkRERC8KjtxoQPLz85GUlITg4GDRl5xK5ubm2L17N/Ly8jBr1iy1fZibm2t1rIqKCsTExMDPzw/Ozs5wcnLCtm3b1LYNDAxETEyM8FqhUCAgIECr4zwtHx8fODk5ISIiQm29tbU1dHR0sG3bNjx8+LDWx0lISICzszM6duwIPz8/KBQKVFRU1LifwMBAJCQkoLi4GMCj6SceHh6wtbWtVVy6urpYvHgxVq9ejStXrtSqD23o6enB19dX9D4rlUoEBgZqtb+pqSk+//xzREREoKioSKt9SktLUVhYKNqIiBqTvLw8PHz4UOW/8ba2tsjNza12/yNHjuDUqVN4//33ReWrV69G586d0bJlS+jp6cHDwwPR0dHo27dvncZPRET0ImFyowE5f/48Kioq4OzsrLHNP//8AwBVttHGnj17UFxcDJlMBgDw8/ODXC5X29bPzw/79+9HVlYWsrKycODAAfj5+T3V8bVVuRbEd999hwsXLqjU29vb4+uvv8a8efPQrFkzDBgwAAsWLMDFixdrdBy5XC6ck4eHBwoKCpCWllbjeLt164a2bdti27ZtqKioqDJBUFBQAGNjY9E2ePBglXZeXl5wc3NDeHh4jeOpicrETFFREfbu3YuCggIMHTpU6/2nTZsGAwMDrFixQqv2kZGRwsgTMzMzYXoQEdHLQi6Xo2vXrujRo4eofPXq1UhPT8fOnTtx7NgxLF++HMHBwdizZ089RUpERNTwMbnRgGgzUqA2ownUUSgU8Pb2hlT6aGaSj48PDhw4oDaBYG1tDU9PTyiVSsTExMDT0xNWVlZ1Eoc2ZDIZ3nzzTY3riQQHByM3NxebNm1Cr169sHXrVri4uCA5OVmr/s+dO4cjR47Ax8cHwKPpJd7e3hqTPdWpHOmSlpaGoqIiDBkyRG07ExMTZGRkiLbvv/9ebdslS5YgNjYWZ86cqVVM2nB1dUX79u2xbds2KBQKjB8/Xvh8aENfXx8RERFYtmwZ8vLyqm0fFhaGgoICYbt8+fLThE9E9NxZWVlBV1cX169fF5Vfv3692umBRUVFiIuLw6RJk0TlJSUl+Oyzz7BixQoMGzYMr7zyCkJCQuDt7V3lGlREREQvOyY3GpD27dtDIpHg7NmzGtt06NABAKpsU538/Hxs374da9asgVQqhVQqhb29PcrKyjQugBYYGAilUonY2FitpyrUpaioKMTHx+PPP/9UW29iYoJhw4Zh0aJFOHHiBN566y0sXLhQq77lcjnKysrQokUL4XqsXbsWP/74IwoKCmocq6+vL9LT0zF//vwqEwQ6OjpwcnISbfb29mrb9u3bFzKZTDSv+1kIDAxEdHQ0tm3bVqv32c/PD61bt9bq2uvr68PU1FS0ERE1Jnp6eujevbvoaWbl5eVISUkR1l7SZOvWrSgtLVUZCfngwQM8ePAAOjrir2i6urooLy+vu+CJiIheMExuNCAWFhaQyWSIjo5Wu27B7du3MWjQIFhZWaldXLKyTXU2bdqEli1b4sSJE6JRA8uXL4dSqVS7doWHhwfu37+PBw8eCFNZnqcePXrg3XffxaefflptW4lEAmdnZ63WfigrK8OGDRuwfPly0bU4ceIEWrRoobIIqDYsLCwwfPhwpKWl1WkiKCoqCj/99JPWi9TVxrhx43Dy5El06dIFnTt3rvH+Ojo6iIyMxNq1a4VH7BIRvchCQ0Oxfv16YXTd1KlTUVRUJKxNNWHCBLWJablcjpEjR8LS0lJUbmpqirfffhuffPIJUlNTcenSJSiVSmzYsAFeXl7P5ZyIiIgaIz4tpYGJjo5Gnz590KNHD0REROCVV15BWVkZkpOTsXbtWpw5cwbff/89Ro8ejeHDh+ODDz6Ak5MT8vLykJCQgOzsbMTFxVV5DLlcjvfeew9dunQRlTs4OCAsLAxJSUnw9PQU1enq6gpTInR1dev2pLW0aNEiuLi4iEZCZGRkIDw8HOPHj0fnzp2hp6eHtLQ0KBQKzJ49u9o+d+3ahVu3bmHSpEkqTx0ZNWoU5HI5goKChLJz586p9OHi4qJSplQqsWbNGpUvrY+rqKhQu+CcjY2Nyh07AOjatSt8fX3x9ddfV3lOT6NZs2bIyclBkyZNat2Hp6cnevbsiW+//bbWC6kSETUW3t7euHnzJubNm4fc3Fy4ubkhKSlJ+O9fdna2yn/Tz507h/379+PXX39V22dcXBzCwsLg6+uL/Px8tG7dGosWLRL9PSIiIiIxJjcamLZt2+L48eNYtGgRPvroI+Tk5MDa2hrdu3fH2rVrAQAjRozAwYMHERkZiXHjxqGwsBAODg4YMGBAtdMBjh07hhMnTmD9+vUqdWZmZhg4cCDkcrlKcgNAvU8b6NChAwIDA/Hdd98JZS1btoSjoyO++OILZGZmQiKRCK9nzpxZbZ9yuRzu7u4qiQ3gUXJj6dKl+O9//yuc+9ixY1XaqVsrwtDQUOWxuk8qLCxE8+bNVcpzcnI0ztWOiIhAfHx8lf0+LW2fuFOVJUuWoHfv3k8fDBFRIxASEoKQkBC1dampqSplHTt2rHINLTs7O9HTq4iIiKh6koq6WqGSiOgpFBYWPnpqyowE6Ogb1Xc4REQqMqNUE/9ERERU9yp/GxQUFGh9k51rbhARERERERFRo8bkxgsoKCgIxsbGarf6mq/bEGN6Gezbt0/jdTc2Nq7v8IiIiIiIiOoEp6W8gG7cuIHCwkK1daamprCxsXnOETXMmF4GJSUluHr1qsZ6Jyen5xhN1TgthYgaOk5LISIiej5qMy2FC4q+gGxsbBpcsqAhxvQyMDQ0bFAJDCIiIiIiomeB01KIiIiIiIiIqFHjyA0ialBOfSGr98cOExERERFR48KRG0RERERERETUqDG5QURERERERESNGpMbRERERERERNSoMblBRERERERERI0akxtERERERERE1KgxuUFEREREREREjRofBUtEDUqX8N3Q0Teq7zCIiASZUZ71HQIRERFVgyM3iIiIiIiIiKhRY3KDiIiIiIiIiBo1JjeIiIiIiIiIqFFjcoOIiIiIiIiIGjUmN4iIiIiIiIioUWNyg4iIiKgWoqOj4ejoCAMDA/Ts2RNHjhzR2LZfv36QSCQqm6fn/z2JRV29RCLBl19++TxOh4iIqFFjcoOIiIiohuLj4xEaGorw8HAcP34crq6ukMlkuHHjhtr2iYmJyMnJEbZTp05BV1cXo0ePFto8Xp+TkwOFQgGJRIJRo0Y9r9MiIiJqtJjceE78/f0xcuRI4d8SiQRRUVGiNjt27IBEIhGVrV+/Hq6urjA2Noa5uTm6deuGyMhIAICjo6PGuzwSiQT+/v5CPzKZDLq6ujh69GiVsdVWZGQkdHV11d5dUiqVkEgk6NSpk0rd1q1bIZFI4OjoCEDzna3KrV+/ftXG8vh1adq0KV599VVs3bpV1KakpAQWFhawsrJCaWkpAODvv/+GkZERNm/eLGpbXl6O3r1747333gPwf+9fUFCQyrGDg4NVrn1l+yc3Dw8PlZjT09NF/c2YMUM455q835pUtn3yOKWlpbC0tIREIkFqaqrKfv/617+gq6urch0BwNvbGz169MDDhw+FsgcPHqB79+7w9fWtNiYiosZoxYoVmDx5MgICAtC5c2esW7cORkZGUCgUattbWFjAzs5O2JKTk2FkZCRKbjxeb2dnh3//+9/o378/2rZt+7xOi4iIqNFicqOeGBgYYMmSJbh165bGNgqFAjNmzMAHH3yAjIwMHDhwALNmzcLdu3cBAEePHhXu7vz4448AgHPnzgllq1atAgBkZ2fj4MGDCAkJ0fil62kpFArMmjVLY/9NmzbFjRs3cOjQIVG5XC5Hq1athNeP39mqHN67Z88eoSwxMVGreCIiIpCTk4M///wTr7/+Ory9vXHw4EGh/scff4SLiwucnZ2xY8cOAECHDh0QFRWF6dOnIycnR2i7fPlyXLx4EevWrRPKHBwcEBcXh5KSEqHs3r172Lx5s+h8Knl4eKjckduyZYuojYGBAWbPnq3xnLR9v6vj4OCAmJgYUdn27dthbGystn1xcTHi4uI0vr9r1qxBdna2KFm3YMEC5OTk4JtvvtEqJiKixuT+/fs4duwY3N3dhTIdHR24u7ur/J3TRC6XY+zYsWjatKna+uvXr+Pnn3/GpEmT6iRmIiKiFx2TG/XE3d0ddnZ2wigMdXbu3IkxY8Zg0qRJcHJygouLC3x8fLBo0SIAgLW1tXB3x8LCAgBgY2MjlJmZmQEAYmJiMHToUEydOhVbtmwR/SCvC2lpaSgpKUFERAQKCwtFSYRKUqkU48aNE/04vnLlClJTUzFu3Dih7PE7W9bW1gAAS0tLlfOsjomJCezs7NChQwdER0fD0NAQP/30k1Avl8vh5+cHPz8/yOVyoXz69OlwdXXF5MmTAQBnz57FvHnz8N1338HKykpo9+qrr8LBwUGUbElMTESrVq3QrVs3lXj09fVV7sg1a9ZM1GbKlClIT0/HL7/8ovactH2/qzNx4kSVxIxCocDEiRPVtt+6dSs6d+6MTz/9FHv37sXly5dF9ZaWlvjuu+8QERGB//73v/jjjz8QGRmJ77//XuUciYheBHl5eXj48CFsbW1F5ba2tsjNza12/yNHjuDUqVN4//33NbaJjY2FiYkJ3n333aeOl4iI6GXA5EY90dXVxeLFi7F69WpcuXJFbRs7Ozukp6cjKyur1sepqKhATEwM/Pz84OzsDCcnJ2zbtq3W/akjl8vh4+ODJk2awMfHR5QseFxgYCASEhJQXFwM4NF0FQ8PD5Uvh3VNKpWiSZMmuH//PgDgwoULOHToEMaMGYMxY8Zg3759wjWWSCSIiYnBvn37sH79evj7+2Ps2LEYPny42vN5fASEQqFAQEBAreNs06YNgoKCEBYWhvLy8lr3U53u3bvD0dFRGP2RnZ2NvXv3Yvz48WrbVyaCzMzMMHjwYCiVSpU2w4cPx9ixYzFhwgRMnDgREydOxJAhQ6qMo7S0FIWFhaKNiOhlIJfL0bVrV/To0UNjG4VCAV9fXxgYGDzHyIiIiBovJjfqkZeXF9zc3BAeHq62Pjw8HObm5nB0dETHjh3h7++PhISEGv3w3bNnD4qLiyGTyQBAZaTC0yosLMS2bdvg5+cn9J+QkCBMnXlct27d0LZtW2zbtg0VFRVQKpUIDAyss1jUuX//PiIjI1FQUIABAwYAePSFcfDgwWjWrBksLCwgk8lESYrWrVtj5cqVCAoKqnK6h5+fH/bv34+srCxkZWXhwIEDwnV40q5du2BsbCzaFi9erNJuzpw5uHTpEjZt2lQHZ69ZYGCgMIpGqVRiyJAhwkiZx/3zzz9IT0+Ht7c3gEfnHBMTg4qKCpW2K1euxN9//43//e9/WLFiRbUxREZGwszMTNgcHBye8qyIiJ4PKysr6Orq4vr166Ly69evw87Orsp9i4qKEBcXV+V0k3379uHcuXNVjuwgIiIiMSY36tmSJUsQGxuLM2fOqNQ1b94chw4dwsmTJ/Hhhx+irKwMEydOhIeHh9YJDoVCAW9vb0ilUgCAj48PDhw4gAsXLtRJ/Fu2bEG7du3g6uoKAHBzc0Pr1q0RHx+vtn3laIe0tDQUFRVVe3e/tmbPng1jY2MYGRlhyZIliIqKgqenJx4+fIjY2FhREsLPzw9KpVJ0TQMCAtC8eXNMnz4dpqamao9hbW0NT09PKJVKxMTEwNPTUzR15XH9+/dHRkaGaFO3IKm1tTU+/vhjzJs3Txhp8iz4+fnh0KFDuHjxYpVJJoVCAZlMJpzXkCFDUFBQgN9++02l7ZYtWyCRSJCXl4ezZ89WG0NYWBgKCgqE7cnpLkREDZWenh66d++OlJQUoay8vBwpKSno1atXlftu3boVpaWlGpPhwKORHd27dxf+thIREVH1mNyoZ3379oVMJkNYWJjGNl26dMG0adOwceNGJCcnIzk5GWlpadX2nZ+fj+3bt2PNmjWQSqWQSqWwt7dHWVlZnS0sKpfLcfr0aaF/qVSKv/76S2P/vr6+SE9Px/z58zF+/Hgh6VLXPvnkE2RkZODKlSu4deuWsFDn7t27cfXqVSHhI5VKMXbsWGRlZYm+pAIQ6qsSGBgIpVKJ2NjYKkehNG3aFE5OTqJN0/ohoaGhKCkpwZo1a2p41tqztLTE0KFDMWnSJNy7dw+DBw9WaVOZCPr555+Fa2FkZIT8/HyV9/fixYuYNWsW1q5di/Hjx8Pf3194Co0m+vr6MDU1FW1ERI1FaGgo1q9fL9ygmDp1KoqKioTpiRMmTFD7t10ul2PkyJGwtLRU229hYSG2bt3KURtEREQ19Gx+WVKNREVFwc3NDR07dqy2befOnQE8GtZanU2bNqFly5bC00Aq/frrr1i+fDkiIiKgq6tbq5gB4OTJk/jjjz+Qmpoq+qGen5+Pfv364ezZs3B2dhbtY2FhgeHDhyMhIUH09JG6ZmVlBScnJ5XyytXpP//8c1H5okWLIJfL8c4779ToOB4eHrh//z4kEokw9edpGRsbY+7cuZg/f77atT7qSmBgIIYMGYLZs2er/Rz88ssvuHPnDv78809R/alTpxAQEIDbt2/D3Nwc5eXl8Pf3x8CBAzFhwgSMGDECXbp0wbx587BkyZJnFj8RUX3y9vbGzZs3MW/ePOTm5sLNzQ1JSUnCOlLZ2dnQ0RHfQzp37hz279+PX3/9VWO/cXFxqKiogI+PzzONn4iI6EXD5EYD0LVrV/j6+uLrr78WlU+dOhUtWrTAgAED0LJlS+Tk5GDhwoWwtraudtgr8OiH/HvvvYcuXbqIyh0cHBAWFoakpCR4enoCAAoKCpCRkSFqZ2lpWeU6CHK5HD169EDfvn1V6l5//XXI5XJ8+eWXKnVKpRJr1qzReNfqWbl58yZ++ukn7Ny5U+WaTJgwAV5eXsjPz9f6iSzAo4VhK6cUVZUoKi0tVVlBXyqVapzGMmXKFHz11VfYvHkzevbsqXU8NeHh4YGbN29qHDEhl8vh6empMiy6c+fOmDlzJjZt2oTg4GCsWrUKp0+fxunTpwEAZmZm+P777zF06FCMGjWqygXziIgas5CQEISEhKitS01NVSnr2LGj2jWLHjdlyhRMmTKlLsIjIiJ6qXBaSgMRERGhso6Gu7s70tPTMXr0aHTo0AGjRo2CgYEBUlJSqk0MHDt2DCdOnMCoUaNU6szMzDBw4EDRwqKpqano1q2baPviiy809n///n1s3LhRbf8AMGrUKGzYsAEPHjxQqTM0NHzuiQ0A2LBhA5o2bYqBAweq1A0cOBCGhobYuHFjjfvVZkpFUlISmjdvLtrefPNNje2bNGmCBQsW4N69ezWOR1sSiQRWVlbQ09NTqbt+/Tp+/vlnte+vjo4OvLy8IJfL8ffff+Pzzz/H6tWrRYvoyWQyBAQEaDU9hYiIiIiI6GlJKqq7hUBE9BwUFhY+emrKjATo6BvVdzhERILMKM/6DoGIiOilUvnboKCgQOu1+Thyg4iIiIiIiIgaNSY3SKNNmzbB2NhY7ebi4sKYGpjFixdrvDbqnoZCRERERET0ouC0FNLozp07uH79utq6Jk2aoHXr1s85ooYZU0ORn5+P/Px8tXWGhoawt7d/zhHVDKelEFFDxWkpREREz1dtpqXwaSmkkYmJCUxMTOo7DJGGGFNDYWFhUaMnvRAREREREb0oOC2FiIiIiIiIiBo1jtwgogbl1BcyrYeeERERERERARy5QURERERERESNHJMbRERERERERNSoMblBRERERERERI0akxtERERERERE1KgxuUFEREREREREjRqflkJEDUqX8N3Q0Teq7zCIiASZUZ71HQIRERFVgyM3iIiIiIiIiKhRY3KDiIiIiIiIiBo1JjeIiIiIiIiIqFFjcoOIiIiIiIiIGjUmN4iIiIiIiIioUWNyg4iIiIiIiIgaNSY3iIiIiGohOjoajo6OMDAwQM+ePXHkyBGNbfv16weJRKKyeXr+32Nm1dVLJBJ8+eWXz+N0iIiIGjUmN4iIiIhqKD4+HqGhoQgPD8fx48fh6uoKmUyGGzduqG2fmJiInJwcYTt16hR0dXUxevRooc3j9Tk5OVAoFJBIJBg1atTzOi0iIqJGi8mNF1xubi6mT5+Otm3bQl9fHw4ODhg2bBhSUlKENn/++SdGjx4NW1tbGBgYoH379pg8eTL+/vvvGh1LJpNBV1cXR48eVanz9/eHRCJBUFCQSl1wcDAkEgn8/f0BaL5zVbnNnz+/yjgyMzNF7S0tLTFo0CD8+eefonaHDh2Crq6u6K7ZDz/8gKZNm+L8+fOitteuXUOzZs3wzTffAAAcHR0hkUgQFxencnwXFxdIJBIolUqhrLL9k1tUVJQoZhsbG9y5c0fUn5ubG+bPn69yXuq2x4+pTmpqKiQSCZo1a4Z79+6J6o4ePSr0o46zszP09fWRm5srKi8qKkK7du0QGhoqKs/MzISpqSnWr19fZUxERI3RihUrMHnyZAQEBKBz585Yt24djIyMoFAo1La3sLCAnZ2dsCUnJ8PIyEiU3Hi83s7ODv/+97/Rv39/tG3b9nmdFhERUaPF5MYLLDMzE927d8dvv/2GL7/8EidPnkRSUhL69++P4OBgAMCuXbvwxhtvoLS0FJs2bcKZM2ewceNGmJmZYe7cuVofKzs7GwcPHkRISIjGL3YODg6Ii4tDSUmJUHbv3j1s3rwZrVq1Esoev2u1cuVKmJqaiso+/vhjrWLas2cPcnJysHv3bty9exeDBw/G7du3hXq5XI7p06dj7969uHbtGgBg/PjxkMlk8Pf3R3l5udB28uTJ6N69u3DdKs8nJiZGdMz09HTk5uaiadOmKvFERESo3JWbPn26qM2dO3ewbNkytefj4OAg2vejjz6Ci4uLqMzb21ura2NiYoLt27eLyuRyueh9eNz+/ftRUlKC9957D7GxsaK6pk2bIiYmBqtXr8a+ffsAABUVFQgICECfPn0wefJkrWIiImos7t+/j2PHjsHd3V0o09HRgbu7Ow4dOqRVH3K5HGPHjlX79wIArl+/jp9//hmTJk2qk5iJiIhedExuvMCmTZsGiUSCI0eOYNSoUejQoQNcXFwQGhqK9PR0FBcXIyAgAEOGDMHOnTvh7u6ONm3aoGfPnli2bBm+/fZbrY8VExODoUOHYurUqdiyZYsogVHp1VdfhYODAxITE4WyxMREtGrVCt26dRPKHr9rZWZmBolEIiozNjbWKiZLS0vY2dnhtddew7Jly3D9+nUcPnwYAHD37l3Ex8dj6tSp8PT0FI14+Pbbb/H3339jxYoVAAClUokDBw4gJiZGNKrB19cXaWlpuHz5slCmUCjg6+sLqVSqEo+JiYnKXbknv9ROnz4dK1asUDusWVdXV+U6SKVSUZmhoaFW12bixImiJFRJSQni4uIwceJEte3lcjnGjRuH8ePHq01e9e3bF9OnT0dAQACKioqwwj4mZQAAYedJREFUatUqZGRk4Pvvv9cqHiKixiQvLw8PHz6Era2tqNzW1lZldJs6R44cwalTp/D+++9rbBMbGwsTExO8++67Tx0vERHRy4DJjRdUfn4+kpKSEBwcrPaukLm5OXbv3o28vDzMmjVLbR/m5uZaHauiogIxMTHw8/ODs7MznJycsG3bNrVtAwMDRaMdFAoFAgICtDrO06j80X///n0AQEJCApydndGxY0f4+flBoVCgoqICAGBtbY3vvvsOc+fORXJyMmbOnIlVq1bBwcFB1KetrS1kMpkwkqG4uBjx8fEIDAysdZw+Pj5wcnJCRERErfvQxvjx47Fv3z5kZ2cDAH788Uc4Ojri1VdfVWl7584dbN26FX5+fnjnnXdQUFAgjNB43KJFiyCVSuHn54fPPvsMq1evhr29vcYYSktLUVhYKNqIiF4GcrkcXbt2RY8ePTS2qUyWGxgYPMfIiIiIGi8mN15Q58+fR0VFBZydnTW2+eeffwCgyjba2LNnD4qLiyGTyQAAfn5+kMvlatv6+flh//79yMrKQlZWFg4cOAA/P7+nOn51bt++jQULFsDY2Fj4IimXy4Xjenh4oKCgAGlpacI+I0eOxJgxY+Dh4YG3335b44iGwMBAKJVKVFRUYNu2bWjXrh3c3NzUtp09ezaMjY1F25NJgsp1OL777jtcuHChDs5ePRsbGwwePFgYsaJQKDQmZeLi4tC+fXu4uLhAV1cXY8eOVfv+GhoaYtWqVdixYwf69etX7fsaGRkJMzMzYXsyeURE1FBZWVlBV1cX169fF5Vfv34ddnZ2Ve5bVFSEuLi4Kqeb7Nu3D+fOnatyZAcRERGJMbnxgqochfC0bbShUCjg7e0tTMXw8fHBgQMH1P44t7a2FqaBxMTEwNPTE1ZWVnUSx5N69+4NY2NjNGvWDCdOnEB8fDxsbW1x7tw5HDlyBD4+PgAAqVQKb29vlR/sc+fORXl5OebMmaPxGJ6enrh79y727t1bZYIAAD755BNkZGSIttdee02lnUwmw5tvvlmjNU9qozIxc/HiRRw6dAi+vr5q2ykUClGiws/PD1u3blVZ+BR4lDQyMjLCyZMnUVBQUOXxw8LCUFBQIGyPT+8hImrI9PT00L17d9Hi3OXl5UhJSUGvXr2q3Hfr1q0oLS2tMgEsl8vRvXt3uLq61lnMRERELzomN15Q7du3h0QiwdmzZzW26dChAwBU2aY6+fn52L59O9asWQOpVAqpVAp7e3uUlZVpXFi08kd1bGzsU03hqE58fDxOnDiBW7du4cKFCxgyZAiAR18ay8rK0KJFCyHmtWvX4scffxT9IK9M1qhbP+PxNuPHj0d4eDgOHz6sMUEAPLrT5+TkJNo0rZERFRWF+Ph4lSe81KXBgwejpKQEkyZNwrBhw2BpaanS5q+//kJ6ejpmzZolXKs33ngDxcXFKk+KiY+Px65du3Dw4EGYmJhg5syZVR5fX18fpqamoo2IqLEIDQ3F+vXrERsbizNnzmDq1KkoKioSplpOmDABYWFhKvvJ5XKMHDlS7X9zAaCwsBBbt27lqA0iIqIaYnLjBWVhYQGZTIbo6GgUFRWp1N++fRuDBg2ClZUVli5dqraPx58sosmmTZvQsmVLnDhxQjQiYfny5VAqlXj48KHKPh4eHrh//z4ePHggTGV5FhwcHNCuXTvR2iFlZWXYsGEDli9fLor3xIkTaNGiBbZs2VLj4wQGBiItLQ0jRoxAs2bN6iT2Hj164N1338Wnn35aJ/2pI5VKMWHCBKSmpmpMMsnlcvTt21fl/Q0NDRWNdLl+/TqCg4OxcOFCuLq6QqlUYsOGDfjPf/7zzOInIqpP3t7eWLZsGebNmwc3NzdkZGQgKSlJWGQ0OzsbOTk5on3OnTuH/fv3VzklJS4uDhUVFcLoQiIiItKO5lvS1OhFR0ejT58+6NGjByIiIvDKK6+grKwMycnJWLt2Lc6cOYPvv/8eo0ePxvDhw/HBBx/AyckJeXl5SEhIQHZ2tsrd+SfJ5XK899576NKli6jcwcEBYWFhSEpKgqenp6hOV1cXZ86cEf79PO3atQu3bt3CpEmTYGZmJqobNWoU5HI5goKCatRnp06dkJeXByMjoyrb3blzR2UVfSMjI40jFhYtWgQXF5cqR448rQULFuCTTz5RewfxwYMH+OGHHxAREaHy/r7//vtYsWIFTp8+DRcXF0yZMgWdOnXCjBkzADxKznzyySeYMmUKTp06pXKtiYheBCEhIQgJCVFbl5qaqlLWsWPHaqeETpkyBVOmTKmL8IiIiF4qHLnxAmvbti2OHz+O/v3746OPPkKXLl3wzjvvICUlBWvXrgUAjBgxAgcPHkSTJk0wbtw4ODs7w8fHBwUFBVi4cGGV/R87dgwnTpzAqFGjVOrMzMwwcOBAjQuL1tc0BLlcDnd3d7U/tkeNGoU//vgD//3vf2vcr6WlZbWPYZ03bx6aN28u2jQ9qQZ4NG0oMDAQ9+7dq3E82tLT04OVlZXoEbeVdu7cif/973/w8vJSqevUqRM6deoEuVyODRs2YM+ePYiJiYGOzv/9J+WLL76Aubl5tdNTiIiIiIiInpakoq5WlSQiegqFhYWPnpoyIwE6+lWPgiEiep4yozyrb0RERER1pvK3QUFBgdY3xTlyg4iIiIiIiIgaNSY3qEpBQUEwNjZWu9V0bYoXOaaGYvDgwRqvzeLFi+s7PCIiIiIiomeC01KoSjdu3EBhYaHaOlNTU9jY2DzniBpmTA3F1atXUVJSorbOwsICFhYWzzki7XFaChE1VJyWQkRE9HzVZloKn5ZCVbKxsWlwyYKGGFNDYW9vX98hEBERERERPXeclkJEREREREREjRpHbhBRg3LqC1m9PCaYiIiIiIgaL47cICIiIiIiIqJGjckNIiIiIiIiImrUmNwgIiIiIiIiokaNyQ0iIiIiIiIiatSY3CAiIiIiIiKiRo1PSyGiBqVL+G7o6BvVdxhE9ILLjPKs7xCIiIioDnHkBhERERERERE1akxuEBEREREREVGjxuQGERERERERETVqTG4QERERERERUaPG5AYRERERERERNWpMbhARERERERFRo8bkBhERERGA6OhoODo6wsDAAD179sSRI0c0tu3Xrx8kEonK5ump/hGzQUFBkEgkWLly5TOKnoiI6OXG5AYRERG99OLj4xEaGorw8HAcP34crq6ukMlkuHHjhtr2iYmJyMnJEbZTp05BV1cXo0ePVmm7fft2pKeno0WLFs/6NIiIiF5azz254e/vL7rDYWlpCY//196dx+WU/v8Df93te2lRIYVUJPtIjLFF2QZjUtYoZhjrMLb5IHsxTJaP+cwMLWYYLWSfydLIvpM12ctQjFAKSZ3fH373+Xbcd3XflMTr+Xicx0zXeZ/rvM91X+W+r/s61/H2xrlz5wAAt27dgkwmQ1JSksKx7dq1w/jx48WfHRwcxHoMDAzg5uaG1atXKxxXUFCA0NBQuLm5QU9PD1WqVEGXLl1w6NAhtfN/9uwZzM3NYWlpiby8PIX98pyioqIU9rm6ukImkyEyMhKJiYlKv/EpuiUmJpaaz4sXL7Bo0SI0atQIBgYGsLS0ROvWrREREYH8/HwA0jbX1tZGrVq1MHnyZDx//lxSV3F5yK+laM4aGhowNTVFkyZNMHnyZKSnp0vqmjVrFho3bixpk+K2IUOGlHqd+/btQ4cOHWBubg4DAwPUrVsX/v7+ePHiBQAgMjISZmZmSo+VyWTYvHkzgP/rX5qamrhz544kLj09HVpaWpDJZLh161apOcnrkm/GxsZwdXXFqFGjcPXqVUlsZGSk0mvX09MTY0r73SiujqJbaXk/ffoU06ZNQ506daCnpwcrKyu0bdsWW7ZsEWMcHByUfrNY9DUtmu+IESMUYkeNGqXya0tE9D748ccfMXz4cAwdOhT169fHzz//DAMDA4SHhyuNNzc3h42Njbjt3r0bBgYGCoMbd+7cwZgxY7Bu3Tpoa2u/i0shIiL6KFXIzA1vb2/xm46EhARoaWmhe/fub1TXnDlzxG9MBg4ciOHDh+Ovv/4S9wuCAD8/P8yZMwfjxo1DcnIyEhMTYWdnh3bt2okfelW1ceNGuLq6wsXFpdhj7ezsEBERISk7evQoMjIyYGhoCABo1aqV5Bufvn37StolPT0drVq1KjGXFy9ewMvLCyEhIfjqq69w+PBhHD9+HKNGjcKKFStw8eJFMVZe940bNxAaGopffvkFQUFBCnVGRERIckhPT0evXr0kMSkpKbh79y5OnDiBKVOmYM+ePWjQoAHOnz+vNM8TJ06IdW3cuFGsQ162bNmyEq/z0qVL8Pb2RvPmzbF//36cP38eK1asgI6ODgoKCko8tjjVq1fHb7/9Jilbs2YNqlevrnZde/bsQXp6Os6ePYsFCxYgOTkZjRo1QkJCgiTOxMREoW1TU1MlMSX9bvj6+kqO9fDwwPDhwyVldnZ2JeY6YsQIxMXFYcWKFbh8+TLi4+Px5ZdfIjMzU+3rBl719aioKDx79kwse/78Of744w/UrFnzjeokInrXXrx4gVOnTsHT01Ms09DQgKenJ44cOaJSHWFhYfDz8xP/nQeAwsJCDBo0CJMmTYKrq2uZ501ERET/R6siTqqrqwsbGxsAgI2NDaZOnYo2bdrg33//VbsuY2Njsa4pU6Zg0aJF2L17N7p06QIAiImJwYYNG7B161b06NFDPO7XX39FZmYmhg0bhk6dOknejJQkLCwMAwcOhCAICAsLg6+vr0LMgAEDEBoaitu3b4sfNsPDwzFgwADxA7WOjo6YNwDo6+sjLy9PUlaapUuXYv/+/Th58iSaNGkilteuXRs+Pj7irAZA2uZ2dnbw9PTE7t27sXDhQkmdZmZmpeZQtWpVMc7JyQk9e/ZEkyZNMHLkSBw8eFAh3srKSvx/c3NzSR2q2LVrF2xsbLBo0SKxrE6dOvD29lbpeGX8/f0RERGBadOmiWURERHw9/fH3Llz1arLwsJCbLPatWujR48e6NixIwIDA3H9+nVoamoCeDWDpLS2Lel3w8rKCvr6+mKsjo4ODAwM1OozW7duxbJly9C1a1cAr2ZpNGvWTK3rLapp06a4fv064uLiMGDAAACvpmrXrFkTtWrVKvHYvLw8yeyn7OzsN86DiOhtPHjwAAUFBbC2tpaUW1tb4/Lly6Uef/z4cVy4cAFhYWGS8oULF0JLSwtjx44t03yJiIhIUYWvuZGTk4O1a9fC0dERFhYWb1xPYWEhNm7ciEePHkFHR0cs/+OPP+Dk5CQZ2JCbOHEiMjMzsXv3bpXOcf36dRw5cgR9+/ZF3759ceDAAYVv3oFXb4a8vLywZs0aAK9uBYiOjkZAQMAbXp1y69atg6enp2RgQ05bW7vYAZsLFy7g8OHDknZ6G/r6+hgxYgQOHTpU7L3Jb8PGxgbp6enYv39/mdX5+eef49GjR+JgzMGDB/Ho0SOl/URdGhoaGDduHFJTU3Hq1Kk3rqesfjeKsrGxwZ9//oknT56USX0AEBAQIJmpFB4ejqFDh5Z6XHBwMExNTcWttFknRETvq7CwMLi5uaFFixZi2alTp7Bs2TLxlkIiIiIqXxUyuLF9+3YYGRnByMgIxsbG2Lp1K6Kjo6GhoX46U6ZMgZGREXR1dfHll1+iSpUqGDZsmLj/ypUrqFevntJj5eVXrlxR6Vzh4eHo0qULqlSpAnNzc3h5eSncfiIXEBCAyMhICIKADRs2oE6dOpL1CsrC1atX4eLiolKsvM319PTg5uaG+/fvY9KkSQpx/fr1E18b+ZaWllZq/fI8VFmrQl0+Pj7o168f2rZtC1tbW/Tu3Rv//e9/3+qbfm1tbQwcOFC8lzo8PBwDBw4ss/uhlbVHVlaWQtvKZxjJleXvhjK//vorDh8+DAsLC3zyySf49ttv32jtmaIGDhyIgwcPIjU1FampqTh06BAGDhxY6nHTpk1DVlaWuN2+ffut8iAielOWlpbQ1NTEvXv3JOX37t0rdXZcbm4uoqKiEBgYKCk/cOAA7t+/j5o1a0JLSwtaWlpITU3FxIkT4eDgUNaXQERE9NGrkMGN9u3bIykpCUlJSTh+/Di8vLzQpUsXpbMgSjNp0iQkJSXh77//hru7O0JDQ+Ho6CiJEQThrXMuKCjAmjVrJB/aBg4ciMjISBQWFirEd+vWDTk5Odi/fz/Cw8PLfNYGoN51ydv82LFj8Pf3x9ChQ9GnTx+FuNDQUPG1kW+qrO4uz6U8vp3S1NREREQE/vnnHyxatAjVq1fHggUL4OrqqrCQqToCAgIQGxuLjIwMxMbGlulrpKw9jI2NFdr29QVwy/J3Q5nPPvsMN27cQEJCAr788ktcvHgRbdq0UftWnKKsrKzQrVs3REZGIiIiAt26dYOlpWWpx+nq6sLExESyERFVBB0dHTRr1kyyVlJhYSESEhLg4eFR4rGxsbHIy8tTGNQdNGgQzp07p/Dv6aRJk7Bz585yuQ4iIqKPWYWsuWFoaCgZgFi9ejVMTU2xatUqTJgwAcCrb7lf9/jxY5iamkrKLC0t4ejoCEdHR8TGxsLNzQ3NmzdH/fr1AQBOTk5ITk5Wmoe83MnJqdScd+7ciTt37iissVFQUICEhAR06tRJUq6lpYVBgwYhKCgIx44dw6ZNm0o9h7qcnJxUuhcYkLZ5eHg4GjVqhLCwMIVvmmxsbBQGh1Qhb8vy/DaqevXqGDRoEAYNGoS5c+fCyckJP//8M2bPng0TExPk5uaisLBQMsvh8ePHAKDQbwDAzc0NLi4u6NevH+rVq4cGDRoofUrPm5C3R9F1JzQ0NEpt25J+N+bNm1cmuWlra6NNmzZo06YNpkyZgnnz5mHOnDmYMmUKdHR0YGJiovLvn1xAQABGjx4NAFi5cmWZ5ElE9C5NmDAB/v7+aN68OVq0aIGlS5ciNzdXvM1u8ODBqF69OoKDgyXHhYWFoVevXgq3D1pYWCiUaWtrw8bGBs7OzuV7MURERB+hCl9zA4D4aNGij1l9fa2C7OxsXLt2rcSBCDs7O/j6+koWifTz88PVq1exbds2hfglS5bAwsJCYWBCGfkq6K9/8+7n56ewgJhcQEAA9u3bh549e6JKlSqlnkNd/fv3x549e3DmzBmFffn5+cjNzVV6nIaGBr7//ntMnz5d8pSLN/Xs2TP8+uuv+OyzzySLh5anKlWqwNbWVrxGZ2dnvHz5UmFw4vTp0wCKH8AKCAhAYmJimc7aKCwsxPLly1GrVi2l66Goo+jvRnmpX78+Xr58KT4a2NnZWelaIadPny62Hb29vfHixQvk5+fDy8ur3HIlIiovvr6+WLx4MWbOnInGjRsjKSkJ8fHx4iKjaWlpCrMFU1JScPDgQYUvCoiIiOjdq5CZG3l5ecjIyAAAPHr0CP/973+Rk5MjLuY4YcIELFiwANbW1mjZsiUyMzMxd+5cWFlZ4Ysvviix7nHjxqFBgwY4efIkmjdvDj8/P8TGxsLf3x8//PADOnbsiOzsbKxcuRJbt25FbGxsqU9K+ffff7Ft2zZs3boVDRo0kOwbPHgwevfujYcPH4pPApGrV68eHjx4AAMDA3WbSCXjx4/Hjh070LFjR8ydOxeffvopjI2NcfLkSSxcuBBhYWHFrvPh4+ODSZMmYeXKlfjuu+/E8sePH4uvjZyxsbGkje7fv4/nz5/jyZMnOHXqFBYtWoQHDx4gLi6uXK7zl19+QVJSEnr37o06derg+fPn+O2333Dx4kWsWLECAODq6orOnTsjICAAS5YsQe3atZGSkoLx48fD19e32Ee8Dh8+HD4+Pio/uUWZzMxMZGRk4OnTp7hw4QKWLl2K48ePY8eOHeKTUoBXt6q83rbAqyfHyGeblPa78bbatWuHfv36oXnz5rCwsMClS5fw/fffo3379uJtId9++y3atGmD+fPn44svvkBBQQHWr1+PI0eO4KefflJar6ampjhbpeg1ExFVJqNHjxZnob0uMTFRoczZ2VmtW0TLY10qIiIieqVCBjfi4+Nha2sL4NUHZxcXF8TGxqJdu3YAgMmTJ8PIyAgLFy7E9evXYW5ujtatW2Pv3r2SR2EqU79+fXTu3BkzZ87En3/+CZlMhpiYGCxduhShoaH45ptvoKenBw8PDyQmJqJ169al5vvbb7/B0NAQHTt2VNjXsWNH6OvrY+3atUof9VZWT7lQRldXF7t370ZoaCh++eUXfPfddzAwMEC9evUwduxYhYGYorS0tDB69GgsWrQII0eOFAcvlD3lIjg4GFOnThV/dnZ2hkwmg5GREWrXro3OnTtjwoQJaj2SVB0tWrTAwYMHMWLECNy9exdGRkZwdXXF5s2b0bZtWzEuOjoaQUFB+Prrr3H37l3UqFEDvXv3xowZM4qtW0tLS6X1IUri6ekJADAwMIC9vT3at2+PX3/9VeEWlOzsbLHfF5Weni62XWm/G29L/hSf77//Hk+fPkW1atXQvXt3zJw5U4xp1aoV/vrrL8yZMwdLliyBhoYG3NzckJCQUGKf4poZRERERERUUWRCWay2SUT0lrKzs189EnZ8DDR0y2e2ExGR3K2QbhWdAhERERVD/tkgKytL5S9R34s1N4iIiIiIiIiI3hQHN/4/V1dXGBkZKd3WrVvHnMrRggULir3OLl26VFheI0aMKDavESNGVFheJSkuXyMjIxw4cKCi0yMiIiIiIioXvC3l/0tNTUV+fr7SfdbW1jA2Nn7HGb2fOZWHhw8f4uHDh0r36evrF7sYaHm7f/8+srOzle4zMTFB1apV33FGpbt27Vqx+6pXr17qmjUVibelENG7xNtSiIiI3l9vcltKhSwo+j6yt7ev6BQUvI85lQdzc3OFJ828D6pWrfpeDmCU5PVFTImIiIiIiD4GvC2FiIiIiIiIiCo1ztwgovfKhdlefKwsERERERGphTM3iIiIiIiIiKhS4+AGEREREREREVVqHNwgIiIiIiIiokqNgxtEREREREREVKlxcIOIiIiIiIiIKjU+LYWI3isNgnZCQ9egotMgog/YrZBuFZ0CERERlTHO3CAiIiIiIiKiSo2DG0RERERERERUqXFwg4iIiIiIiIgqNQ5uEBEREREREVGlxsENIiIiIiIiIqrUOLhBRERERERERJUaBzeIiIjoo7dy5Uo4ODhAT08P7u7uOH78eLGx7dq1g0wmU9i6dVP+iNkRI0ZAJpNh6dKl5ZQ9ERERVZrBjSFDhohvHrS1tVGrVi1MnjwZz58/F2OUvdGQyWSIiooCACQmJirdP336dABAZGQkzMzMlJ5fJpNh8+bN4s+bNm1Cy5YtYWpqCmNjY7i6umL8+PHi/sjISKXn0tPTK/aarK2t0alTJ4SHh6OwsFDtNgoODoampiZ++OEHhX3yfOrVq6ewLzY2FjKZDA4ODgCKf9Mm39q1a6dSPmfOnIGvry9sbW2hq6sLe3t7dO/eHdu2bYMgCJLYNWvW4JNPPoGBgQGMjY3Rtm1bbN++XaHOgoIChIaGws3NDXp6eqhSpQq6dOmCQ4cOKb1emUwGTU1NVKlSBe7u7pgzZw6ysrIksf/++y9GjhyJmjVrQldXFzY2NvDy8lKoszgODg7iuQwNDdG0aVPExsaK+2fNmiXJxc7ODl999RUePnyoUM/rb3zPnDkDHx8fWFtbQ09PD3Xr1sXw4cNx5coVAMCtW7eKfZ2OHj1aau7q9At5fNHfEfnx3t7ekmMfP34MmUyGxMTEUnMgIqpo0dHRmDBhAoKCgnD69Gk0atQIXl5euH//vtL4uLg4pKeni9uFCxegqakJHx8fhdhNmzbh6NGjqFatWnlfBhER0Uet0gxuAIC3tzfS09Nx48YNhIaG4pdffkFQUJAkJiIiQvKGIz09Hb169ZLEpKSkSPZPnTpVrTwSEhLg6+uLPn364Pjx4zh16hTmz5+P/Px8SZyJiYlCLqmpqUqv6datW/jrr7/Qvn17jBs3Dt27d8fLly/Vyis8PByTJ09GeHi40v2Ghoa4f/8+jhw5IikPCwtDzZo1xZ+LvmmTf3O1Z88esSwuLq7UXLZs2YKWLVsiJycHa9asQXJyMuLj49G7d29Mnz5dMsDw3Xff4euvv4avry/OnTuH48eP49NPP0XPnj3x3//+V4wTBAF+fn6YM2cOxo0bh+TkZCQmJsLOzg7t2rWTDD4B/9f+//zzDw4fPoyvvvoKv/32Gxo3boy7d++KcX369MGZM2ewZs0aXLlyBVu3bkW7du2QmZlZ6nXKzZkzB+np6Thz5gw++eQT+Pr64vDhw+J+V1dXpKenIy0tDREREYiPj8fIkSNLrHP79u1o2bIl8vLysG7dOiQnJ2Pt2rUwNTXFjBkzJLFFXx/51qxZM5VyV7VfFEdLSwt79uzB3r17VTofEdH75scff8Tw4cMxdOhQ1K9fHz///DMMDAyK/ffU3NwcNjY24rZ7924YGBgoDG7cuXMHY8aMwbp166Ctrf0uLoWIiOijpVXRCahD/q06ANjZ2cHT0xO7d+/GwoULxRgzMzMxpjhVq1YtdoaGKrZt24bWrVtj0qRJYpmTk5PCIIpMJis1l6LXVL16dTRt2hQtW7ZEx44dERkZiWHDhqmU0759+/Ds2TPMmTMHv/32Gw4fPoxWrVpJYrS0tNC/f3+Eh4fDw8MDAPDPP/8gMTER3377LdavXw/g1Zs2OfnMGAsLi1KvRS43NxeBgYHo1q2bwkBIvXr1EBgYKM7cOHr0KJYsWYLly5djzJgxYtz8+fPx/PlzTJgwAT179oSdnR1iYmKwYcMGbN26FT169BBjf/31V2RmZmLYsGHo1KkTDA0NAUjb39bWFvXq1UOPHj3g6uqKyZMnY+3atXj8+DEOHDiAxMREtG3bFgBgb2+PFi1aqHStcsbGxuKb3JUrV2Lt2rXYtm2b+BpoaWlJXmcfHx9EREQUW9/Tp08xdOhQdO3aFZs2bRLLa9WqBXd3dzx+/FgSr87r8zpV+0VxDA0N0bdvX0ydOhXHjh17oxyIiCrKixcvcOrUKUybNk0s09DQgKenp8Kgb3HCwsLg5+cn/vsDAIWFhRg0aBAmTZoEV1fXMs+biIiIpCrVzI2iLly4gMOHD0NHR+edn9vGxgYXL17EhQsXyqX+Dh06oFGjRirNkJALCwtDv379oK2tjX79+iEsLExpXEBAAGJiYvD06VMAr24r8Pb2hrW1dZnkDgC7du1CZmYmJk+eXGyMTCYDAKxfvx5GRkb4+uuvFWImTpyI/Px8bNy4EQDwxx9/wMnJSTKwUTQ2MzMTu3fvLjG3qlWrYsCAAdi6dSsKCgpgZGQEIyMjbN68GXl5eepcZrG0tLSgra2NFy9eKN1/69Yt7Ny5s8S+u3PnTjx48KDYNnybwTll3rZfzJo1C+fPn8eGDRtUPmdeXh6ys7MlGxHRu/bgwQMUFBQo/L2ztrZGRkZGqccfP34cFy5cUPgyYuHChdDS0sLYsWPLNF8iIiJSrlINbmzfvh1GRkbQ09ODm5sb7t+/L5k9AQD9+vUTP7DKt7S0NElMjRo1JPvVuf0AAMaMGYNPPvkEbm5ucHBwgJ+fH8LDwxU+HGdlZSnk0qVLF5XO4eLiglu3bqkUm52djQ0bNmDgwIEAgIEDByImJgY5OTkKsU2aNEHt2rWxYcMGCIKAyMhIBAQEqHQeVcnXg3B2dhbLTpw4IWkH+XoaV65cQZ06dZR+0K9WrRpMTEzE+q5cuaJ0bQgAYrk8tiQuLi548uQJMjMzoaWlhcjISKxZswZmZmZo3bo1vv/+e5w7d069i/7/Xrx4geDgYGRlZaFDhw5i+fnz52FkZAR9fX3UqlULFy9exJQpU4qt5+rVq2KuqmjVqpVCX1PH2/aLatWqYdy4cfjPf/6j8u1UwcHBMDU1FTc7Ozu1ciYieh+EhYXBzc1NMuPv1KlTWLZsmbguEREREZW/SjW40b59eyQlJeHYsWPw9/fH0KFD0adPH0lMaGgokpKSJNvri3gdOHBAsr9KlSpq5WFoaIgdO3bg2rVrmD59OoyMjDBx4kS0aNFC/OYbeHWrwuu5rF69WqVzCIKg8hui9evXo06dOmjUqBEAoHHjxrC3t0d0dLTS+ICAAERERGDfvn3Izc1F165dVTrP22jYsKHYBrm5uZIPwK8vLloSdWJLq0Pevn369MHdu3exdetWeHt7IzExEU2bNkVkZKTKdU6ZMgVGRkYwMDDAwoULERISIlk139nZGUlJSThx4gSmTJkCLy8vyW04xeWoqujoaIW+pq637RdTpkzBv//+W+w96q+bNm0asrKyxO327dtq50xE9LYsLS2hqamJe/fuScrv3btX6u1+ubm5iIqKQmBgoKT8wIEDuH//PmrWrAktLS1oaWkhNTUVEydOlCzSTERERGWnUg1uGBoawtHREY0aNUJ4eDiOHTumcPuFjY0NHB0dJZuWlnRpkVq1akn2a2i8agYTExPk5uYqPKlEvr6BqamppLxOnToYNmwYVq9ejdOnT+PSpUuSAQUNDQ2FXKpXr67StSYnJ6NWrVoqxYaFheHixYviGygtLS1cunSp2A+ZAwYMwNGjRzFr1iwMGjRIoX3eVt26dQG8WrhVTldXV2yDopycnHDjxg2lt3DcvXsX2dnZcHJyEmOTk5OVnlNeLo8tSXJyMkxMTGBhYSGW6enpoVOnTpgxYwYOHz6MIUOGKCxWW5JJkyYhKSkJ//zzDx49eqQwK0NHRweOjo5o0KABQkJCoKmpidmzZxdbn/w6Ll++rNL57ezsFPqaut62X5iZmWHatGmYPXu2ZJCvOLq6ujAxMZFsRETvmo6ODpo1a4aEhASxrLCwEAkJCeI6RMWJjY1FXl6eOHNSbtCgQTh37pzCFy2TJk3Czp07y+U6iIiIPnaVanCjKA0NDXz//feYPn06nj17ViZ1Ojs74+XLlwrfep8+fRpAyR+cHRwcYGBggNzc3LfO4++//8b58+cVZqUoc/78eZw8eRKJiYmSN1GJiYk4cuSI0g/H5ubm+Pzzz7Fv374yvyUFADp37gxzc3PJQq/F8fPzQ05ODn755ReFfYsXL4a2trbYDn5+frh69Sq2bdumELtkyRJYWFigU6dOJZ7v/v37+OOPP9CrVy9xUEuZ+vXrq/VaWlpawtHRETY2NirNuJk+fToWL14seWpLUZ07d4alpSUWLVqkdP/rC4qWhbLoF2PGjIGGhgaWLVtWxtkREZWfCRMmYNWqVeLTvUaOHInc3FwMHToUADB48GDJgqNyYWFh6NWrl2SwHHi1yHODBg0km7a2NmxsbCS3bBIREVHZqVRPS3mdj48PJk2ahJUrV+K7774D8OpD3+sLgBkbG0tWMC+Oq6srOnfujICAACxZsgS1a9dGSkoKxo8fD19fX3HWxaxZs/D06VN07doV9vb2ePz4MZYvX478/HzJh2tBEJQuRla1alXxg3VeXh4yMjJQUFCAe/fuIT4+HsHBwejevTsGDx5cas5hYWFo0aIFPvvsM4V9n3zyCcLCwvDDDz8o7IuMjMRPP/2k8IasLBgZGWH16tXw9fVFt27dMHbsWNStWxc5OTmIj48HAGhqagIAPDw8MG7cOEyaNAkvXrxAr169kJ+fj7Vr12LZsmVYunSpuBaDn58fYmNj4e/vjx9++AEdO3ZEdnY2Vq5cia1btyI2NlbyOsvbXxAEPH78GEeOHMGCBQtgamqKkJAQAEBmZiZ8fHwQEBCAhg0bwtjYGCdPnsSiRYvQs2fPMm8bOQ8PDzRs2BALFiyQPO5WztDQEKtXr4aPjw8+//xzjB07Fo6Ojnjw4AFiYmKQlpaGqKgoMT4zM1Ohr5mZmUFPT0+tvN62X+jp6WH27NkYNWrUGx1PRFQRfH198e+//2LmzJnIyMhA48aNER8fLy4ympaWpjAgnpKSgoMHD2LXrl0VkTIRERG9plIPbmhpaWH06NFYtGgRRo4cCQDityxFBQcHY+rUqSrVGR0djaCgIHz99de4e/cuatSogd69e2PGjBliTNu2bbFy5UoMHjwY9+7dQ5UqVdCkSRPs2rVL8o1MdnY2bG1tFc6Rnp4u3scbHx8PW1tbaGlpoUqVKmjUqBGWL18Of3//EmcWAK8Wr1y7dm2xC1P26dMHS5YswYIFCxT26evrQ19fX6U2eRO9e/fG4cOHsXDhQgwePBgPHz6EqakpmjdvjqioKHTv3l2MXbp0KRo2bIiffvoJ06dPh6amJpo2bYrNmzdLnowik8kQExODpUuXIjQ0FN988w309PTg4eGBxMREtG7dWpKDvP1lMhlMTEzg7OwMf39/jBs3TrwFwsjICO7u7ggNDcX169eRn58POzs7DB8+HN9//325tQ8AfPvttxgyZAimTJmidDHNnj174vDhwwgODkb//v2RnZ0NOzs7dOjQAfPmzZPEenp6Khy/fv16+Pn5qZVTWfQLf39/LFmyBJcuXXqreoiI3qXRo0dj9OjRSvclJiYqlDk7O6u1PpKqi4QTERHRm5EJZbFCIxHRW8rOzn711JTxMdDQNajodIjoA3YrpFvpQURERFRh5J8NsrKyVF6br9KuuUFEREREREREBHBw4723bt06GBkZKd1cXV2ZUzmr7Nfq6upabP7r1q2r6PSIiIiIiIjKRKVec+Nj8Pnnn8Pd3V3pPm1t7XeczSvvY07lpbJf659//on8/Hyl++QL5REREREREVV2HNx4zxkbG8PY2Lii05B4H3MqL5X9Wu3t7Ss6BSIiIiIionLH21KIiIiIiIiIqFLj4AYRERERERERVWq8LYWI3isXZnup/LgnIiIiIiIigDM3iIiIiIiIiKiS4+AGEREREREREVVqHNwgIiIiIiIiokqNgxtEREREREREVKlxcIOIiIiIiIiIKjU+LYWI3isNgnZCQ9egotMgokriVki3ik6BiIiI3gOcuUFERERERERElRoHN4iIiIiIiIioUuPgBhERERERERFVahzcICIiIiIiIqJKjYMbRERERERERFSpcXCDiIiIiIiIiCo1Dm4QERHRB2flypVwcHCAnp4e3N3dcfz48WJj27VrB5lMprB16/Z/j5mNi4tD586dYWFhAZlMhqSkpHdwFURERKSqdza4kZGRgTFjxqB27drQ1dWFnZ0devTogYSEBDHmzJkz8PHxgbW1NfT09FC3bl0MHz4cV65cKbX+W7duQSaToWrVqnjy5IlkX+PGjTFr1izxZwcHByxdulShjlmzZqFx48biz0OGDIFMJsOIESMUYkeNGgWZTIYhQ4aUmpu8rl69einUHRISIonbvHkzZDKZpGzVqlVo1KgRjIyMYGZmhiZNmiA4OFi8FmVvyORb0fy8vLygqamJEydOlJrf6+TniYqKUtjn6uoKmUyGyMhIhfjXN/n1qvJ6yWNK2oqeU5nExETIZDK4urqioKBAss/MzExyvEwmw+bNm0ttG/mb4NdfOwDo1q0bZDKZpL+9/qbZ2toaPj4+SE1NFWNKutajR48CACIjI8UyDQ0N2NrawtfXF2lpaSW2QVE3b95E//79Ua1aNejp6aFGjRro2bMnLl++LMlD2Zv2du3aYfz48eLP6vYJIqJ3JTo6GhMmTEBQUBBOnz6NRo0awcvLC/fv31caHxcXh/T0dHG7cOECNDU14ePjI8bk5ubi008/xcKFC9/VZRAREZEa3sngxq1bt9CsWTP8/fff+OGHH3D+/HnEx8ejffv2GDVqFABg+/btaNmyJfLy8rBu3TokJydj7dq1MDU1xYwZM1Q+15MnT7B48eIyy93Ozg5RUVF49uyZWPb8+XP88ccfqFmz5lvVraenh4ULF+LRo0fFxoSHh2P8+PEYO3YskpKScOjQIUyePBk5OTkAgBMnTohvxjZu3AgASElJEcuWLVsGAEhLS8Phw4cxevRohIeHv1G+dnZ2iIiIkJQdPXoUGRkZMDQ0VIifM2eO5M1ieno6xowZI4kp6fWys7OTHDtx4kS4urpKynx9fVXK/caNG/jtt99UvNLS2dnZKXxwv3PnDhISEmBra6sQP3z4cKSnp+Pu3bvYsmULbt++jYEDByrE7dmzR6HNmjVrJu43MTFBeno67ty5g40bNyIlJUXy5rsk+fn56NSpE7KyshAXF4eUlBRER0fDzc0Njx8/Vuv65dTtE0RE78KPP/6I4cOHY+jQoahfvz5+/vlnGBgYFPvvn7m5OWxsbMRt9+7dMDAwkPx9HTRoEGbOnAlPT893dRlERESkBq13cZJvvvkGMpkMx48fl3zgcXV1RUBAAJ4+fYqhQ4eia9eu2LRpk7i/Vq1acHd3V+uD15gxY/Djjz9i1KhRqFq16lvn3rRpU1y/fh1xcXEYMGAAgFff8NSsWRO1atV6q7o9PT1x7do1BAcHY9GiRUpjtm7dir59+yIwMFAsc3V1Ff/fyspK/H9zc3MAQNWqVWFmZiapJyIiAt27d8fIkSPRsmVL/Pjjj9DX11cr3wEDBiA0NBS3b9+GnZ0dgFeDLwMGDFA6cGBsbAwbG5sS6yzp9dLU1JQcb2RkBC0trVLrLO48QUFB6N+/P3R1ddU+/nXdu3dHTEwMDh06hNatWwMA1qxZg86dOyudSWFgYCDmbWtri9GjR+Prr79WiLOwsCjx+mQymaSewMBAjB07FtnZ2TAxMSkx54sXL+L69etISEiAvb09AMDe3l7M/02o2yeIiMrbixcvcOrUKUybNk0s09DQgKenJ44cOaJSHWFhYfDz8+MgLRERUSVS7jM3Hj58iPj4eIwaNUrpmwQzMzPs3LkTDx48wOTJk5XW8foH9ZL069cPjo6OmDNnzpumrCAgIEDy7XR4eDiGDh361vVqampiwYIFWLFiBf755x+lMTY2Njh69KjkFgZ1CYKAiIgIDBw4EC4uLnB0dMSGDRvUrsfa2hpeXl5Ys2YNAODp06eIjo5GQEDAG+dWHq+XMuPHj8fLly+xYsWKMqlPR0cHAwYMkPSLyMhIldri4cOHiImJgbu7+1vlcP/+fWzatAmamprQ1NQsNd7KygoaGhrYsGGDwi06b+pt+kReXh6ys7MlGxHR23rw4AEKCgpgbW0tKbe2tkZGRkapxx8/fhwXLlzAsGHDyitFIiIiKgflPrhx7do1CIIAFxeXYmOuXr0KACXGqEq+FsKvv/6K69evv3V9ADBw4EAcPHgQqampSE1NxaFDh5TeUvAmevfujcaNGyMoKEjp/qCgIJiZmcHBwQHOzs4YMmQIYmJiUFhYqPI59uzZg6dPn8LLy0u8nrCwsDfKNyAgAJGRkRAEARs2bECdOnUk65QUNWXKFBgZGUm2AwcOSGLK4/VSxsDAAEFBQQgODkZWVlaZ1BkQEICYmBjk5uZi//79yMrKQvfu3ZXG/vTTTzAyMoKhoSEsLCyQkpKidHp0q1atFNqsqKysLLEea2tr7N27t9iBw9dVr14dy5cvx8yZM1GlShV06NABc+fOxY0bN96sAf4/dfpEUcHBwTA1NRU3+cwPIqKKFBYWBjc3N7Ro0aKiUyEiIiI1lPvghiAIZRKjDi8vL3z66adqrdVREisrK3Tr1g2RkZGIiIhAt27dYGlpWSZ1A8DChQuxZs0aJCcnK+yztbXFkSNHcP78eYwbNw4vX76Ev78/vL29VR7gCA8Ph6+vL7S0Xt2F1K9fPxw6dOiNBhO6deuGnJwc7N+/H+Hh4SV+Qz9p0iQkJSVJtubNmyvElfXrVZzAwEBYWFiU2WJwjRo1Qt26dbFhwwaEh4dj0KBBYhu/bsCAAUhKSsLZs2dx8OBBODo6onPnzgqLqUZHRyu0WVHGxsZISkrCyZMnsWTJEjRt2hTz589XOedRo0YhIyMD69atg4eHB2JjY+Hq6ordu3erff1y6vSJoqZNm4asrCxxu3379hvnQEQkZ2lpCU1NTdy7d09Sfu/evVJva8zNzUVUVJTkVlAiIiKqHMp9cKNu3bqQyWTi0xiUcXJyAoASY9QVEhKC6OhonDlzRmGfiYmJ0m/vHz9+DFNTU6X1yb+dXrNmzVvdhqHMZ599Bi8vL8n9wa9r0KABvvnmG6xduxa7d+/G7t27sW/fvlLrfvjwITZt2oSffvoJWlpa0NLSQvXq1fHy5cs3WlhUS0sLgwYNQlBQEI4dOyauQ6KMpaUlHB0dJVtx63yU9HqVFS0tLcyfPx/Lli3D3bt3FfYbGxu/Ub9YuXIlNmzYUGK/MDU1FdugdevWCAsLw9WrVxEdHS2Js7OzU2izojQ0NODo6Ih69ephwoQJaNmyJUaOHKnK5Uuus0ePHpg/fz7Onj2LNm3aYN68eQAgrtuhTjuo0yeK0tXVhYmJiWQjInpbOjo6aNasmeRpbIWFhUhISICHh0eJx8bGxiIvL6/MZmcSERHRu1Pugxvm5ubw8vLCypUrkZubq7D/8ePH6Ny5MywtLYtdVPNNnuTQokULfPHFF5g6darCPmdnZ5w6dUqh/PTp0+JAy+u8vb3x4sUL5Ofni7d3lKWQkBBs27ZNpcXO6tevDwBK2/N169atQ40aNXD27FnJbIAlS5YgMjLyjdZeCAgIwL59+9CzZ09UqVJF7eOVKen1Kks+Pj5wdXXF7NmzFfYp6xcFBQU4e/Zssf2if//+OH/+PBo0aCC+LqqQr5FR9Ck8b2Lq1KmIjo7G6dOn3+h4mUwGFxcXsS+Zm5vD0tJSoR2ys7Nx7dq1YtuhPPoEEdGbmjBhAlatWiXOihw5ciRyc3PF9bIGDx6s9AuFsLAw9OrVCxYWFgr7Hj58iKSkJFy6dAnAqyeTJSUlqbSOBxEREZW/d/K0lJUrV6J169Zo0aIF5syZg4YNG+Lly5fYvXs3/ve//yE5ORmrV6+Gj48PPv/8c4wdOxaOjo548OABYmJikJaWhqioKLXPO3/+fLi6uircKvDtt9+iTZs2mD9/Pr744gsUFBRg/fr1OHLkCH766SeldWlqaoq3jaiyeKO63NzcMGDAACxfvlxSPnLkSFSrVg0dOnRAjRo1kJ6ejnnz5sHKyqrUb6CAV2/UvvzySzRo0EBSbmdnh2nTpiE+Ph7dunUD8Orb+tdvg7CwsFBYC6FevXp48OABDAwMSjz3kydPFN70GRgYFPsNfXGvV1kLCQlROkA1YcIEBAYGwsXFBZ06dUJubi5WrFiBR48eFbuwXJUqVZCeng5tbe0Sz/n06VOxLe7du4e5c+dCT08PnTt3lsRlZmYqtJmZmRn09PSU1mtnZ4fevXtj5syZ2L59e4k5JCUlISgoCIMGDUL9+vWho6ODffv2ITw8HFOmTJG0w4IFC2BtbY2WLVsiMzMTc+fOhZWVFb744guldavaJ4iI3gVfX1/8+++/mDlzJjIyMtC4cWPEx8eLi4ympaVBQ0P6/U5KSgoOHjyIXbt2Ka1z69atksXE/fz8ALxaG2vWrFnlcyFERESksnKfuQEAtWvXxunTp9G+fXtMnDgRDRo0QKdOnZCQkID//e9/AICePXvi8OHD0NbWRv/+/eHi4oJ+/fohKytLnDKvLicnJwQEBOD58+eS8latWuGvv/7CX3/9hdatW6Ndu3Y4fPgwEhISFAYBiirvqfNz5sxRWEfD09MTR48ehY+PD5ycnNCnTx/o6ekhISFB6TdLRZ06dQpnz55Fnz59FPaZmpqiY8eOkoVFExMT0aRJE8mmbIYD8GrQo7RHyc6cORO2traSrbgn4gDFv15lrUOHDujQoQNevnwpKe/Xrx9Wr16N8PBwNGvWDN7e3sjIyMD+/fsVVt0vyszMrNQFPVetWiW2Qfv27fHgwQP8+eefcHZ2lsR5enoqtNnmzZtLrPvbb7/Fjh07cPz48RLjatSoAQcHB8yePRvu7u5o2rQpli1bhtmzZ+M///mPGDd58mQEBQVh4cKFaNiwIfr06QNDQ0Ps3bu3xNdclT5BRPSujB49GqmpqcjLy8OxY8ckT6hKTExEZGSkJN7Z2RmCIKBTp05K6xsyZAgEQVDYOLBBRET0fpAJZb2aJxHRG8jOzn711JTxMdDQ5QwQIlLNrZBuFZ0CERERlTH5Z4OsrCyVJxi8k5kbRERERERERETlpdIMbowYMQJGRkZKtxEjRlRobmlpacXmZmRkhLS0tArN70PWpUuXYtt9wYIFFZ3eO3XgwIES+yEREREREdGHqtLclnL//n1kZ2cr3WdiYoKqVau+44z+z8uXL3Hr1q1i9zs4OJT7Ipkfqzt37hT7xBFzc3OYm5u/44wqzrNnz3Dnzp1i97/+WNn3DW9LIaI3wdtSiIiIPjxvcltKpfnEXbVq1QodwCiJlpbWe//B8UNVvXr1ik7hvaGvr89+SEREREREH6VKc1sKEREREREREZEylWbmBhF9HC7M9irXRy4TEREREdGHhzM3iIiIiIiIiKhS4+AGEREREREREVVqHNwgIiIiIiIiokqNgxtEREREREREVKlxcIOIiIiIiIiIKjUObhARERERERFRpcZHwRLRe6VB0E5o6BpUdBpE9J67FdKtolMgIiKi9whnbhARERERERFRpcbBDSIiIiIiIiKq1Di4QURERERERESVGgc3iIiIiIiIiKhS4+AGEREREREREVVqHNwgIiKiD8bKlSvh4OAAPT09uLu74/jx48XGtmvXDjKZTGHr1u3/nsQiCAJmzpwJW1tb6Ovrw9PTE1evXn0Xl0JERERq4OAGERERfRCio6MxYcIEBAUF4fTp02jUqBG8vLxw//59pfFxcXFIT08XtwsXLkBTUxM+Pj5izKJFi7B8+XL8/PPPOHbsGAwNDeHl5YXnz5+/q8siIiIiFXBwg97KkCFDlH7rde3aNQwZMgS9evUqtY5//vkHOjo6aNCggdL9giBg1apV8PDwgImJCYyMjODq6opx48bh2rVrKuU5a9YsMTctLS1YWlris88+w9KlS5GXl6cQf/HiRfTt2xdWVlbQ1dWFk5MTZs6ciadPnyrEnjlzBj4+PrC2toaenh7q1q2L4cOH48qVKwCAxMREyGQyPH78WOFYBwcHLF26VPxZnuPRo0clcXl5ebCwsIBMJkNiYqJC/OtbVFSU5Nyurq4oKCiQ1GlmZobIyEgxpqSt6DmVOXjwIFq3bg0LCwvo6+vDxcUFoaGhJR5DRFTWfvzxRwwfPhxDhw5F/fr18fPPP8PAwADh4eFK483NzWFjYyNuu3fvhoGBgTi4IQgCli5diunTp6Nnz55o2LAhfvvtN9y9exebN29+h1dGREREpeHgBr01b29vyTdf6enpqFWrlsrHR0ZGom/fvsjOzsaxY8ck+wRBQP/+/TF27Fh07doVu3btwqVLlxAWFgY9PT3MmzdP5fO4uroiPT0daWlp2Lt3L3x8fBAcHIxWrVrhyZMnYtzRo0fh7u6OFy9eYMeOHbhy5Qrmz5+PyMhIdOrUCS9evBBjt2/fjpYtWyIvLw/r1q1DcnIy1q5dC1NTU8yYMUPl3Iqys7NDRESEpGzTpk0wMjJSGh8REaHQ/q8PKt24cQO//fab0uNbtWolObZv374Kr2mrVq1KzNnQ0BCjR4/G/v37kZycjOnTp2P69On49ddfVb9wIqK38OLFC5w6dQqenp5imYaGBjw9PXHkyBGV6ggLC4Ofnx8MDQ0BADdv3kRGRoakTlNTU7i7u6tcJxEREb0bWhWdAFV+urq6sLGxeaNjBUFAREQEfvrpJ9SoUQNhYWFwd3cX90dHRyMqKgpbtmzB559/LpbXrFkTLVu2hCAIKp9LS0tLzLNatWpwc3NDp06d0KhRIyxcuBDz5s2DIAgIDAxEvXr1EBcXBw2NV+N/9vb2cHJyQpMmTRAaGoopU6bg6dOnGDp0KLp27YpNmzaJ56lVqxbc3d2VztRQhb+/P5YvX46lS5dCX18fABAeHg5/f3/MnTtXId7MzKzU9h8zZgyCgoLQv39/6OrqSvbp6OhIjtfX10deXp5ar2mTJk3QpEkT8WcHBwfExcXhwIED+Oqrr1Suh4joTT148AAFBQWwtraWlFtbW+Py5culHn/8+HFcuHABYWFhYllGRoZYx+t1yvcRERHR+4EzN6hC7d27F0+fPoWnpycGDhyIqKgo5ObmivvXr18PZ2dnycBGUTKZ7K3O7+Ligi5duiAuLg4AkJSUhEuXLmHChAniwIZco0aN4OnpifXr1wMAdu7ciQcPHmDy5MlK6zYzM3ujnJo1awYHBwds3LgRAJCWlob9+/dj0KBBb1QfAIwfPx4vX77EihUr3rgOdZw5cwaHDx9G27Zti43Jy8tDdna2ZCMiqihhYWFwc3NDixYtKjoVIiIiegMc3KC3tn37dhgZGYlb0YXYSiOfAqypqYkGDRqgdu3aiI2NFfdfuXIFzs7OkmPGjx8vnqtGjRpvnb+Liwtu3bolng8A6tWrpzS2Xr16Yox8tXwXF5e3zuF1AQEB4j3ikZGR6Nq1K6ysrJTG9uvXT9L+RkZGSEtLk8QYGBggKCgIwcHByMrKKvN85WrUqAFdXV00b94co0aNwrBhw4qNDQ4OhqmpqbjZ2dmVW15E9OGztLSEpqYm7t27Jym/d+9eqTPRcnNzERUVhcDAQEm5/Lg3qZOIiIjeLQ5u0Ftr3749kpKSxG358uUqHff48WPExcVh4MCBYtnAgQMlU4KV+c9//oOkpCTMnDkTOTk5b5U78OrWmNdngKhyu4s6t8Soa+DAgThy5Ahu3LiByMhIBAQEFBsbGhoqaf+kpCRUq1ZNIS4wMBAWFhZYuHBhueV94MABnDx5Ej///DOWLl0qznJRZtq0acjKyhK327dvl1teRPTh09HRQbNmzZCQkCCWFRYWIiEhAR4eHiUeGxsbi7y8PMm/R8Cr2wxtbGwkdcrXhyqtTiIiInq3uOYGvTVDQ0M4Ojqqfdwff/yB58+fS9bYEAQBhYWFuHLlCpycnFC3bl2kpKRIjrOysoKVlRWqVq361rkDQHJysrgAqpOTk1hWdA2JorHyGPl/L1++XOKbXBMTEwBAVlaWwq0qjx8/hqmpqcIxFhYW6N69OwIDA/H8+XN06dJFsuhpUTY2Niq1v5aWFubPn48hQ4Zg9OjRpca/CXk7urm54d69e5g1axb69eunNFZXV1dh/Q8iorcxYcIE+Pv7o3nz5mjRogWWLl2K3NxcDB06FAAwePBgVK9eHcHBwZLjwsLC0KtXL1hYWEjKZTIZxo8fj3nz5qFu3bqoVasWZsyYgWrVqqn0NDAiIiJ6dzhzgypMWFgYJk6cKJlxcPbsWbRp00a8JaNfv35ISUnBli1byiWHy5cvIz4+Hn369AEANG7cWHyMaWFhoST27Nmz2LNnj/hhvXPnzrC0tMSiRYuU1i1fULRu3brQ0NDAqVOnJPtv3LiBrKwscZDkdQEBAUhMTMTgwYOhqan5Npcp8vHxgaurK2bPnl0m9ZWksLBQ6WN2iYjKi6+vLxYvXoyZM2eicePGSEpKQnx8vLggaFpaGtLT0yXHpKSk4ODBgwq3pMhNnjwZY8aMwVdffYVPPvkEOTk5iI+Ph56eXrlfDxEREamOMzeoXGVlZSEpKUlSZmFhgczMTJw+fRrr1q1TWLOiX79+mDNnDubNmwc/Pz/ExcXBz88P06ZNg5eXF6ytrZGamoro6Gi1PvS/fPkSGRkZKCwsRGZmJhITEzFv3jw0btwYkyZNAvDqW7qwsDB06tQJffr0wbRp02BjY4Njx45h4sSJ8PDwwPjx4wG8mrGyevVq+Pj44PPPP8fYsWPh6OiIBw8eICYmBmlpaYiKioKxsTGGDRuGiRMnQktLC25ubrh9+zamTJmCli1bFvuYVW9vb/z777/izI/iPH78WGHVfmNjY/FRhq8LCQmBl5eXyu2mipUrV6JmzZria7l//34sXrwYY8eOLdPzEBGVZvTo0cXOTktMTFQoc3Z2LvE2Q5lMhjlz5mDOnDlllSIRERGVAw5uULlKTExUuL0jMDAQ+vr6qF+/vtLFOHv37o3Ro0fjzz//xOeff47o6GisWrUKERERWLRoEfLz81GjRg107NgRP/74o8q5XLx4Eba2ttDU1ISpqSnq16+PadOmYeTIkZLbI1q1aoWjR49i9uzZ4u0gNWvWhL+/P6ZNmyaJ7dmzJw4fPozg4GD0798f2dnZsLOzQ4cOHTBv3jwxbtmyZQgJCcGUKVOQmpoKGxsbdOrUCfPnzy/2iS8ymQyWlpalXpd8unVRwcHBmDp1qtL4Dh06oEOHDti1a1epdauqsLAQ06ZNw82bN6GlpYU6depg4cKF+Prrr8vsHERERERERMWRCeW5KiIRkYqys7NfPTVlfAw0dA0qOh0ies/dCulW0SkQERFROZF/NsjKyip1Jrsc19wgIiIiIiIiokqNgxv0QTAyMip2O3DgQEWnV+m5uroW277r1q2r6PSIiIiIiOgjxzU36IPw+qKlRVWvXv3dJfKB+vPPP5Gfn690n/wpBERERERERBWFgxv0QXB0dKzoFD5o9vb2FZ0CERERERFRsXhbChERERERERFVapy5QUTvlQuzvVReEZmIiIiIiAjgzA0iIiIiIiIiquQ4c4OI3guCIAB49UxrIiIiIiL6eMk/E8g/I6iCgxtE9F7IzMwEANjZ2VVwJkRERERE9D548uQJTE1NVYrl4AYRvRfMzc0BAGlpaSr/AaOykZ2dDTs7O9y+fZvrnVQAtn/FYdtXHLZ9xWHbVxy2fcVi+1ecN2l7QRDw5MkTVKtWTeXzcHCDiN4LGhqvlgAyNTXlPzgVxMTEhG1fgdj+FYdtX3HY9hWHbV9x2PYVi+1fcdRte3W/8OSCokRERERERERUqXFwg4iIiIiIiIgqNQ5uENF7QVdXF0FBQdDV1a3oVD46bPuKxfavOGz7isO2rzhs+4rDtq9YbP+K867aXiao82wVIiIiIiIiIqL3DGduEBEREREREVGlxsENIiIiIiIiIqrUOLhBRERERERERJUaBzeIiIiIiIiIqFLj4AYRERERERERVWoc3CCicrNy5Uo4ODhAT08P7u7uOH78eInxsbGxcHFxgZ6eHtzc3PDnn39K9guCgJkzZ8LW1hb6+vrw9PTE1atXy/MSKi112n7VqlVo06YNqlSpgipVqsDT01MhfsiQIZDJZJLN29u7vC+jUlKn7SMjIxXaVU9PTxLDfq86ddq+Xbt2Cm0vk8nQrVs3MYb9XjX79+9Hjx49UK1aNchkMmzevLnUYxITE9G0aVPo6urC0dERkZGRCjHq/hvyMVK37ePi4tCpUydYWVnBxMQEHh4e2LlzpyRm1qxZCv3excWlHK+i8lK3/RMTE5X+3cnIyJDEse+XTt22V/b3XCaTwdXVVYxh3y9dcHAwPvnkExgbG6Nq1aro1asXUlJSSj3uXb3H5+AGEZWL6OhoTJgwAUFBQTh9+jQaNWoELy8v3L9/X2n84cOH0a9fPwQGBuLMmTPo1asXevXqhQsXLogxixYtwvLly/Hzzz/j2LFjMDQ0hJeXF54/f/6uLqtSULftExMT0a9fP+zduxdHjhyBnZ0dOnfujDt37kjivL29kZ6eLm7r169/F5dTqajb9gBgYmIiadfU1FTJfvZ71ajb9nFxcZJ2v3DhAjQ1NeHj4yOJY78vXW5uLho1aoSVK1eqFH/z5k1069YN7du3R1JSEsaPH49hw4ZJPmS/ye/Sx0jdtt+/fz86deqEP//8E6dOnUL79u3Ro0cPnDlzRhLn6uoq6fcHDx4sj/QrPXXbXy4lJUXSvlWrVhX3se+rRt22X7ZsmaTNb9++DXNzc4W/+ez7Jdu3bx9GjRqFo0ePYvfu3cjPz0fnzp2Rm5tb7DHv9D2+QERUDlq0aCGMGjVK/LmgoECoVq2aEBwcrDS+b9++Qrdu3SRl7u7uwtdffy0IgiAUFhYKNjY2wg8//CDuf/z4saCrqyusX7++HK6g8lK37V/38uVLwdjYWFizZo1Y5u/vL/Ts2bOsU/3gqNv2ERERgqmpabH1sd+r7m37fWhoqGBsbCzk5OSIZez36gMgbNq0qcSYyZMnC66urpIyX19fwcvLS/z5bV/Pj5Eqba9M/fr1hdmzZ4s/BwUFCY0aNSq7xD4SqrT/3r17BQDCo0ePio1h31ffm/T9TZs2CTKZTLh165ZYxr6vvvv37wsAhH379hUb8y7f43PmBhGVuRcvXuDUqVPw9PQUyzQ0NODp6YkjR44oPebIkSOSeADw8vIS42/evImMjAxJjKmpKdzd3Yut82P0Jm3/uqdPnyI/Px/m5uaS8sTERFStWhXOzs4YOXIkMjMzyzT3yu5N2z4nJwf29vaws7NDz549cfHiRXEf+71qyqLfh4WFwc/PD4aGhpJy9vuyV9rf+7J4PUk1hYWFePLkicLf+6tXr6JatWqoXbs2BgwYgLS0tArK8MPUuHFj2NraolOnTjh06JBYzr7/7oSFhcHT0xP29vaScvZ99WRlZQGAwt+Qot7le3wObhBRmXvw4AEKCgpgbW0tKbe2tla4r1QuIyOjxHj5f9Wp82P0Jm3/uilTpqBatWqSf2S8vb3x22+/ISEhAQsXLsS+ffvQpUsXFBQUlGn+ldmbtL2zszPCw8OxZcsWrF27FoWFhWjVqhX++ecfAOz3qnrbfn/8+HFcuHABw4YNk5Sz35eP4v7eZ2dn49mzZ2Xyd4xUs3jxYuTk5KBv375imbu7OyIjIxEfH4///e9/uHnzJtq0aYMnT55UYKYfBltbW/z888/YuHEjNm7cCDs7O7Rr1w6nT58GUDb/hlPp7t69i7/++kvhbz77vnoKCwsxfvx4tG7dGg0aNCg27l2+x9dSK5qIiD5oISEhiIqKQmJiomRhSz8/P/H/3dzc0LBhQ9SpUweJiYno2LFjRaT6QfDw8ICHh4f4c6tWrVCvXj388ssvmDt3bgVm9nEJCwuDm5sbWrRoISlnv6cP2R9//IHZs2djy5YtkjUfunTpIv5/w4YN4e7uDnt7e8TExCAwMLAiUv1gODs7w9nZWfy5VatWuH79OkJDQ/H7779XYGYflzVr1sDMzAy9evWSlLPvq2fUqFG4cOHCe7UuCWduEFGZs7S0hKamJu7duycpv3fvHmxsbJQeY2NjU2K8/L/q1PkxepO2l1u8eDFCQkKwa9cuNGzYsMTY2rVrw9LSEteuXXvrnD8Ub9P2ctra2mjSpInYruz3qnmbts/NzUVUVJRKb1zZ78tGcX/vTUxMoK+vXya/S1SyqKgoDBs2DDExMQrTxV9nZmYGJycn9vty0qJFC7Ft2ffLnyAICA8Px6BBg6Cjo1NiLPt+8UaPHo3t27dj7969qFGjRomx7/I9Pgc3iKjM6ejooFmzZkhISBDLCgsLkZCQIPmWuigPDw9JPADs3r1bjK9VqxZsbGwkMdnZ2Th27FixdX6M3qTtgVerVM+dOxfx8fFo3rx5qef5559/kJmZCVtb2zLJ+0Pwpm1fVEFBAc6fPy+2K/u9at6m7WNjY5GXl4eBAweWeh72+7JR2t/7svhdouKtX78eQ4cOxfr16yWPPi5OTk4Orl+/zn5fTpKSksS2Zd8vf/v27cO1a9dUGtBm31ckCAJGjx6NTZs24e+//0atWrVKPeadvsdXa/lRIiIVRUVFCbq6ukJkZKRw6dIl4auvvhLMzMyEjIwMQRAEYdCgQcLUqVPF+EOHDglaWlrC4sWLheTkZCEoKEjQ1tYWzp8/L8aEhIQIZmZmwpYtW4Rz584JPXv2FGrVqiU8e/bsnV/f+0zdtg8JCRF0dHSEDRs2COnp6eL25MkTQRAE4cmTJ8J3330nHDlyRLh586awZ88eoWnTpkLdunWF58+fV8g1vq/UbfvZs2cLO3fuFK5fvy6cOnVK8PPzE/T09ISLFy+KMez3qlG37eU+/fRTwdfXV6Gc/V51T548Ec6cOSOcOXNGACD8+OOPwpkzZ4TU1FRBEARh6tSpwqBBg8T4GzduCAYGBsKkSZOE5ORkYeXKlYKmpqYQHx8vxpT2etIr6rb9unXrBC0tLWHlypWSv/ePHz8WYyZOnCgkJiYKN2/eFA4dOiR4enoKlpaWwv3799/59b3v1G3/0NBQYfPmzcLVq1eF8+fPC+PGjRM0NDSEPXv2iDHs+6pRt+3lBg4cKLi7uyutk32/dCNHjhRMTU2FxMREyd+Qp0+fijEV+R6fgxtEVG5WrFgh1KxZU9DR0RFatGghHD16VNzXtm1bwd/fXxIfExMjODk5CTo6OoKrq6uwY8cOyf7CwkJhxowZgrW1taCrqyt07NhRSElJeReXUumo0/b29vYCAIUtKChIEARBePr0qdC5c2fByspK0NbWFuzt7YXhw4fzjVYx1Gn78ePHi7HW1tZC165dhdOnT0vqY79Xnbp/cy5fviwAEHbt2qVQF/u96uSPt3x9k7e3v7+/0LZtW4VjGjduLOjo6Ai1a9cWIiIiFOot6fWkV9Rt+7Zt25YYLwivHstra2sr6OjoCNWrVxd8fX2Fa9euvdsLqyTUbf+FCxcKderUEfT09ARzc3OhXbt2wt9//61QL/t+6d7k787jx48FfX194ddff1VaJ/t+6ZS1OQDJ3/CKfI8v+/9JEhERERERERFVSlxzg4iIiIiIiIgqNQ5uEBEREREREVGlxsENIiIiIiIiIqrUOLhBRERERERERJUaBzeIiIiIiIiIqFLj4AYRERERERERVWoc3CAiIiIiIiKiSo2DG0REREREZejWrVuYN28ecnJyKjoVIqKPBgc3iIiIiEht7dq1w/jx4ys6jfdOXl4efHx8YGlpCSMjo1LjHRwcsHTp0jc+X2RkJMzMzN74eCKiDwUHN4iIiOiDM2TIEPTq1aui0yjWrVu3IJPJkJSUVNGpkJpK61vffvstOnfujBEjRqhU34kTJ/DVV1+pFKtsIMTX1xdXrlxR6Xgiog+ZVkUnQERERPQxefHiRUWn8FF68eIFdHR0yv08P/30k0px8nysrKze6nz6+vrQ19d/qzqIiD4EnLlBREREH7x27dphzJgxGD9+PKpUqQJra2usWrUKubm5GDp0KIyNjeHo6Ii//vpLPCYxMREymQw7duxAw4YNoaenh5YtW+LChQuSujdu3AhXV1fo6urCwcEBS5Yskex3cHDA3LlzMXjwYJiYmOCrr75CrVq1AABNmjSBTCZDu3btALz6Fr9Tp06wtLSEqakp2rZti9OnT0vqk8lkWL16NXr37g0DAwPUrVsXW7dulcRcvHgR3bt3h4mJCYyNjdGmTRtcv35d3L969WrUq1cPenp6cHFxKfUDeW5uLgYPHgwjIyPY2toqXCPw6naM7777DtWrV4ehoSHc3d2RmJgo7k9NTUWPHj1QpUoVGBoawtXVFX/++Wex58zLy8OUKVNgZ2cHXV1dODo6IiwsDABQUFCAwMBA1KpVC/r6+nB2dsayZcskx8tnWMyfPx/VqlWDs7MzAOD3339H8+bNYWxsDBsbG/Tv3x/3799Xqf1mzZqFNWvWYMuWLZDJZJDJZOI13r59G3379oWZmRnMzc3Rs2dP3Lp1q9R8is7GEAQBs2bNQs2aNaGrq4tq1aph7NixAF714dTUVHz77bfiuQHlt6WEhITA2toaxsbGCAwMxNSpU9G4cWNxv7Jbinr16oUhQ4aU2+tJRFTeOLhBREREH4U1a9bA0tISx48fx5gxYzBy5Ej4+PigVatWOH36NDp37oxBgwbh6dOnkuMmTZqEJUuW4MSJE7CyskKPHj2Qn58PADh16hT69u0LPz8/nD9/HrNmzcKMGTMQGRkpqWPx4sVo1KgRzpw5gxkzZuD48eMAgD179iA9PR1xcXEAgCdPnsDf3x8HDx7E0aNHUbduXXTt2hVPnjyR1Dd79mz07dsX586dQ9euXTFgwAA8fPgQAHDnzh189tln0NXVxd9//41Tp04hICAAL1++BACsW7cOM2fOxPz585GcnIwFCxZgxowZWLNmTbFtN2nSJOzbtw9btmzBrl27kJiYqDDoMnr0aBw5cgRRUVE4d+4cfHx84O3tjatXrwIARo0ahby8POzfvx/nz5/HwoULS1yTYvDgwVi/fj2WL1+O5ORk/PLLL2J8YWEhatSogdjYWFy6dAkzZ87E999/j5iYGEkdCQkJSElJwe7du7F9+3YAQH5+PubOnYuzZ89i8+bNuHXrluRDfUnt991336Fv377w9vZGeno60tPT0apVK+Tn58PLywvGxsY4cOAADh06BCMjI3h7e0tm6ijLp6iNGzciNDQUv/zyC65evYrNmzfDzc0NABAXF4caNWpgzpw54rmViYmJwaxZs7BgwQKcPHkStra2Ks8mKaqsX08ionInEBEREX1g/P39hZ49e4o/t23bVvj000/Fn1++fCkYGhoKgwYNEsvS09MFAMKRI0cEQRCEvXv3CgCEqKgoMSYzM1PQ19cXoqOjBUEQhP79+wudOnWSnHvSpElC/fr1xZ/t7e2FXr16SWJu3rwpABDOnDlT4nUUFBQIxsbGwrZt28QyAML06dPFn3NycgQAwl9//SUIgiBMmzZNqFWrlvDixQulddapU0f4448/JGVz584VPDw8lMY/efJE0NHREWJiYsQyeTuMGzdOEARBSE1NFTQ1NYU7d+5Iju3YsaMwbdo0QRAEwc3NTZg1a1aJ1yuXkpIiABB2796tUrwgCMKoUaOEPn36iD/7+/sL1tbWQl5eXonHnThxQgAgPHnyRBCE0tvv9b4lCILw+++/C87OzkJhYaFYlpeXJ+jr6ws7d+4sMR97e3shNDRUEARBWLJkieDk5FTsuYvGykVERAimpqbizx4eHsI333wjiXF3dxcaNWok/ty2bVvxtZPr2bOn4O/vLwhC2b+eRETvAmduEBER0UehYcOG4v9ramrCwsJC/FYcAKytrQFA4RYFDw8P8f/Nzc3h7OyM5ORkAEBycjJat24tiW/dujWuXr2KgoICsax58+Yq5Xjv3j0MHz4cdevWhampKUxMTJCTk4O0tLRir8XQ0BAmJiZi3klJSWjTpg20tbUV6s/NzcX169cRGBgIIyMjcZs3b57ktpWirl+/jhcvXsDd3V2hHeTOnz+PgoICODk5Serdt2+fWO/YsWMxb948tG7dGkFBQTh37lyx7ZCUlARNTU20bdu22JiVK1eiWbNmsLKygpGREX799VeFdnJzc1NYZ+PUqVPo0aMHatasCWNjY/Ec8mNLar/inD17FteuXYOxsbF47ebm5nj+/LmkXZXlU5SPjw+ePXuG2rVrY/jw4di0aZM440ZVycnJktcKkPZhVZT160lE9C5wQVEiIiL6KLz+YVUmk0nK5GsYFBYWlvm5DQ0NVYrz9/dHZmYmli1bBnt7e+jq6sLDw0NhEVJl1yLPu6TFJXNycgAAq1atUvgArKmpqVKOxdWrqamJU6dOKdQjv1Vh2LBh8PLywo4dO7Br1y4EBwdjyZIlGDNmjEJ9pS2QGRUVhe+++w5LliyBh4cHjI2N8cMPP+DYsWOSuNfbPTc3F15eXvDy8sK6detgZWWFtLQ0eHl5iW38Jotz5uTkoFmzZli3bp3CvqILhpbWD+zs7JCSkoI9e/Zg9+7d+Oabb/DDDz9g3759ag22lEZDQwOCIEjK5LdaAWX/ehIRvQucuUFERERUgqNHj4r//+jRI1y5cgX16tUDANSrVw+HDh2SxB86dAhOTk4lDhbIv70vOrtDfuzYsWPRtWtXcZHSBw8eqJVvw4YNceDAAcmHVTlra2tUq1YNN27cgKOjo2STL3L6ujp16kBbW1sycCBvB7kmTZqgoKAA9+/fV6jXxsZGjLOzs8OIESMQFxeHiRMnYtWqVUrP6ebmhsLCQuzbt0/p/kOHDqFVq1b45ptv0KRJEzg6OhY786Soy5cvIzMzEyEhIWjTpg1cXFwUZuqU1H7Aq9fu9detadOmuHr1KqpWrapw/aampqXmVZS+vj569OiB5cuXIzExEUeOHMH58+eLPffr6tWrpzDIU7QPA68GXIqu2VFQUCBZKLesX08ioneBgxtEREREJZgzZw4SEhJw4cIFDBkyBJaWlujVqxcAYOLEiUhISMDcuXNx5coVrFmzBv/973/x3XfflVhn1apVoa+vj/j4eNy7dw9ZWVkAgLp16+L3339HcnIyjh07hgEDBqg9k2D06NHIzs6Gn58fTp48iatXr+L3339HSkoKgFeLkQYHB2P58uW4cuUKzp8/j4iICPz4449K6zMyMkJgYCAmTZqEv//+W2wHDY3/exvp5OSEAQMGYPDgwYiLi8PNmzdx/PhxBAcHY8eOHQCA8ePHY+fOnbh58yZOnz6NvXv3ioNEr3NwcIC/vz8CAgKwefNm3Lx5E4mJieKCoXXr1sXJkyexc+dOXLlyBTNmzMCJEydKbZuaNWtCR0cHK1aswI0bN7B161bMnTtXrfZzcHDAuXPnkJKSggcPHiA/Px8DBgyApaUlevbsiQMHDoj5jh07Fv/880+peclFRkYiLCwMFy5cwI0bN7B27Vro6+vD3t5ePPf+/ftx586dYge9xo0bh/DwcERERODKlSsICgrCxYsXJTEdOnTAjh07sGPHDly+fBkjR47E48ePxf1l/XoSEb0LHNwgIiIiKkFISAjGjRuHZs2aISMjA9u2bRNnXjRt2hQxMTGIiopCgwYNMHPmTMyZM0fy9A1ltLS0sHz5cvzyyy+oVq0aevbsCQAICwvDo0eP0LRpUwwaNAhjx45F1apV1crXwsICf//9N3JyctC2bVs0a9YMq1atEm9rGDZsGFavXo2IiAi4ubmhbdu2iIyMLHbmBgD88MMPaNOmDXr06AFPT098+umnaNasmSQmIiICgwcPxsSJE+Hs7IxevXrhxIkTqFmzJoBXswNGjRqFevXqwdvbG05OTiU+xeN///sfvvzyS3zzzTdwcXHB8OHDkZubCwD4+uuv8cUXX8DX1xfu7u7IzMzEN998U2rbWFlZITIyErGxsahfvz5CQkKwePFitdpv+PDhcHZ2RvPmzWFlZYVDhw7BwMAA+/fvR82aNfHFF1+gXr16CAwMxPPnz2FiYlJqXnJmZmZYtWoVWrdujYYNG2LPnj3Ytm0bLCwsALwaaLt16xbq1Kkjud2lKF9fX8yYMQOTJ09Gs2bNkJqaipEjR0piAgIC4O/vj8GDB6Nt27aoXbs22rdvL4kp69eTiKi8yYTXb7gjIiIiIiQmJqJ9+/Z49OgRzMzMKjodojc2a9YsbN68GUlJSRWdChFRueHMDSIiIiIiIiKq1Di4QURERERERESVGm9LISIiIiIiIqJKjTM3iIiIiIiIiKhS4+AGEREREREREVVqHNwgIiIiIiIiokqNgxtEREREREREVKlxcIOIiIiIiIiIKjUObhARERERERFRpcbBDSIiIiIiIiKq1Di4QURERERERESV2v8D5qP9ewHY0psAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les colonnes 'feature' et 'importance' de votre dataframe\n",
    "features = df_feature_importances_log_reg['feature'].to_numpy()\n",
    "importances = df_feature_importances_log_reg['importance'].to_numpy()\n",
    "\n",
    "# Création d'un masque pour les importances supérieures à 0\n",
    "mask = importances > 0\n",
    "\n",
    "# Filtrage des features et des importances en utilisant le masque\n",
    "filtered_feature_names = features[mask]\n",
    "filtered_feature_importances = importances[mask]\n",
    "\n",
    "# Supposons que vous vouliez afficher les 10 principales caractéristiques\n",
    "n_features_to_display = 20\n",
    "\n",
    "# Tri des indices filtrés par importance\n",
    "sorted_idx = filtered_feature_importances.argsort()\n",
    "\n",
    "# Limiter aux n principales caractéristiques\n",
    "top_n_idx = sorted_idx[-n_features_to_display:]\n",
    "\n",
    "# Affichage de l'importance des caractéristiques filtrées pour les n principales\n",
    "plt.figure(figsize=(10, len(top_n_idx) * 0.4))\n",
    "bars = plt.barh(filtered_feature_names[top_n_idx], filtered_feature_importances[top_n_idx])\n",
    "\n",
    "# Ajuster les limites de l'axe des y pour réduire l'espace autour des barres\n",
    "plt.ylim(-0.5, len(top_n_idx)-0.5)\n",
    "\n",
    "plt.xlabel(\"Importance des caractéristiques\")\n",
    "plt.title(\"Importance des caractéristiques avec la Régression Logistique\")\n",
    "\n",
    "# Ajouter les valeurs à côté des barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    \n",
    "    # Déterminez le formatage en fonction de la magnitude\n",
    "    if width < 0.01:\n",
    "        fmt = '{:.4f}'\n",
    "    elif width < 0.1:\n",
    "        fmt = '{:.3f}'\n",
    "    else:\n",
    "        fmt = '{:.2f}'\n",
    "    \n",
    "    plt.text(width + 0.01 * width,  # Ajouter un padding proportionnel à la valeur\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             fmt.format(width),\n",
    "             va='center', ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac4d80-0c4d-4eb2-b8ef-3e4b07b89c07",
   "metadata": {},
   "source": [
    "### 2.2.2 Dataset petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a9d3e77-6664-4c73-9d3d-0d3fed8f9be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 1299.50 secondes\n",
      "{'C_val': 122.46519294640589, 'threshold': 0.53}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('Logistic_Regression')\n",
    "\n",
    "nb_0 = (application_train_petit_clean['TARGET'] == 0).sum()\n",
    "nb_1 = (application_train_petit_clean['TARGET'] == 1).sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='mean')  # Vous pouvez utiliser 'median' ou 'constant' selon votre besoin\n",
    "X = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X = imputer.fit_transform(X)\n",
    "y = application_train_petit_clean[\"TARGET\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "results = []\n",
    "\n",
    "total_combinations = 50 \n",
    "\n",
    "def objective(trial):\n",
    "    C_val = trial.suggest_float('C_val', 50, 200)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LogisticRegression(C=C_val, class_weight=class_weights, verbose=0, max_iter=1000)\n",
    "    \n",
    "    # Enregistrement du temps de début pour le fit\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    # Calculer le temps de fit\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    # Enregistrement du temps de début pour la prédiction\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    # Calculer le temps de prédiction\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "    \n",
    "    results.append({\n",
    "        \"C\": C_val,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"C\", C_val)\n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        \n",
    "        # Enregistrer les temps dans mlflow\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "362aa244-90cc-47e9-a9b9-5048645dc751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Log_Reg = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62b1f21c-58eb-44b7-8bc4-8fc521bb065f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_Log_Reg .to_csv('results_Log_Reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "380a6b7f-51ef-44d2-b7f1-37b46b53c2de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1fae8_row0_col4, #T_1fae8_row2_col2, #T_1fae8_row46_col3 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1fae8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1fae8_level0_col0\" class=\"col_heading level0 col0\" >C</th>\n",
       "      <th id=\"T_1fae8_level0_col1\" class=\"col_heading level0 col1\" >Threshold</th>\n",
       "      <th id=\"T_1fae8_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_1fae8_level0_col3\" class=\"col_heading level0 col3\" >Accuracy</th>\n",
       "      <th id=\"T_1fae8_level0_col4\" class=\"col_heading level0 col4\" >Business Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row0\" class=\"row_heading level0 row0\" >27</th>\n",
       "      <td id=\"T_1fae8_row0_col0\" class=\"data row0 col0\" >122.465193</td>\n",
       "      <td id=\"T_1fae8_row0_col1\" class=\"data row0 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row0_col2\" class=\"data row0 col2\" >0.747856</td>\n",
       "      <td id=\"T_1fae8_row0_col3\" class=\"data row0 col3\" >0.723782</td>\n",
       "      <td id=\"T_1fae8_row0_col4\" class=\"data row0 col4\" >166996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row1\" class=\"row_heading level0 row1\" >31</th>\n",
       "      <td id=\"T_1fae8_row1_col0\" class=\"data row1 col0\" >102.430697</td>\n",
       "      <td id=\"T_1fae8_row1_col1\" class=\"data row1 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row1_col2\" class=\"data row1 col2\" >0.747856</td>\n",
       "      <td id=\"T_1fae8_row1_col3\" class=\"data row1 col3\" >0.723837</td>\n",
       "      <td id=\"T_1fae8_row1_col4\" class=\"data row1 col4\" >167033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row2\" class=\"row_heading level0 row2\" >22</th>\n",
       "      <td id=\"T_1fae8_row2_col0\" class=\"data row2 col0\" >70.949651</td>\n",
       "      <td id=\"T_1fae8_row2_col1\" class=\"data row2 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row2_col2\" class=\"data row2 col2\" >0.747866</td>\n",
       "      <td id=\"T_1fae8_row2_col3\" class=\"data row2 col3\" >0.723752</td>\n",
       "      <td id=\"T_1fae8_row2_col4\" class=\"data row2 col4\" >167041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row3\" class=\"row_heading level0 row3\" >5</th>\n",
       "      <td id=\"T_1fae8_row3_col0\" class=\"data row3 col0\" >134.291697</td>\n",
       "      <td id=\"T_1fae8_row3_col1\" class=\"data row3 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row3_col2\" class=\"data row3 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row3_col3\" class=\"data row3 col3\" >0.723762</td>\n",
       "      <td id=\"T_1fae8_row3_col4\" class=\"data row3 col4\" >167056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row4\" class=\"row_heading level0 row4\" >42</th>\n",
       "      <td id=\"T_1fae8_row4_col0\" class=\"data row4 col0\" >87.214196</td>\n",
       "      <td id=\"T_1fae8_row4_col1\" class=\"data row4 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row4_col2\" class=\"data row4 col2\" >0.747853</td>\n",
       "      <td id=\"T_1fae8_row4_col3\" class=\"data row4 col3\" >0.723811</td>\n",
       "      <td id=\"T_1fae8_row4_col4\" class=\"data row4 col4\" >167068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row5\" class=\"row_heading level0 row5\" >36</th>\n",
       "      <td id=\"T_1fae8_row5_col0\" class=\"data row5 col0\" >93.172129</td>\n",
       "      <td id=\"T_1fae8_row5_col1\" class=\"data row5 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row5_col2\" class=\"data row5 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row5_col3\" class=\"data row5 col3\" >0.723808</td>\n",
       "      <td id=\"T_1fae8_row5_col4\" class=\"data row5 col4\" >167069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row6\" class=\"row_heading level0 row6\" >41</th>\n",
       "      <td id=\"T_1fae8_row6_col0\" class=\"data row6 col0\" >98.929739</td>\n",
       "      <td id=\"T_1fae8_row6_col1\" class=\"data row6 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row6_col2\" class=\"data row6 col2\" >0.747853</td>\n",
       "      <td id=\"T_1fae8_row6_col3\" class=\"data row6 col3\" >0.723759</td>\n",
       "      <td id=\"T_1fae8_row6_col4\" class=\"data row6 col4\" >167075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row7\" class=\"row_heading level0 row7\" >49</th>\n",
       "      <td id=\"T_1fae8_row7_col0\" class=\"data row7 col0\" >90.057235</td>\n",
       "      <td id=\"T_1fae8_row7_col1\" class=\"data row7 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row7_col2\" class=\"data row7 col2\" >0.747849</td>\n",
       "      <td id=\"T_1fae8_row7_col3\" class=\"data row7 col3\" >0.723762</td>\n",
       "      <td id=\"T_1fae8_row7_col4\" class=\"data row7 col4\" >167083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row8\" class=\"row_heading level0 row8\" >23</th>\n",
       "      <td id=\"T_1fae8_row8_col0\" class=\"data row8 col0\" >74.703880</td>\n",
       "      <td id=\"T_1fae8_row8_col1\" class=\"data row8 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row8_col2\" class=\"data row8 col2\" >0.747859</td>\n",
       "      <td id=\"T_1fae8_row8_col3\" class=\"data row8 col3\" >0.723788</td>\n",
       "      <td id=\"T_1fae8_row8_col4\" class=\"data row8 col4\" >167084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row9\" class=\"row_heading level0 row9\" >11</th>\n",
       "      <td id=\"T_1fae8_row9_col0\" class=\"data row9 col0\" >89.549079</td>\n",
       "      <td id=\"T_1fae8_row9_col1\" class=\"data row9 col1\" >0.53</td>\n",
       "      <td id=\"T_1fae8_row9_col2\" class=\"data row9 col2\" >0.747853</td>\n",
       "      <td id=\"T_1fae8_row9_col3\" class=\"data row9 col3\" >0.723775</td>\n",
       "      <td id=\"T_1fae8_row9_col4\" class=\"data row9 col4\" >167088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row10\" class=\"row_heading level0 row10\" >43</th>\n",
       "      <td id=\"T_1fae8_row10_col0\" class=\"data row10 col0\" >85.979856</td>\n",
       "      <td id=\"T_1fae8_row10_col1\" class=\"data row10 col1\" >0.51</td>\n",
       "      <td id=\"T_1fae8_row10_col2\" class=\"data row10 col2\" >0.747856</td>\n",
       "      <td id=\"T_1fae8_row10_col3\" class=\"data row10 col3\" >0.701147</td>\n",
       "      <td id=\"T_1fae8_row10_col4\" class=\"data row10 col4\" >167254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row11\" class=\"row_heading level0 row11\" >18</th>\n",
       "      <td id=\"T_1fae8_row11_col0\" class=\"data row11 col0\" >51.148449</td>\n",
       "      <td id=\"T_1fae8_row11_col1\" class=\"data row11 col1\" >0.51</td>\n",
       "      <td id=\"T_1fae8_row11_col2\" class=\"data row11 col2\" >0.747857</td>\n",
       "      <td id=\"T_1fae8_row11_col3\" class=\"data row11 col3\" >0.701046</td>\n",
       "      <td id=\"T_1fae8_row11_col4\" class=\"data row11 col4\" >167258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row12\" class=\"row_heading level0 row12\" >17</th>\n",
       "      <td id=\"T_1fae8_row12_col0\" class=\"data row12 col0\" >155.737928</td>\n",
       "      <td id=\"T_1fae8_row12_col1\" class=\"data row12 col1\" >0.55</td>\n",
       "      <td id=\"T_1fae8_row12_col2\" class=\"data row12 col2\" >0.747852</td>\n",
       "      <td id=\"T_1fae8_row12_col3\" class=\"data row12 col3\" >0.745856</td>\n",
       "      <td id=\"T_1fae8_row12_col4\" class=\"data row12 col4\" >167261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row13\" class=\"row_heading level0 row13\" >39</th>\n",
       "      <td id=\"T_1fae8_row13_col0\" class=\"data row13 col0\" >127.119228</td>\n",
       "      <td id=\"T_1fae8_row13_col1\" class=\"data row13 col1\" >0.55</td>\n",
       "      <td id=\"T_1fae8_row13_col2\" class=\"data row13 col2\" >0.747846</td>\n",
       "      <td id=\"T_1fae8_row13_col3\" class=\"data row13 col3\" >0.745899</td>\n",
       "      <td id=\"T_1fae8_row13_col4\" class=\"data row13 col4\" >167266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row14\" class=\"row_heading level0 row14\" >37</th>\n",
       "      <td id=\"T_1fae8_row14_col0\" class=\"data row14 col0\" >119.047844</td>\n",
       "      <td id=\"T_1fae8_row14_col1\" class=\"data row14 col1\" >0.51</td>\n",
       "      <td id=\"T_1fae8_row14_col2\" class=\"data row14 col2\" >0.747843</td>\n",
       "      <td id=\"T_1fae8_row14_col3\" class=\"data row14 col3\" >0.701131</td>\n",
       "      <td id=\"T_1fae8_row14_col4\" class=\"data row14 col4\" >167277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row15\" class=\"row_heading level0 row15\" >48</th>\n",
       "      <td id=\"T_1fae8_row15_col0\" class=\"data row15 col0\" >139.215797</td>\n",
       "      <td id=\"T_1fae8_row15_col1\" class=\"data row15 col1\" >0.55</td>\n",
       "      <td id=\"T_1fae8_row15_col2\" class=\"data row15 col2\" >0.747845</td>\n",
       "      <td id=\"T_1fae8_row15_col3\" class=\"data row15 col3\" >0.745889</td>\n",
       "      <td id=\"T_1fae8_row15_col4\" class=\"data row15 col4\" >167314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row16\" class=\"row_heading level0 row16\" >35</th>\n",
       "      <td id=\"T_1fae8_row16_col0\" class=\"data row16 col0\" >148.702013</td>\n",
       "      <td id=\"T_1fae8_row16_col1\" class=\"data row16 col1\" >0.55</td>\n",
       "      <td id=\"T_1fae8_row16_col2\" class=\"data row16 col2\" >0.747845</td>\n",
       "      <td id=\"T_1fae8_row16_col3\" class=\"data row16 col3\" >0.745876</td>\n",
       "      <td id=\"T_1fae8_row16_col4\" class=\"data row16 col4\" >167318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row17\" class=\"row_heading level0 row17\" >2</th>\n",
       "      <td id=\"T_1fae8_row17_col0\" class=\"data row17 col0\" >93.365760</td>\n",
       "      <td id=\"T_1fae8_row17_col1\" class=\"data row17 col1\" >0.54</td>\n",
       "      <td id=\"T_1fae8_row17_col2\" class=\"data row17 col2\" >0.747860</td>\n",
       "      <td id=\"T_1fae8_row17_col3\" class=\"data row17 col3\" >0.735047</td>\n",
       "      <td id=\"T_1fae8_row17_col4\" class=\"data row17 col4\" >167324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row18\" class=\"row_heading level0 row18\" >16</th>\n",
       "      <td id=\"T_1fae8_row18_col0\" class=\"data row18 col0\" >83.037338</td>\n",
       "      <td id=\"T_1fae8_row18_col1\" class=\"data row18 col1\" >0.5</td>\n",
       "      <td id=\"T_1fae8_row18_col2\" class=\"data row18 col2\" >0.747862</td>\n",
       "      <td id=\"T_1fae8_row18_col3\" class=\"data row18 col3\" >0.689211</td>\n",
       "      <td id=\"T_1fae8_row18_col4\" class=\"data row18 col4\" >167330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row19\" class=\"row_heading level0 row19\" >32</th>\n",
       "      <td id=\"T_1fae8_row19_col0\" class=\"data row19 col0\" >102.504110</td>\n",
       "      <td id=\"T_1fae8_row19_col1\" class=\"data row19 col1\" >0.51</td>\n",
       "      <td id=\"T_1fae8_row19_col2\" class=\"data row19 col2\" >0.747850</td>\n",
       "      <td id=\"T_1fae8_row19_col3\" class=\"data row19 col3\" >0.701010</td>\n",
       "      <td id=\"T_1fae8_row19_col4\" class=\"data row19 col4\" >167332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row20\" class=\"row_heading level0 row20\" >14</th>\n",
       "      <td id=\"T_1fae8_row20_col0\" class=\"data row20 col0\" >57.867233</td>\n",
       "      <td id=\"T_1fae8_row20_col1\" class=\"data row20 col1\" >0.54</td>\n",
       "      <td id=\"T_1fae8_row20_col2\" class=\"data row20 col2\" >0.747861</td>\n",
       "      <td id=\"T_1fae8_row20_col3\" class=\"data row20 col3\" >0.735096</td>\n",
       "      <td id=\"T_1fae8_row20_col4\" class=\"data row20 col4\" >167345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row21\" class=\"row_heading level0 row21\" >29</th>\n",
       "      <td id=\"T_1fae8_row21_col0\" class=\"data row21 col0\" >166.994487</td>\n",
       "      <td id=\"T_1fae8_row21_col1\" class=\"data row21 col1\" >0.54</td>\n",
       "      <td id=\"T_1fae8_row21_col2\" class=\"data row21 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row21_col3\" class=\"data row21 col3\" >0.735053</td>\n",
       "      <td id=\"T_1fae8_row21_col4\" class=\"data row21 col4\" >167358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row22\" class=\"row_heading level0 row22\" >44</th>\n",
       "      <td id=\"T_1fae8_row22_col0\" class=\"data row22 col0\" >79.201849</td>\n",
       "      <td id=\"T_1fae8_row22_col1\" class=\"data row22 col1\" >0.54</td>\n",
       "      <td id=\"T_1fae8_row22_col2\" class=\"data row22 col2\" >0.747853</td>\n",
       "      <td id=\"T_1fae8_row22_col3\" class=\"data row22 col3\" >0.735037</td>\n",
       "      <td id=\"T_1fae8_row22_col4\" class=\"data row22 col4\" >167372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row23\" class=\"row_heading level0 row23\" >47</th>\n",
       "      <td id=\"T_1fae8_row23_col0\" class=\"data row23 col0\" >95.747400</td>\n",
       "      <td id=\"T_1fae8_row23_col1\" class=\"data row23 col1\" >0.52</td>\n",
       "      <td id=\"T_1fae8_row23_col2\" class=\"data row23 col2\" >0.747851</td>\n",
       "      <td id=\"T_1fae8_row23_col3\" class=\"data row23 col3\" >0.712201</td>\n",
       "      <td id=\"T_1fae8_row23_col4\" class=\"data row23 col4\" >167377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row24\" class=\"row_heading level0 row24\" >34</th>\n",
       "      <td id=\"T_1fae8_row24_col0\" class=\"data row24 col0\" >112.246125</td>\n",
       "      <td id=\"T_1fae8_row24_col1\" class=\"data row24 col1\" >0.54</td>\n",
       "      <td id=\"T_1fae8_row24_col2\" class=\"data row24 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row24_col3\" class=\"data row24 col3\" >0.735053</td>\n",
       "      <td id=\"T_1fae8_row24_col4\" class=\"data row24 col4\" >167394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row25\" class=\"row_heading level0 row25\" >40</th>\n",
       "      <td id=\"T_1fae8_row25_col0\" class=\"data row25 col0\" >107.689738</td>\n",
       "      <td id=\"T_1fae8_row25_col1\" class=\"data row25 col1\" >0.52</td>\n",
       "      <td id=\"T_1fae8_row25_col2\" class=\"data row25 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row25_col3\" class=\"data row25 col3\" >0.712145</td>\n",
       "      <td id=\"T_1fae8_row25_col4\" class=\"data row25 col4\" >167403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row26\" class=\"row_heading level0 row26\" >13</th>\n",
       "      <td id=\"T_1fae8_row26_col0\" class=\"data row26 col0\" >101.440894</td>\n",
       "      <td id=\"T_1fae8_row26_col1\" class=\"data row26 col1\" >0.52</td>\n",
       "      <td id=\"T_1fae8_row26_col2\" class=\"data row26 col2\" >0.747860</td>\n",
       "      <td id=\"T_1fae8_row26_col3\" class=\"data row26 col3\" >0.712168</td>\n",
       "      <td id=\"T_1fae8_row26_col4\" class=\"data row26 col4\" >167405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row27\" class=\"row_heading level0 row27\" >24</th>\n",
       "      <td id=\"T_1fae8_row27_col0\" class=\"data row27 col0\" >70.330154</td>\n",
       "      <td id=\"T_1fae8_row27_col1\" class=\"data row27 col1\" >0.5</td>\n",
       "      <td id=\"T_1fae8_row27_col2\" class=\"data row27 col2\" >0.747856</td>\n",
       "      <td id=\"T_1fae8_row27_col3\" class=\"data row27 col3\" >0.689140</td>\n",
       "      <td id=\"T_1fae8_row27_col4\" class=\"data row27 col4\" >167406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row28\" class=\"row_heading level0 row28\" >21</th>\n",
       "      <td id=\"T_1fae8_row28_col0\" class=\"data row28 col0\" >52.983132</td>\n",
       "      <td id=\"T_1fae8_row28_col1\" class=\"data row28 col1\" >0.52</td>\n",
       "      <td id=\"T_1fae8_row28_col2\" class=\"data row28 col2\" >0.747854</td>\n",
       "      <td id=\"T_1fae8_row28_col3\" class=\"data row28 col3\" >0.712210</td>\n",
       "      <td id=\"T_1fae8_row28_col4\" class=\"data row28 col4\" >167410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row29\" class=\"row_heading level0 row29\" >6</th>\n",
       "      <td id=\"T_1fae8_row29_col0\" class=\"data row29 col0\" >113.240549</td>\n",
       "      <td id=\"T_1fae8_row29_col1\" class=\"data row29 col1\" >0.52</td>\n",
       "      <td id=\"T_1fae8_row29_col2\" class=\"data row29 col2\" >0.747846</td>\n",
       "      <td id=\"T_1fae8_row29_col3\" class=\"data row29 col3\" >0.712145</td>\n",
       "      <td id=\"T_1fae8_row29_col4\" class=\"data row29 col4\" >167412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row30\" class=\"row_heading level0 row30\" >25</th>\n",
       "      <td id=\"T_1fae8_row30_col0\" class=\"data row30 col0\" >199.483567</td>\n",
       "      <td id=\"T_1fae8_row30_col1\" class=\"data row30 col1\" >0.56</td>\n",
       "      <td id=\"T_1fae8_row30_col2\" class=\"data row30 col2\" >0.747846</td>\n",
       "      <td id=\"T_1fae8_row30_col3\" class=\"data row30 col3\" >0.755996</td>\n",
       "      <td id=\"T_1fae8_row30_col4\" class=\"data row30 col4\" >167476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row31\" class=\"row_heading level0 row31\" >20</th>\n",
       "      <td id=\"T_1fae8_row31_col0\" class=\"data row31 col0\" >141.636946</td>\n",
       "      <td id=\"T_1fae8_row31_col1\" class=\"data row31 col1\" >0.56</td>\n",
       "      <td id=\"T_1fae8_row31_col2\" class=\"data row31 col2\" >0.747846</td>\n",
       "      <td id=\"T_1fae8_row31_col3\" class=\"data row31 col3\" >0.756018</td>\n",
       "      <td id=\"T_1fae8_row31_col4\" class=\"data row31 col4\" >167505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row32\" class=\"row_heading level0 row32\" >8</th>\n",
       "      <td id=\"T_1fae8_row32_col0\" class=\"data row32 col0\" >66.387614</td>\n",
       "      <td id=\"T_1fae8_row32_col1\" class=\"data row32 col1\" >0.56</td>\n",
       "      <td id=\"T_1fae8_row32_col2\" class=\"data row32 col2\" >0.747852</td>\n",
       "      <td id=\"T_1fae8_row32_col3\" class=\"data row32 col3\" >0.755960</td>\n",
       "      <td id=\"T_1fae8_row32_col4\" class=\"data row32 col4\" >167568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row33\" class=\"row_heading level0 row33\" >7</th>\n",
       "      <td id=\"T_1fae8_row33_col0\" class=\"data row33 col0\" >183.140296</td>\n",
       "      <td id=\"T_1fae8_row33_col1\" class=\"data row33 col1\" >0.49</td>\n",
       "      <td id=\"T_1fae8_row33_col2\" class=\"data row33 col2\" >0.747854</td>\n",
       "      <td id=\"T_1fae8_row33_col3\" class=\"data row33 col3\" >0.677018</td>\n",
       "      <td id=\"T_1fae8_row33_col4\" class=\"data row33 col4\" >168043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row34\" class=\"row_heading level0 row34\" >30</th>\n",
       "      <td id=\"T_1fae8_row34_col0\" class=\"data row34 col0\" >121.909289</td>\n",
       "      <td id=\"T_1fae8_row34_col1\" class=\"data row34 col1\" >0.49</td>\n",
       "      <td id=\"T_1fae8_row34_col2\" class=\"data row34 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row34_col3\" class=\"data row34 col3\" >0.677041</td>\n",
       "      <td id=\"T_1fae8_row34_col4\" class=\"data row34 col4\" >168054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row35\" class=\"row_heading level0 row35\" >45</th>\n",
       "      <td id=\"T_1fae8_row35_col0\" class=\"data row35 col0\" >62.830747</td>\n",
       "      <td id=\"T_1fae8_row35_col1\" class=\"data row35 col1\" >0.49</td>\n",
       "      <td id=\"T_1fae8_row35_col2\" class=\"data row35 col2\" >0.747857</td>\n",
       "      <td id=\"T_1fae8_row35_col3\" class=\"data row35 col3\" >0.677002</td>\n",
       "      <td id=\"T_1fae8_row35_col4\" class=\"data row35 col4\" >168057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row36\" class=\"row_heading level0 row36\" >28</th>\n",
       "      <td id=\"T_1fae8_row36_col0\" class=\"data row36 col0\" >123.972072</td>\n",
       "      <td id=\"T_1fae8_row36_col1\" class=\"data row36 col1\" >0.57</td>\n",
       "      <td id=\"T_1fae8_row36_col2\" class=\"data row36 col2\" >0.747845</td>\n",
       "      <td id=\"T_1fae8_row36_col3\" class=\"data row36 col3\" >0.765930</td>\n",
       "      <td id=\"T_1fae8_row36_col4\" class=\"data row36 col4\" >168294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row37\" class=\"row_heading level0 row37\" >38</th>\n",
       "      <td id=\"T_1fae8_row37_col0\" class=\"data row37 col0\" >166.312393</td>\n",
       "      <td id=\"T_1fae8_row37_col1\" class=\"data row37 col1\" >0.57</td>\n",
       "      <td id=\"T_1fae8_row37_col2\" class=\"data row37 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row37_col3\" class=\"data row37 col3\" >0.765949</td>\n",
       "      <td id=\"T_1fae8_row37_col4\" class=\"data row37 col4\" >168297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row38\" class=\"row_heading level0 row38\" >19</th>\n",
       "      <td id=\"T_1fae8_row38_col0\" class=\"data row38 col0\" >106.044173</td>\n",
       "      <td id=\"T_1fae8_row38_col1\" class=\"data row38 col1\" >0.48</td>\n",
       "      <td id=\"T_1fae8_row38_col2\" class=\"data row38 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row38_col3\" class=\"data row38 col3\" >0.664712</td>\n",
       "      <td id=\"T_1fae8_row38_col4\" class=\"data row38 col4\" >169070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row39\" class=\"row_heading level0 row39\" >33</th>\n",
       "      <td id=\"T_1fae8_row39_col0\" class=\"data row39 col0\" >136.977968</td>\n",
       "      <td id=\"T_1fae8_row39_col1\" class=\"data row39 col1\" >0.58</td>\n",
       "      <td id=\"T_1fae8_row39_col2\" class=\"data row39 col2\" >0.747846</td>\n",
       "      <td id=\"T_1fae8_row39_col3\" class=\"data row39 col3\" >0.775577</td>\n",
       "      <td id=\"T_1fae8_row39_col4\" class=\"data row39 col4\" >169101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row40\" class=\"row_heading level0 row40\" >0</th>\n",
       "      <td id=\"T_1fae8_row40_col0\" class=\"data row40 col0\" >176.144678</td>\n",
       "      <td id=\"T_1fae8_row40_col1\" class=\"data row40 col1\" >0.58</td>\n",
       "      <td id=\"T_1fae8_row40_col2\" class=\"data row40 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row40_col3\" class=\"data row40 col3\" >0.775577</td>\n",
       "      <td id=\"T_1fae8_row40_col4\" class=\"data row40 col4\" >169128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row41\" class=\"row_heading level0 row41\" >1</th>\n",
       "      <td id=\"T_1fae8_row41_col0\" class=\"data row41 col0\" >156.615783</td>\n",
       "      <td id=\"T_1fae8_row41_col1\" class=\"data row41 col1\" >0.58</td>\n",
       "      <td id=\"T_1fae8_row41_col2\" class=\"data row41 col2\" >0.747850</td>\n",
       "      <td id=\"T_1fae8_row41_col3\" class=\"data row41 col3\" >0.775597</td>\n",
       "      <td id=\"T_1fae8_row41_col4\" class=\"data row41 col4\" >169140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row42\" class=\"row_heading level0 row42\" >10</th>\n",
       "      <td id=\"T_1fae8_row42_col0\" class=\"data row42 col0\" >147.069601</td>\n",
       "      <td id=\"T_1fae8_row42_col1\" class=\"data row42 col1\" >0.47</td>\n",
       "      <td id=\"T_1fae8_row42_col2\" class=\"data row42 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row42_col3\" class=\"data row42 col3\" >0.651969</td>\n",
       "      <td id=\"T_1fae8_row42_col4\" class=\"data row42 col4\" >169826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row43\" class=\"row_heading level0 row43\" >9</th>\n",
       "      <td id=\"T_1fae8_row43_col0\" class=\"data row43 col0\" >81.191053</td>\n",
       "      <td id=\"T_1fae8_row43_col1\" class=\"data row43 col1\" >0.59</td>\n",
       "      <td id=\"T_1fae8_row43_col2\" class=\"data row43 col2\" >0.747850</td>\n",
       "      <td id=\"T_1fae8_row43_col3\" class=\"data row43 col3\" >0.785016</td>\n",
       "      <td id=\"T_1fae8_row43_col4\" class=\"data row43 col4\" >170053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row44\" class=\"row_heading level0 row44\" >12</th>\n",
       "      <td id=\"T_1fae8_row44_col0\" class=\"data row44 col0\" >133.378647</td>\n",
       "      <td id=\"T_1fae8_row44_col1\" class=\"data row44 col1\" >0.46</td>\n",
       "      <td id=\"T_1fae8_row44_col2\" class=\"data row44 col2\" >0.747850</td>\n",
       "      <td id=\"T_1fae8_row44_col3\" class=\"data row44 col3\" >0.638832</td>\n",
       "      <td id=\"T_1fae8_row44_col4\" class=\"data row44 col4\" >170883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row45\" class=\"row_heading level0 row45\" >3</th>\n",
       "      <td id=\"T_1fae8_row45_col0\" class=\"data row45 col0\" >115.811429</td>\n",
       "      <td id=\"T_1fae8_row45_col1\" class=\"data row45 col1\" >0.6</td>\n",
       "      <td id=\"T_1fae8_row45_col2\" class=\"data row45 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row45_col3\" class=\"data row45 col3\" >0.793971</td>\n",
       "      <td id=\"T_1fae8_row45_col4\" class=\"data row45 col4\" >171019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_1fae8_row46_col0\" class=\"data row46 col0\" >114.575147</td>\n",
       "      <td id=\"T_1fae8_row46_col1\" class=\"data row46 col1\" >0.6</td>\n",
       "      <td id=\"T_1fae8_row46_col2\" class=\"data row46 col2\" >0.747847</td>\n",
       "      <td id=\"T_1fae8_row46_col3\" class=\"data row46 col3\" >0.793974</td>\n",
       "      <td id=\"T_1fae8_row46_col4\" class=\"data row46 col4\" >171045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row47\" class=\"row_heading level0 row47\" >26</th>\n",
       "      <td id=\"T_1fae8_row47_col0\" class=\"data row47 col0\" >72.624602</td>\n",
       "      <td id=\"T_1fae8_row47_col1\" class=\"data row47 col1\" >0.45</td>\n",
       "      <td id=\"T_1fae8_row47_col2\" class=\"data row47 col2\" >0.747858</td>\n",
       "      <td id=\"T_1fae8_row47_col3\" class=\"data row47 col3\" >0.625675</td>\n",
       "      <td id=\"T_1fae8_row47_col4\" class=\"data row47 col4\" >171937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row48\" class=\"row_heading level0 row48\" >15</th>\n",
       "      <td id=\"T_1fae8_row48_col0\" class=\"data row48 col0\" >130.608957</td>\n",
       "      <td id=\"T_1fae8_row48_col1\" class=\"data row48 col1\" >0.43</td>\n",
       "      <td id=\"T_1fae8_row48_col2\" class=\"data row48 col2\" >0.747848</td>\n",
       "      <td id=\"T_1fae8_row48_col3\" class=\"data row48 col3\" >0.597996</td>\n",
       "      <td id=\"T_1fae8_row48_col4\" class=\"data row48 col4\" >174663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fae8_level0_row49\" class=\"row_heading level0 row49\" >4</th>\n",
       "      <td id=\"T_1fae8_row49_col0\" class=\"data row49 col0\" >185.406278</td>\n",
       "      <td id=\"T_1fae8_row49_col1\" class=\"data row49 col1\" >0.4</td>\n",
       "      <td id=\"T_1fae8_row49_col2\" class=\"data row49 col2\" >0.747850</td>\n",
       "      <td id=\"T_1fae8_row49_col3\" class=\"data row49 col3\" >0.553768</td>\n",
       "      <td id=\"T_1fae8_row49_col4\" class=\"data row49 col4\" >179863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17b1babd400>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_Log_Reg  = (styled_df_Log_Reg.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score'])\n",
    "                          .format({'Threshold': \"{:g}\", 'Business Score': \"{:.0f}\"}))\n",
    "\n",
    "styled_df_Log_Reg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ce75e-9768-4c04-a842-92e2832e4429",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddfb0fef-b777-412a-b84a-83bed3253e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_val': 122.46519294640589, 'threshold': 0.53}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96cb3331-1eee-491a-bfb5-402cb2700431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Feature  Importance\n",
      "29                     EXT_SOURCE_3    0.480069\n",
      "4                   AMT_GOODS_PRICE    0.460824\n",
      "28                     EXT_SOURCE_2    0.387003\n",
      "2                        AMT_CREDIT    0.363860\n",
      "31                 BASEMENTAREA_AVG    0.244715\n",
      "..                              ...         ...\n",
      "181  WEEKDAY_APPR_PROCESS_START_nan    0.000000\n",
      "147          NAME_FAMILY_STATUS_nan    0.000000\n",
      "115                FLAG_OWN_CAR_nan    0.000000\n",
      "154           NAME_HOUSING_TYPE_nan    0.000000\n",
      "112                 CODE_GENDER_nan    0.000000\n",
      "\n",
      "[261 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Supposition que df_classification_imputed est déjà défini\n",
    "nb_0 = len(application_train_petit_clean[application_train_petit_clean[\"TARGET\"] == 0])\n",
    "nb_1 = len(application_train_petit_clean[application_train_petit_clean[\"TARGET\"] == 1])\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "# Enregistrer les noms des colonnes avant l'imputation\n",
    "column_names = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"]).columns\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='mean')  # Vous pouvez utiliser 'median' ou 'constant' selon votre besoin\n",
    "X = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X = imputer.fit_transform(X)\n",
    "y = application_train_petit_clean[\"TARGET\"]\n",
    "\n",
    "# Convertir le tableau NumPy en DataFrame pandas\n",
    "X = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Stockage des noms des colonnes pour utilisation ultérieure\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Remplacer 'C_val' par 'C'\n",
    "params = study.best_params.copy()\n",
    "params['C'] = params.pop('C_val')\n",
    "\n",
    "# Suppression de 'threshold'\n",
    "params.pop('threshold', None)\n",
    "\n",
    "# Instanciation du modèle\n",
    "clf = LogisticRegression(**params, class_weight=class_weights, max_iter=1000, verbose=1)\n",
    "\n",
    "# Validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    clf.fit(X[train], y.iloc[train])\n",
    "    # Pour la régression logistique, l'importance des caractéristiques est donnée par les coefficients\n",
    "    feature_importances += np.abs(clf.coef_[0])\n",
    "\n",
    "# Moyenne des importances de caractéristiques sur les plis\n",
    "feature_importances /= 5\n",
    "\n",
    "# Afficher les importances des caractéristiques\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3aa92591-ffd9-43dd-a81d-076c3fb0e99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_log_reg = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "df_feature_importances_log_reg = df_feature_importances_log_reg.sort_values(by='importance', ascending=False)\n",
    "df_feature_importances_log_reg[\"importance\"] = (df_feature_importances_log_reg[\"importance\"]/ df_feature_importances_log_reg[\"importance\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e11ec462-827a-40b0-9ab7-5ddb4070ab90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_log_reg.to_csv('df_feature_importances_log_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e764173b-317b-481d-b4b6-a2e9a5690ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EXT_SOURCE_3</td>\n",
       "      <td>4.638454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMT_GOODS_PRICE</td>\n",
       "      <td>4.452509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>EXT_SOURCE_2</td>\n",
       "      <td>3.739245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMT_CREDIT</td>\n",
       "      <td>3.515638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BASEMENTAREA_AVG</td>\n",
       "      <td>2.364449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>WEEKDAY_APPR_PROCESS_START_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>NAME_FAMILY_STATUS_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>FLAG_OWN_CAR_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>NAME_HOUSING_TYPE_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>CODE_GENDER_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            feature  importance\n",
       "29                     EXT_SOURCE_3    4.638454\n",
       "4                   AMT_GOODS_PRICE    4.452509\n",
       "28                     EXT_SOURCE_2    3.739245\n",
       "2                        AMT_CREDIT    3.515638\n",
       "31                 BASEMENTAREA_AVG    2.364449\n",
       "..                              ...         ...\n",
       "181  WEEKDAY_APPR_PROCESS_START_nan    0.000000\n",
       "147          NAME_FAMILY_STATUS_nan    0.000000\n",
       "115                FLAG_OWN_CAR_nan    0.000000\n",
       "154           NAME_HOUSING_TYPE_nan    0.000000\n",
       "112                 CODE_GENDER_nan    0.000000\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importances_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0eec1850-d060-45ed-893c-c7e94e3a0616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pourcentage_importance_totale = 95\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "df_sorted = df_feature_importances_log_reg.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "df_sorted[\"cumulative_importance\"] = df_sorted[\"importance\"].cumsum()\n",
    "\n",
    "# Find the number of features needed to reach 99% of the total importance\n",
    "num_features = df_sorted[df_sorted[\"cumulative_importance\"] <= df_sorted[\"importance\"].sum() * Pourcentage_importance_totale/100].shape[0]\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c9a3181-1588-4ae3-b540-6e5ed66e0a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les colonnes 'feature' et 'importance' de votre dataframe\n",
    "features = df_feature_importances_log_reg['feature'].to_numpy()\n",
    "importances = df_feature_importances_log_reg['importance'].to_numpy()\n",
    "\n",
    "# Création d'un masque pour les importances supérieures à 0\n",
    "mask = importances > 0\n",
    "\n",
    "# Filtrage des features et des importances en utilisant le masque\n",
    "filtered_feature_names = features[mask]\n",
    "filtered_feature_importances = importances[mask]\n",
    "\n",
    "# Supposons que vous vouliez afficher les 10 principales caractéristiques\n",
    "n_features_to_display = 20\n",
    "\n",
    "# Tri des indices filtrés par importance\n",
    "sorted_idx = filtered_feature_importances.argsort()\n",
    "\n",
    "# Limiter aux n principales caractéristiques\n",
    "top_n_idx = sorted_idx[-n_features_to_display:]\n",
    "\n",
    "# Affichage de l'importance des caractéristiques filtrées pour les n principales\n",
    "plt.figure(figsize=(10, len(top_n_idx) * 0.4))\n",
    "bars = plt.barh(filtered_feature_names[top_n_idx], filtered_feature_importances[top_n_idx])\n",
    "\n",
    "# Ajuster les limites de l'axe des y pour réduire l'espace autour des barres\n",
    "plt.ylim(-0.5, len(top_n_idx)-0.5)\n",
    "\n",
    "plt.xlabel(\"Importance des caractéristiques\")\n",
    "plt.title(\"Importance des caractéristiques avec la Régression Logistique\")\n",
    "\n",
    "# Ajouter les valeurs à côté des barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    \n",
    "    # Déterminez le formatage en fonction de la magnitude\n",
    "    if width < 0.01:\n",
    "        fmt = '{:.4f}'\n",
    "    elif width < 0.1:\n",
    "        fmt = '{:.3f}'\n",
    "    else:\n",
    "        fmt = '{:.2f}'\n",
    "    \n",
    "    plt.text(width + 0.01 * width,  # Ajouter un padding proportionnel à la valeur\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             fmt.format(width),\n",
    "             va='center', ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b511a-b2e5-4d71-b888-4db88fe05a90",
   "metadata": {},
   "source": [
    "## 2.3 LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15822aa4-a746-4e28-82e4-295c422bb20d",
   "metadata": {},
   "source": [
    "### 2.3.1 Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "546a9017-3321-46c6-a0ff-cc245df52c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 14116.06 secondes\n",
      "{'lr': 0.021541725724886718, 'num_leaves': 70, 'n_estimators': 140, 'threshold': 0.48000000000000004}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('LightGBM')\n",
    "\n",
    "# Supposition : vous avez déjà votre dataframe 'df_classification' chargé\n",
    "nb_0 = (application_train['TARGET'] == 0).sum()\n",
    "nb_1 = (application_train['TARGET'] == 1).sum()\n",
    "\n",
    "X = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train[\"TARGET\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "results = []\n",
    "\n",
    "total_combinations = 50  # définissez cela avant d'appeler study.optimize()\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LGBMClassifier(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators, class_weight=class_weights, verbose=-1)\n",
    "    \n",
    "    # Enregistrement du temps de début pour le fit\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    # Calculer le temps de fit\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    # Enregistrement du temps de début pour la prédiction\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    # Calculer le temps de prédiction\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "    \n",
    "    results.append({\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost,\n",
    "        \"Fit Time\": elapsed_time_fit,  \n",
    "        \"Prediction Time\": elapsed_time_predict\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Learning Rate\", lr)\n",
    "        mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "        mlflow.log_param(\"N Estimators\", n_estimators) \n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        \n",
    "        # Enregistrer les temps dans mlflow\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "# 6. Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3119bdda-1ef6-4f44-bfcd-5d29b71a8656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4c2592c-e6d6-4c7d-892f-5eac1f334328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM.to_csv('results_LGM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae3836c3-e223-4937-9a7d-2d48e3f57307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9f3e3_row0_col5, #T_9f3e3_row0_col6, #T_9f3e3_row0_col7, #T_9f3e3_row1_col6, #T_9f3e3_row1_col7, #T_9f3e3_row2_col6, #T_9f3e3_row2_col7, #T_9f3e3_row3_col6, #T_9f3e3_row3_col7, #T_9f3e3_row4_col6, #T_9f3e3_row4_col7, #T_9f3e3_row5_col6, #T_9f3e3_row5_col7, #T_9f3e3_row6_col6, #T_9f3e3_row6_col7, #T_9f3e3_row7_col6, #T_9f3e3_row7_col7, #T_9f3e3_row8_col6, #T_9f3e3_row8_col7, #T_9f3e3_row9_col6, #T_9f3e3_row9_col7, #T_9f3e3_row10_col6, #T_9f3e3_row10_col7, #T_9f3e3_row11_col6, #T_9f3e3_row11_col7, #T_9f3e3_row12_col6, #T_9f3e3_row12_col7, #T_9f3e3_row13_col6, #T_9f3e3_row13_col7, #T_9f3e3_row14_col6, #T_9f3e3_row14_col7, #T_9f3e3_row15_col6, #T_9f3e3_row15_col7, #T_9f3e3_row16_col6, #T_9f3e3_row16_col7, #T_9f3e3_row17_col6, #T_9f3e3_row17_col7, #T_9f3e3_row18_col6, #T_9f3e3_row18_col7, #T_9f3e3_row19_col6, #T_9f3e3_row19_col7, #T_9f3e3_row20_col6, #T_9f3e3_row20_col7, #T_9f3e3_row21_col6, #T_9f3e3_row21_col7, #T_9f3e3_row22_col6, #T_9f3e3_row22_col7, #T_9f3e3_row23_col6, #T_9f3e3_row23_col7, #T_9f3e3_row24_col6, #T_9f3e3_row24_col7, #T_9f3e3_row25_col6, #T_9f3e3_row25_col7, #T_9f3e3_row26_col6, #T_9f3e3_row26_col7, #T_9f3e3_row27_col6, #T_9f3e3_row27_col7, #T_9f3e3_row28_col6, #T_9f3e3_row28_col7, #T_9f3e3_row29_col6, #T_9f3e3_row29_col7, #T_9f3e3_row30_col6, #T_9f3e3_row30_col7, #T_9f3e3_row31_col6, #T_9f3e3_row31_col7, #T_9f3e3_row32_col6, #T_9f3e3_row32_col7, #T_9f3e3_row33_col6, #T_9f3e3_row33_col7, #T_9f3e3_row34_col6, #T_9f3e3_row34_col7, #T_9f3e3_row35_col6, #T_9f3e3_row35_col7, #T_9f3e3_row36_col6, #T_9f3e3_row36_col7, #T_9f3e3_row37_col6, #T_9f3e3_row37_col7, #T_9f3e3_row38_col6, #T_9f3e3_row38_col7, #T_9f3e3_row39_col6, #T_9f3e3_row39_col7, #T_9f3e3_row40_col3, #T_9f3e3_row40_col6, #T_9f3e3_row40_col7, #T_9f3e3_row41_col6, #T_9f3e3_row41_col7, #T_9f3e3_row42_col6, #T_9f3e3_row42_col7, #T_9f3e3_row43_col6, #T_9f3e3_row43_col7, #T_9f3e3_row44_col6, #T_9f3e3_row44_col7, #T_9f3e3_row45_col6, #T_9f3e3_row45_col7, #T_9f3e3_row46_col6, #T_9f3e3_row46_col7, #T_9f3e3_row47_col6, #T_9f3e3_row47_col7, #T_9f3e3_row48_col6, #T_9f3e3_row48_col7, #T_9f3e3_row49_col4, #T_9f3e3_row49_col6, #T_9f3e3_row49_col7 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9f3e3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9f3e3_level0_col0\" class=\"col_heading level0 col0\" >Learning Rate</th>\n",
       "      <th id=\"T_9f3e3_level0_col1\" class=\"col_heading level0 col1\" >Num Leaves</th>\n",
       "      <th id=\"T_9f3e3_level0_col2\" class=\"col_heading level0 col2\" >Threshold</th>\n",
       "      <th id=\"T_9f3e3_level0_col3\" class=\"col_heading level0 col3\" >AUC</th>\n",
       "      <th id=\"T_9f3e3_level0_col4\" class=\"col_heading level0 col4\" >Accuracy</th>\n",
       "      <th id=\"T_9f3e3_level0_col5\" class=\"col_heading level0 col5\" >Business Score</th>\n",
       "      <th id=\"T_9f3e3_level0_col6\" class=\"col_heading level0 col6\" >Fit Time</th>\n",
       "      <th id=\"T_9f3e3_level0_col7\" class=\"col_heading level0 col7\" >Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row0\" class=\"row_heading level0 row0\" >12</th>\n",
       "      <td id=\"T_9f3e3_row0_col0\" class=\"data row0 col0\" >0.021542</td>\n",
       "      <td id=\"T_9f3e3_row0_col1\" class=\"data row0 col1\" >70</td>\n",
       "      <td id=\"T_9f3e3_row0_col2\" class=\"data row0 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row0_col3\" class=\"data row0 col3\" >0.729493</td>\n",
       "      <td id=\"T_9f3e3_row0_col4\" class=\"data row0 col4\" >0.704718</td>\n",
       "      <td id=\"T_9f3e3_row0_col5\" class=\"data row0 col5\" >174167</td>\n",
       "      <td id=\"T_9f3e3_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row0_col7\" class=\"data row0 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row1\" class=\"row_heading level0 row1\" >10</th>\n",
       "      <td id=\"T_9f3e3_row1_col0\" class=\"data row1 col0\" >0.017984</td>\n",
       "      <td id=\"T_9f3e3_row1_col1\" class=\"data row1 col1\" >60</td>\n",
       "      <td id=\"T_9f3e3_row1_col2\" class=\"data row1 col2\" >0.46</td>\n",
       "      <td id=\"T_9f3e3_row1_col3\" class=\"data row1 col3\" >0.725554</td>\n",
       "      <td id=\"T_9f3e3_row1_col4\" class=\"data row1 col4\" >0.675495</td>\n",
       "      <td id=\"T_9f3e3_row1_col5\" class=\"data row1 col5\" >176719</td>\n",
       "      <td id=\"T_9f3e3_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row1_col7\" class=\"data row1 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row2\" class=\"row_heading level0 row2\" >48</th>\n",
       "      <td id=\"T_9f3e3_row2_col0\" class=\"data row2 col0\" >0.026191</td>\n",
       "      <td id=\"T_9f3e3_row2_col1\" class=\"data row2 col1\" >47</td>\n",
       "      <td id=\"T_9f3e3_row2_col2\" class=\"data row2 col2\" >0.5</td>\n",
       "      <td id=\"T_9f3e3_row2_col3\" class=\"data row2 col3\" >0.723242</td>\n",
       "      <td id=\"T_9f3e3_row2_col4\" class=\"data row2 col4\" >0.736043</td>\n",
       "      <td id=\"T_9f3e3_row2_col5\" class=\"data row2 col5\" >177134</td>\n",
       "      <td id=\"T_9f3e3_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row2_col7\" class=\"data row2 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row3\" class=\"row_heading level0 row3\" >42</th>\n",
       "      <td id=\"T_9f3e3_row3_col0\" class=\"data row3 col0\" >0.018441</td>\n",
       "      <td id=\"T_9f3e3_row3_col1\" class=\"data row3 col1\" >60</td>\n",
       "      <td id=\"T_9f3e3_row3_col2\" class=\"data row3 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row3_col3\" class=\"data row3 col3\" >0.683867</td>\n",
       "      <td id=\"T_9f3e3_row3_col4\" class=\"data row3 col4\" >0.754485</td>\n",
       "      <td id=\"T_9f3e3_row3_col5\" class=\"data row3 col5\" >178857</td>\n",
       "      <td id=\"T_9f3e3_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row3_col7\" class=\"data row3 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row4\" class=\"row_heading level0 row4\" >35</th>\n",
       "      <td id=\"T_9f3e3_row4_col0\" class=\"data row4 col0\" >0.019112</td>\n",
       "      <td id=\"T_9f3e3_row4_col1\" class=\"data row4 col1\" >61</td>\n",
       "      <td id=\"T_9f3e3_row4_col2\" class=\"data row4 col2\" >0.45</td>\n",
       "      <td id=\"T_9f3e3_row4_col3\" class=\"data row4 col3\" >0.682015</td>\n",
       "      <td id=\"T_9f3e3_row4_col4\" class=\"data row4 col4\" >0.740323</td>\n",
       "      <td id=\"T_9f3e3_row4_col5\" class=\"data row4 col5\" >178978</td>\n",
       "      <td id=\"T_9f3e3_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row4_col7\" class=\"data row4 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row5\" class=\"row_heading level0 row5\" >43</th>\n",
       "      <td id=\"T_9f3e3_row5_col0\" class=\"data row5 col0\" >0.010024</td>\n",
       "      <td id=\"T_9f3e3_row5_col1\" class=\"data row5 col1\" >58</td>\n",
       "      <td id=\"T_9f3e3_row5_col2\" class=\"data row5 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row5_col3\" class=\"data row5 col3\" >0.686017</td>\n",
       "      <td id=\"T_9f3e3_row5_col4\" class=\"data row5 col4\" >0.745222</td>\n",
       "      <td id=\"T_9f3e3_row5_col5\" class=\"data row5 col5\" >179273</td>\n",
       "      <td id=\"T_9f3e3_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row5_col7\" class=\"data row5 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row6\" class=\"row_heading level0 row6\" >20</th>\n",
       "      <td id=\"T_9f3e3_row6_col0\" class=\"data row6 col0\" >0.018479</td>\n",
       "      <td id=\"T_9f3e3_row6_col1\" class=\"data row6 col1\" >64</td>\n",
       "      <td id=\"T_9f3e3_row6_col2\" class=\"data row6 col2\" >0.44</td>\n",
       "      <td id=\"T_9f3e3_row6_col3\" class=\"data row6 col3\" >0.681029</td>\n",
       "      <td id=\"T_9f3e3_row6_col4\" class=\"data row6 col4\" >0.743428</td>\n",
       "      <td id=\"T_9f3e3_row6_col5\" class=\"data row6 col5\" >179275</td>\n",
       "      <td id=\"T_9f3e3_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row6_col7\" class=\"data row6 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row7\" class=\"row_heading level0 row7\" >24</th>\n",
       "      <td id=\"T_9f3e3_row7_col0\" class=\"data row7 col0\" >0.031193</td>\n",
       "      <td id=\"T_9f3e3_row7_col1\" class=\"data row7 col1\" >61</td>\n",
       "      <td id=\"T_9f3e3_row7_col2\" class=\"data row7 col2\" >0.49</td>\n",
       "      <td id=\"T_9f3e3_row7_col3\" class=\"data row7 col3\" >0.681320</td>\n",
       "      <td id=\"T_9f3e3_row7_col4\" class=\"data row7 col4\" >0.783132</td>\n",
       "      <td id=\"T_9f3e3_row7_col5\" class=\"data row7 col5\" >179353</td>\n",
       "      <td id=\"T_9f3e3_row7_col6\" class=\"data row7 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row7_col7\" class=\"data row7 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row8\" class=\"row_heading level0 row8\" >38</th>\n",
       "      <td id=\"T_9f3e3_row8_col0\" class=\"data row8 col0\" >0.014025</td>\n",
       "      <td id=\"T_9f3e3_row8_col1\" class=\"data row8 col1\" >68</td>\n",
       "      <td id=\"T_9f3e3_row8_col2\" class=\"data row8 col2\" >0.46</td>\n",
       "      <td id=\"T_9f3e3_row8_col3\" class=\"data row8 col3\" >0.733418</td>\n",
       "      <td id=\"T_9f3e3_row8_col4\" class=\"data row8 col4\" >0.593077</td>\n",
       "      <td id=\"T_9f3e3_row8_col5\" class=\"data row8 col5\" >179423</td>\n",
       "      <td id=\"T_9f3e3_row8_col6\" class=\"data row8 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row8_col7\" class=\"data row8 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row9\" class=\"row_heading level0 row9\" >21</th>\n",
       "      <td id=\"T_9f3e3_row9_col0\" class=\"data row9 col0\" >0.019475</td>\n",
       "      <td id=\"T_9f3e3_row9_col1\" class=\"data row9 col1\" >54</td>\n",
       "      <td id=\"T_9f3e3_row9_col2\" class=\"data row9 col2\" >0.44</td>\n",
       "      <td id=\"T_9f3e3_row9_col3\" class=\"data row9 col3\" >0.680818</td>\n",
       "      <td id=\"T_9f3e3_row9_col4\" class=\"data row9 col4\" >0.737918</td>\n",
       "      <td id=\"T_9f3e3_row9_col5\" class=\"data row9 col5\" >179528</td>\n",
       "      <td id=\"T_9f3e3_row9_col6\" class=\"data row9 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row9_col7\" class=\"data row9 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row10\" class=\"row_heading level0 row10\" >46</th>\n",
       "      <td id=\"T_9f3e3_row10_col0\" class=\"data row10 col0\" >0.014202</td>\n",
       "      <td id=\"T_9f3e3_row10_col1\" class=\"data row10 col1\" >58</td>\n",
       "      <td id=\"T_9f3e3_row10_col2\" class=\"data row10 col2\" >0.52</td>\n",
       "      <td id=\"T_9f3e3_row10_col3\" class=\"data row10 col3\" >0.686536</td>\n",
       "      <td id=\"T_9f3e3_row10_col4\" class=\"data row10 col4\" >0.782673</td>\n",
       "      <td id=\"T_9f3e3_row10_col5\" class=\"data row10 col5\" >179602</td>\n",
       "      <td id=\"T_9f3e3_row10_col6\" class=\"data row10 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row10_col7\" class=\"data row10 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row11\" class=\"row_heading level0 row11\" >49</th>\n",
       "      <td id=\"T_9f3e3_row11_col0\" class=\"data row11 col0\" >0.060280</td>\n",
       "      <td id=\"T_9f3e3_row11_col1\" class=\"data row11 col1\" >48</td>\n",
       "      <td id=\"T_9f3e3_row11_col2\" class=\"data row11 col2\" >0.5</td>\n",
       "      <td id=\"T_9f3e3_row11_col3\" class=\"data row11 col3\" >0.686792</td>\n",
       "      <td id=\"T_9f3e3_row11_col4\" class=\"data row11 col4\" >0.757402</td>\n",
       "      <td id=\"T_9f3e3_row11_col5\" class=\"data row11 col5\" >179626</td>\n",
       "      <td id=\"T_9f3e3_row11_col6\" class=\"data row11 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row11_col7\" class=\"data row11 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row12\" class=\"row_heading level0 row12\" >33</th>\n",
       "      <td id=\"T_9f3e3_row12_col0\" class=\"data row12 col0\" >0.034579</td>\n",
       "      <td id=\"T_9f3e3_row12_col1\" class=\"data row12 col1\" >62</td>\n",
       "      <td id=\"T_9f3e3_row12_col2\" class=\"data row12 col2\" >0.5</td>\n",
       "      <td id=\"T_9f3e3_row12_col3\" class=\"data row12 col3\" >0.680786</td>\n",
       "      <td id=\"T_9f3e3_row12_col4\" class=\"data row12 col4\" >0.791927</td>\n",
       "      <td id=\"T_9f3e3_row12_col5\" class=\"data row12 col5\" >179684</td>\n",
       "      <td id=\"T_9f3e3_row12_col6\" class=\"data row12 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row12_col7\" class=\"data row12 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row13\" class=\"row_heading level0 row13\" >28</th>\n",
       "      <td id=\"T_9f3e3_row13_col0\" class=\"data row13 col0\" >0.037686</td>\n",
       "      <td id=\"T_9f3e3_row13_col1\" class=\"data row13 col1\" >51</td>\n",
       "      <td id=\"T_9f3e3_row13_col2\" class=\"data row13 col2\" >0.46</td>\n",
       "      <td id=\"T_9f3e3_row13_col3\" class=\"data row13 col3\" >0.682205</td>\n",
       "      <td id=\"T_9f3e3_row13_col4\" class=\"data row13 col4\" >0.740336</td>\n",
       "      <td id=\"T_9f3e3_row13_col5\" class=\"data row13 col5\" >179766</td>\n",
       "      <td id=\"T_9f3e3_row13_col6\" class=\"data row13 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row13_col7\" class=\"data row13 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row14\" class=\"row_heading level0 row14\" >31</th>\n",
       "      <td id=\"T_9f3e3_row14_col0\" class=\"data row14 col0\" >0.030573</td>\n",
       "      <td id=\"T_9f3e3_row14_col1\" class=\"data row14 col1\" >60</td>\n",
       "      <td id=\"T_9f3e3_row14_col2\" class=\"data row14 col2\" >0.49</td>\n",
       "      <td id=\"T_9f3e3_row14_col3\" class=\"data row14 col3\" >0.680195</td>\n",
       "      <td id=\"T_9f3e3_row14_col4\" class=\"data row14 col4\" >0.790338</td>\n",
       "      <td id=\"T_9f3e3_row14_col5\" class=\"data row14 col5\" >180127</td>\n",
       "      <td id=\"T_9f3e3_row14_col6\" class=\"data row14 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row14_col7\" class=\"data row14 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row15\" class=\"row_heading level0 row15\" >44</th>\n",
       "      <td id=\"T_9f3e3_row15_col0\" class=\"data row15 col0\" >0.009720</td>\n",
       "      <td id=\"T_9f3e3_row15_col1\" class=\"data row15 col1\" >51</td>\n",
       "      <td id=\"T_9f3e3_row15_col2\" class=\"data row15 col2\" >0.53</td>\n",
       "      <td id=\"T_9f3e3_row15_col3\" class=\"data row15 col3\" >0.689594</td>\n",
       "      <td id=\"T_9f3e3_row15_col4\" class=\"data row15 col4\" >0.784736</td>\n",
       "      <td id=\"T_9f3e3_row15_col5\" class=\"data row15 col5\" >180129</td>\n",
       "      <td id=\"T_9f3e3_row15_col6\" class=\"data row15 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row15_col7\" class=\"data row15 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row16\" class=\"row_heading level0 row16\" >2</th>\n",
       "      <td id=\"T_9f3e3_row16_col0\" class=\"data row16 col0\" >0.031459</td>\n",
       "      <td id=\"T_9f3e3_row16_col1\" class=\"data row16 col1\" >59</td>\n",
       "      <td id=\"T_9f3e3_row16_col2\" class=\"data row16 col2\" >0.5</td>\n",
       "      <td id=\"T_9f3e3_row16_col3\" class=\"data row16 col3\" >0.707871</td>\n",
       "      <td id=\"T_9f3e3_row16_col4\" class=\"data row16 col4\" >0.747598</td>\n",
       "      <td id=\"T_9f3e3_row16_col5\" class=\"data row16 col5\" >180271</td>\n",
       "      <td id=\"T_9f3e3_row16_col6\" class=\"data row16 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row16_col7\" class=\"data row16 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row17\" class=\"row_heading level0 row17\" >6</th>\n",
       "      <td id=\"T_9f3e3_row17_col0\" class=\"data row17 col0\" >0.028483</td>\n",
       "      <td id=\"T_9f3e3_row17_col1\" class=\"data row17 col1\" >50</td>\n",
       "      <td id=\"T_9f3e3_row17_col2\" class=\"data row17 col2\" >0.53</td>\n",
       "      <td id=\"T_9f3e3_row17_col3\" class=\"data row17 col3\" >0.681757</td>\n",
       "      <td id=\"T_9f3e3_row17_col4\" class=\"data row17 col4\" >0.801177</td>\n",
       "      <td id=\"T_9f3e3_row17_col5\" class=\"data row17 col5\" >180667</td>\n",
       "      <td id=\"T_9f3e3_row17_col6\" class=\"data row17 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row17_col7\" class=\"data row17 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row18\" class=\"row_heading level0 row18\" >14</th>\n",
       "      <td id=\"T_9f3e3_row18_col0\" class=\"data row18 col0\" >0.011804</td>\n",
       "      <td id=\"T_9f3e3_row18_col1\" class=\"data row18 col1\" >65</td>\n",
       "      <td id=\"T_9f3e3_row18_col2\" class=\"data row18 col2\" >0.46</td>\n",
       "      <td id=\"T_9f3e3_row18_col3\" class=\"data row18 col3\" >0.732111</td>\n",
       "      <td id=\"T_9f3e3_row18_col4\" class=\"data row18 col4\" >0.588446</td>\n",
       "      <td id=\"T_9f3e3_row18_col5\" class=\"data row18 col5\" >180693</td>\n",
       "      <td id=\"T_9f3e3_row18_col6\" class=\"data row18 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row18_col7\" class=\"data row18 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row19\" class=\"row_heading level0 row19\" >41</th>\n",
       "      <td id=\"T_9f3e3_row19_col0\" class=\"data row19 col0\" >0.029686</td>\n",
       "      <td id=\"T_9f3e3_row19_col1\" class=\"data row19 col1\" >62</td>\n",
       "      <td id=\"T_9f3e3_row19_col2\" class=\"data row19 col2\" >0.51</td>\n",
       "      <td id=\"T_9f3e3_row19_col3\" class=\"data row19 col3\" >0.680346</td>\n",
       "      <td id=\"T_9f3e3_row19_col4\" class=\"data row19 col4\" >0.804617</td>\n",
       "      <td id=\"T_9f3e3_row19_col5\" class=\"data row19 col5\" >180699</td>\n",
       "      <td id=\"T_9f3e3_row19_col6\" class=\"data row19 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row19_col7\" class=\"data row19 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row20\" class=\"row_heading level0 row20\" >26</th>\n",
       "      <td id=\"T_9f3e3_row20_col0\" class=\"data row20 col0\" >0.022839</td>\n",
       "      <td id=\"T_9f3e3_row20_col1\" class=\"data row20 col1\" >70</td>\n",
       "      <td id=\"T_9f3e3_row20_col2\" class=\"data row20 col2\" >0.47</td>\n",
       "      <td id=\"T_9f3e3_row20_col3\" class=\"data row20 col3\" >0.695279</td>\n",
       "      <td id=\"T_9f3e3_row20_col4\" class=\"data row20 col4\" >0.726314</td>\n",
       "      <td id=\"T_9f3e3_row20_col5\" class=\"data row20 col5\" >180888</td>\n",
       "      <td id=\"T_9f3e3_row20_col6\" class=\"data row20 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row20_col7\" class=\"data row20 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row21\" class=\"row_heading level0 row21\" >37</th>\n",
       "      <td id=\"T_9f3e3_row21_col0\" class=\"data row21 col0\" >0.019409</td>\n",
       "      <td id=\"T_9f3e3_row21_col1\" class=\"data row21 col1\" >54</td>\n",
       "      <td id=\"T_9f3e3_row21_col2\" class=\"data row21 col2\" >0.43</td>\n",
       "      <td id=\"T_9f3e3_row21_col3\" class=\"data row21 col3\" >0.682406</td>\n",
       "      <td id=\"T_9f3e3_row21_col4\" class=\"data row21 col4\" >0.711563</td>\n",
       "      <td id=\"T_9f3e3_row21_col5\" class=\"data row21 col5\" >180965</td>\n",
       "      <td id=\"T_9f3e3_row21_col6\" class=\"data row21 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row21_col7\" class=\"data row21 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row22\" class=\"row_heading level0 row22\" >5</th>\n",
       "      <td id=\"T_9f3e3_row22_col0\" class=\"data row22 col0\" >0.051936</td>\n",
       "      <td id=\"T_9f3e3_row22_col1\" class=\"data row22 col1\" >34</td>\n",
       "      <td id=\"T_9f3e3_row22_col2\" class=\"data row22 col2\" >0.45</td>\n",
       "      <td id=\"T_9f3e3_row22_col3\" class=\"data row22 col3\" >0.684826</td>\n",
       "      <td id=\"T_9f3e3_row22_col4\" class=\"data row22 col4\" >0.706104</td>\n",
       "      <td id=\"T_9f3e3_row22_col5\" class=\"data row22 col5\" >180995</td>\n",
       "      <td id=\"T_9f3e3_row22_col6\" class=\"data row22 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row22_col7\" class=\"data row22 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row23\" class=\"row_heading level0 row23\" >22</th>\n",
       "      <td id=\"T_9f3e3_row23_col0\" class=\"data row23 col0\" >0.014691</td>\n",
       "      <td id=\"T_9f3e3_row23_col1\" class=\"data row23 col1\" >64</td>\n",
       "      <td id=\"T_9f3e3_row23_col2\" class=\"data row23 col2\" >0.47</td>\n",
       "      <td id=\"T_9f3e3_row23_col3\" class=\"data row23 col3\" >0.696216</td>\n",
       "      <td id=\"T_9f3e3_row23_col4\" class=\"data row23 col4\" >0.723196</td>\n",
       "      <td id=\"T_9f3e3_row23_col5\" class=\"data row23 col5\" >181054</td>\n",
       "      <td id=\"T_9f3e3_row23_col6\" class=\"data row23 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row23_col7\" class=\"data row23 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row24\" class=\"row_heading level0 row24\" >34</th>\n",
       "      <td id=\"T_9f3e3_row24_col0\" class=\"data row24 col0\" >0.012719</td>\n",
       "      <td id=\"T_9f3e3_row24_col1\" class=\"data row24 col1\" >57</td>\n",
       "      <td id=\"T_9f3e3_row24_col2\" class=\"data row24 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row24_col3\" class=\"data row24 col3\" >0.722176</td>\n",
       "      <td id=\"T_9f3e3_row24_col4\" class=\"data row24 col4\" >0.614283</td>\n",
       "      <td id=\"T_9f3e3_row24_col5\" class=\"data row24 col5\" >181341</td>\n",
       "      <td id=\"T_9f3e3_row24_col6\" class=\"data row24 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row24_col7\" class=\"data row24 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row25\" class=\"row_heading level0 row25\" >11</th>\n",
       "      <td id=\"T_9f3e3_row25_col0\" class=\"data row25 col0\" >0.023337</td>\n",
       "      <td id=\"T_9f3e3_row25_col1\" class=\"data row25 col1\" >59</td>\n",
       "      <td id=\"T_9f3e3_row25_col2\" class=\"data row25 col2\" >0.47</td>\n",
       "      <td id=\"T_9f3e3_row25_col3\" class=\"data row25 col3\" >0.708678</td>\n",
       "      <td id=\"T_9f3e3_row25_col4\" class=\"data row25 col4\" >0.709971</td>\n",
       "      <td id=\"T_9f3e3_row25_col5\" class=\"data row25 col5\" >181355</td>\n",
       "      <td id=\"T_9f3e3_row25_col6\" class=\"data row25 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row25_col7\" class=\"data row25 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row26\" class=\"row_heading level0 row26\" >1</th>\n",
       "      <td id=\"T_9f3e3_row26_col0\" class=\"data row26 col0\" >0.007813</td>\n",
       "      <td id=\"T_9f3e3_row26_col1\" class=\"data row26 col1\" >62</td>\n",
       "      <td id=\"T_9f3e3_row26_col2\" class=\"data row26 col2\" >0.51</td>\n",
       "      <td id=\"T_9f3e3_row26_col3\" class=\"data row26 col3\" >0.711169</td>\n",
       "      <td id=\"T_9f3e3_row26_col4\" class=\"data row26 col4\" >0.699549</td>\n",
       "      <td id=\"T_9f3e3_row26_col5\" class=\"data row26 col5\" >181461</td>\n",
       "      <td id=\"T_9f3e3_row26_col6\" class=\"data row26 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row26_col7\" class=\"data row26 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row27\" class=\"row_heading level0 row27\" >18</th>\n",
       "      <td id=\"T_9f3e3_row27_col0\" class=\"data row27 col0\" >0.047117</td>\n",
       "      <td id=\"T_9f3e3_row27_col1\" class=\"data row27 col1\" >61</td>\n",
       "      <td id=\"T_9f3e3_row27_col2\" class=\"data row27 col2\" >0.55</td>\n",
       "      <td id=\"T_9f3e3_row27_col3\" class=\"data row27 col3\" >0.685857</td>\n",
       "      <td id=\"T_9f3e3_row27_col4\" class=\"data row27 col4\" >0.805356</td>\n",
       "      <td id=\"T_9f3e3_row27_col5\" class=\"data row27 col5\" >181489</td>\n",
       "      <td id=\"T_9f3e3_row27_col6\" class=\"data row27 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row27_col7\" class=\"data row27 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row28\" class=\"row_heading level0 row28\" >47</th>\n",
       "      <td id=\"T_9f3e3_row28_col0\" class=\"data row28 col0\" >0.007034</td>\n",
       "      <td id=\"T_9f3e3_row28_col1\" class=\"data row28 col1\" >31</td>\n",
       "      <td id=\"T_9f3e3_row28_col2\" class=\"data row28 col2\" >0.47</td>\n",
       "      <td id=\"T_9f3e3_row28_col3\" class=\"data row28 col3\" >0.698623</td>\n",
       "      <td id=\"T_9f3e3_row28_col4\" class=\"data row28 col4\" >0.707413</td>\n",
       "      <td id=\"T_9f3e3_row28_col5\" class=\"data row28 col5\" >181925</td>\n",
       "      <td id=\"T_9f3e3_row28_col6\" class=\"data row28 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row28_col7\" class=\"data row28 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row29\" class=\"row_heading level0 row29\" >19</th>\n",
       "      <td id=\"T_9f3e3_row29_col0\" class=\"data row29 col0\" >0.043197</td>\n",
       "      <td id=\"T_9f3e3_row29_col1\" class=\"data row29 col1\" >70</td>\n",
       "      <td id=\"T_9f3e3_row29_col2\" class=\"data row29 col2\" >0.42</td>\n",
       "      <td id=\"T_9f3e3_row29_col3\" class=\"data row29 col3\" >0.683482</td>\n",
       "      <td id=\"T_9f3e3_row29_col4\" class=\"data row29 col4\" >0.700522</td>\n",
       "      <td id=\"T_9f3e3_row29_col5\" class=\"data row29 col5\" >181999</td>\n",
       "      <td id=\"T_9f3e3_row29_col6\" class=\"data row29 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row29_col7\" class=\"data row29 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row30\" class=\"row_heading level0 row30\" >32</th>\n",
       "      <td id=\"T_9f3e3_row30_col0\" class=\"data row30 col0\" >0.023262</td>\n",
       "      <td id=\"T_9f3e3_row30_col1\" class=\"data row30 col1\" >66</td>\n",
       "      <td id=\"T_9f3e3_row30_col2\" class=\"data row30 col2\" >0.51</td>\n",
       "      <td id=\"T_9f3e3_row30_col3\" class=\"data row30 col3\" >0.679438</td>\n",
       "      <td id=\"T_9f3e3_row30_col4\" class=\"data row30 col4\" >0.816901</td>\n",
       "      <td id=\"T_9f3e3_row30_col5\" class=\"data row30 col5\" >182532</td>\n",
       "      <td id=\"T_9f3e3_row30_col6\" class=\"data row30 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row30_col7\" class=\"data row30 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row31\" class=\"row_heading level0 row31\" >36</th>\n",
       "      <td id=\"T_9f3e3_row31_col0\" class=\"data row31 col0\" >0.008350</td>\n",
       "      <td id=\"T_9f3e3_row31_col1\" class=\"data row31 col1\" >47</td>\n",
       "      <td id=\"T_9f3e3_row31_col2\" class=\"data row31 col2\" >0.45</td>\n",
       "      <td id=\"T_9f3e3_row31_col3\" class=\"data row31 col3\" >0.692305</td>\n",
       "      <td id=\"T_9f3e3_row31_col4\" class=\"data row31 col4\" >0.696304</td>\n",
       "      <td id=\"T_9f3e3_row31_col5\" class=\"data row31 col5\" >182782</td>\n",
       "      <td id=\"T_9f3e3_row31_col6\" class=\"data row31 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row31_col7\" class=\"data row31 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row32\" class=\"row_heading level0 row32\" >8</th>\n",
       "      <td id=\"T_9f3e3_row32_col0\" class=\"data row32 col0\" >0.005014</td>\n",
       "      <td id=\"T_9f3e3_row32_col1\" class=\"data row32 col1\" >55</td>\n",
       "      <td id=\"T_9f3e3_row32_col2\" class=\"data row32 col2\" >0.57</td>\n",
       "      <td id=\"T_9f3e3_row32_col3\" class=\"data row32 col3\" >0.730548</td>\n",
       "      <td id=\"T_9f3e3_row32_col4\" class=\"data row32 col4\" >0.814997</td>\n",
       "      <td id=\"T_9f3e3_row32_col5\" class=\"data row32 col5\" >182802</td>\n",
       "      <td id=\"T_9f3e3_row32_col6\" class=\"data row32 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row32_col7\" class=\"data row32 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row33\" class=\"row_heading level0 row33\" >15</th>\n",
       "      <td id=\"T_9f3e3_row33_col0\" class=\"data row33 col0\" >0.086002</td>\n",
       "      <td id=\"T_9f3e3_row33_col1\" class=\"data row33 col1\" >64</td>\n",
       "      <td id=\"T_9f3e3_row33_col2\" class=\"data row33 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row33_col3\" class=\"data row33 col3\" >0.675855</td>\n",
       "      <td id=\"T_9f3e3_row33_col4\" class=\"data row33 col4\" >0.799068</td>\n",
       "      <td id=\"T_9f3e3_row33_col5\" class=\"data row33 col5\" >182899</td>\n",
       "      <td id=\"T_9f3e3_row33_col6\" class=\"data row33 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row33_col7\" class=\"data row33 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row34\" class=\"row_heading level0 row34\" >17</th>\n",
       "      <td id=\"T_9f3e3_row34_col0\" class=\"data row34 col0\" >0.003821</td>\n",
       "      <td id=\"T_9f3e3_row34_col1\" class=\"data row34 col1\" >67</td>\n",
       "      <td id=\"T_9f3e3_row34_col2\" class=\"data row34 col2\" >0.49</td>\n",
       "      <td id=\"T_9f3e3_row34_col3\" class=\"data row34 col3\" >0.711906</td>\n",
       "      <td id=\"T_9f3e3_row34_col4\" class=\"data row34 col4\" >0.626079</td>\n",
       "      <td id=\"T_9f3e3_row34_col5\" class=\"data row34 col5\" >183720</td>\n",
       "      <td id=\"T_9f3e3_row34_col6\" class=\"data row34 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row34_col7\" class=\"data row34 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row35\" class=\"row_heading level0 row35\" >23</th>\n",
       "      <td id=\"T_9f3e3_row35_col0\" class=\"data row35 col0\" >0.011065</td>\n",
       "      <td id=\"T_9f3e3_row35_col1\" class=\"data row35 col1\" >69</td>\n",
       "      <td id=\"T_9f3e3_row35_col2\" class=\"data row35 col2\" >0.4</td>\n",
       "      <td id=\"T_9f3e3_row35_col3\" class=\"data row35 col3\" >0.684884</td>\n",
       "      <td id=\"T_9f3e3_row35_col4\" class=\"data row35 col4\" >0.671911</td>\n",
       "      <td id=\"T_9f3e3_row35_col5\" class=\"data row35 col5\" >184264</td>\n",
       "      <td id=\"T_9f3e3_row35_col6\" class=\"data row35 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row35_col7\" class=\"data row35 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row36\" class=\"row_heading level0 row36\" >13</th>\n",
       "      <td id=\"T_9f3e3_row36_col0\" class=\"data row36 col0\" >0.015968</td>\n",
       "      <td id=\"T_9f3e3_row36_col1\" class=\"data row36 col1\" >69</td>\n",
       "      <td id=\"T_9f3e3_row36_col2\" class=\"data row36 col2\" >0.42</td>\n",
       "      <td id=\"T_9f3e3_row36_col3\" class=\"data row36 col3\" >0.703255</td>\n",
       "      <td id=\"T_9f3e3_row36_col4\" class=\"data row36 col4\" >0.653889</td>\n",
       "      <td id=\"T_9f3e3_row36_col5\" class=\"data row36 col5\" >186102</td>\n",
       "      <td id=\"T_9f3e3_row36_col6\" class=\"data row36 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row36_col7\" class=\"data row36 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row37\" class=\"row_heading level0 row37\" >39</th>\n",
       "      <td id=\"T_9f3e3_row37_col0\" class=\"data row37 col0\" >0.022815</td>\n",
       "      <td id=\"T_9f3e3_row37_col1\" class=\"data row37 col1\" >59</td>\n",
       "      <td id=\"T_9f3e3_row37_col2\" class=\"data row37 col2\" >0.6</td>\n",
       "      <td id=\"T_9f3e3_row37_col3\" class=\"data row37 col3\" >0.684202</td>\n",
       "      <td id=\"T_9f3e3_row37_col4\" class=\"data row37 col4\" >0.845642</td>\n",
       "      <td id=\"T_9f3e3_row37_col5\" class=\"data row37 col5\" >186482</td>\n",
       "      <td id=\"T_9f3e3_row37_col6\" class=\"data row37 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row37_col7\" class=\"data row37 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row38\" class=\"row_heading level0 row38\" >3</th>\n",
       "      <td id=\"T_9f3e3_row38_col0\" class=\"data row38 col0\" >0.071484</td>\n",
       "      <td id=\"T_9f3e3_row38_col1\" class=\"data row38 col1\" >51</td>\n",
       "      <td id=\"T_9f3e3_row38_col2\" class=\"data row38 col2\" >0.52</td>\n",
       "      <td id=\"T_9f3e3_row38_col3\" class=\"data row38 col3\" >0.675464</td>\n",
       "      <td id=\"T_9f3e3_row38_col4\" class=\"data row38 col4\" >0.830295</td>\n",
       "      <td id=\"T_9f3e3_row38_col5\" class=\"data row38 col5\" >186490</td>\n",
       "      <td id=\"T_9f3e3_row38_col6\" class=\"data row38 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row38_col7\" class=\"data row38 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row39\" class=\"row_heading level0 row39\" >30</th>\n",
       "      <td id=\"T_9f3e3_row39_col0\" class=\"data row39 col0\" >0.061805</td>\n",
       "      <td id=\"T_9f3e3_row39_col1\" class=\"data row39 col1\" >63</td>\n",
       "      <td id=\"T_9f3e3_row39_col2\" class=\"data row39 col2\" >0.49</td>\n",
       "      <td id=\"T_9f3e3_row39_col3\" class=\"data row39 col3\" >0.672677</td>\n",
       "      <td id=\"T_9f3e3_row39_col4\" class=\"data row39 col4\" >0.828987</td>\n",
       "      <td id=\"T_9f3e3_row39_col5\" class=\"data row39 col5\" >186865</td>\n",
       "      <td id=\"T_9f3e3_row39_col6\" class=\"data row39 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row39_col7\" class=\"data row39 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row40\" class=\"row_heading level0 row40\" >16</th>\n",
       "      <td id=\"T_9f3e3_row40_col0\" class=\"data row40 col0\" >0.016913</td>\n",
       "      <td id=\"T_9f3e3_row40_col1\" class=\"data row40 col1\" >57</td>\n",
       "      <td id=\"T_9f3e3_row40_col2\" class=\"data row40 col2\" >0.43</td>\n",
       "      <td id=\"T_9f3e3_row40_col3\" class=\"data row40 col3\" >0.735405</td>\n",
       "      <td id=\"T_9f3e3_row40_col4\" class=\"data row40 col4\" >0.528350</td>\n",
       "      <td id=\"T_9f3e3_row40_col5\" class=\"data row40 col5\" >186925</td>\n",
       "      <td id=\"T_9f3e3_row40_col6\" class=\"data row40 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row40_col7\" class=\"data row40 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row41\" class=\"row_heading level0 row41\" >29</th>\n",
       "      <td id=\"T_9f3e3_row41_col0\" class=\"data row41 col0\" >0.007996</td>\n",
       "      <td id=\"T_9f3e3_row41_col1\" class=\"data row41 col1\" >67</td>\n",
       "      <td id=\"T_9f3e3_row41_col2\" class=\"data row41 col2\" >0.4</td>\n",
       "      <td id=\"T_9f3e3_row41_col3\" class=\"data row41 col3\" >0.691887</td>\n",
       "      <td id=\"T_9f3e3_row41_col4\" class=\"data row41 col4\" >0.646940</td>\n",
       "      <td id=\"T_9f3e3_row41_col5\" class=\"data row41 col5\" >187409</td>\n",
       "      <td id=\"T_9f3e3_row41_col6\" class=\"data row41 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row41_col7\" class=\"data row41 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row42\" class=\"row_heading level0 row42\" >0</th>\n",
       "      <td id=\"T_9f3e3_row42_col0\" class=\"data row42 col0\" >0.008361</td>\n",
       "      <td id=\"T_9f3e3_row42_col1\" class=\"data row42 col1\" >67</td>\n",
       "      <td id=\"T_9f3e3_row42_col2\" class=\"data row42 col2\" >0.4</td>\n",
       "      <td id=\"T_9f3e3_row42_col3\" class=\"data row42 col3\" >0.694477</td>\n",
       "      <td id=\"T_9f3e3_row42_col4\" class=\"data row42 col4\" >0.641302</td>\n",
       "      <td id=\"T_9f3e3_row42_col5\" class=\"data row42 col5\" >188196</td>\n",
       "      <td id=\"T_9f3e3_row42_col6\" class=\"data row42 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row42_col7\" class=\"data row42 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row43\" class=\"row_heading level0 row43\" >27</th>\n",
       "      <td id=\"T_9f3e3_row43_col0\" class=\"data row43 col0\" >0.010372</td>\n",
       "      <td id=\"T_9f3e3_row43_col1\" class=\"data row43 col1\" >57</td>\n",
       "      <td id=\"T_9f3e3_row43_col2\" class=\"data row43 col2\" >0.42</td>\n",
       "      <td id=\"T_9f3e3_row43_col3\" class=\"data row43 col3\" >0.731249</td>\n",
       "      <td id=\"T_9f3e3_row43_col4\" class=\"data row43 col4\" >0.477590</td>\n",
       "      <td id=\"T_9f3e3_row43_col5\" class=\"data row43 col5\" >195005</td>\n",
       "      <td id=\"T_9f3e3_row43_col6\" class=\"data row43 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row43_col7\" class=\"data row43 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row44\" class=\"row_heading level0 row44\" >40</th>\n",
       "      <td id=\"T_9f3e3_row44_col0\" class=\"data row44 col0\" >0.006144</td>\n",
       "      <td id=\"T_9f3e3_row44_col1\" class=\"data row44 col1\" >41</td>\n",
       "      <td id=\"T_9f3e3_row44_col2\" class=\"data row44 col2\" >0.44</td>\n",
       "      <td id=\"T_9f3e3_row44_col3\" class=\"data row44 col3\" >0.712273</td>\n",
       "      <td id=\"T_9f3e3_row44_col4\" class=\"data row44 col4\" >0.428499</td>\n",
       "      <td id=\"T_9f3e3_row44_col5\" class=\"data row44 col5\" >207630</td>\n",
       "      <td id=\"T_9f3e3_row44_col6\" class=\"data row44 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row44_col7\" class=\"data row44 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row45\" class=\"row_heading level0 row45\" >9</th>\n",
       "      <td id=\"T_9f3e3_row45_col0\" class=\"data row45 col0\" >0.003568</td>\n",
       "      <td id=\"T_9f3e3_row45_col1\" class=\"data row45 col1\" >37</td>\n",
       "      <td id=\"T_9f3e3_row45_col2\" class=\"data row45 col2\" >0.45</td>\n",
       "      <td id=\"T_9f3e3_row45_col3\" class=\"data row45 col3\" >0.705797</td>\n",
       "      <td id=\"T_9f3e3_row45_col4\" class=\"data row45 col4\" >0.430941</td>\n",
       "      <td id=\"T_9f3e3_row45_col5\" class=\"data row45 col5\" >208401</td>\n",
       "      <td id=\"T_9f3e3_row45_col6\" class=\"data row45 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row45_col7\" class=\"data row45 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row46\" class=\"row_heading level0 row46\" >25</th>\n",
       "      <td id=\"T_9f3e3_row46_col0\" class=\"data row46 col0\" >0.006325</td>\n",
       "      <td id=\"T_9f3e3_row46_col1\" class=\"data row46 col1\" >65</td>\n",
       "      <td id=\"T_9f3e3_row46_col2\" class=\"data row46 col2\" >0.44</td>\n",
       "      <td id=\"T_9f3e3_row46_col3\" class=\"data row46 col3\" >0.711963</td>\n",
       "      <td id=\"T_9f3e3_row46_col4\" class=\"data row46 col4\" >0.409898</td>\n",
       "      <td id=\"T_9f3e3_row46_col5\" class=\"data row46 col5\" >210879</td>\n",
       "      <td id=\"T_9f3e3_row46_col6\" class=\"data row46 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row46_col7\" class=\"data row46 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row47\" class=\"row_heading level0 row47\" >45</th>\n",
       "      <td id=\"T_9f3e3_row47_col0\" class=\"data row47 col0\" >0.002000</td>\n",
       "      <td id=\"T_9f3e3_row47_col1\" class=\"data row47 col1\" >55</td>\n",
       "      <td id=\"T_9f3e3_row47_col2\" class=\"data row47 col2\" >0.48</td>\n",
       "      <td id=\"T_9f3e3_row47_col3\" class=\"data row47 col3\" >0.686549</td>\n",
       "      <td id=\"T_9f3e3_row47_col4\" class=\"data row47 col4\" >0.426087</td>\n",
       "      <td id=\"T_9f3e3_row47_col5\" class=\"data row47 col5\" >213879</td>\n",
       "      <td id=\"T_9f3e3_row47_col6\" class=\"data row47 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row47_col7\" class=\"data row47 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row48\" class=\"row_heading level0 row48\" >7</th>\n",
       "      <td id=\"T_9f3e3_row48_col0\" class=\"data row48 col0\" >0.001694</td>\n",
       "      <td id=\"T_9f3e3_row48_col1\" class=\"data row48 col1\" >43</td>\n",
       "      <td id=\"T_9f3e3_row48_col2\" class=\"data row48 col2\" >0.53</td>\n",
       "      <td id=\"T_9f3e3_row48_col3\" class=\"data row48 col3\" >0.679693</td>\n",
       "      <td id=\"T_9f3e3_row48_col4\" class=\"data row48 col4\" >0.884174</td>\n",
       "      <td id=\"T_9f3e3_row48_col5\" class=\"data row48 col5\" >222812</td>\n",
       "      <td id=\"T_9f3e3_row48_col6\" class=\"data row48 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row48_col7\" class=\"data row48 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f3e3_level0_row49\" class=\"row_heading level0 row49\" >4</th>\n",
       "      <td id=\"T_9f3e3_row49_col0\" class=\"data row49 col0\" >0.001103</td>\n",
       "      <td id=\"T_9f3e3_row49_col1\" class=\"data row49 col1\" >46</td>\n",
       "      <td id=\"T_9f3e3_row49_col2\" class=\"data row49 col2\" >0.59</td>\n",
       "      <td id=\"T_9f3e3_row49_col3\" class=\"data row49 col3\" >0.678416</td>\n",
       "      <td id=\"T_9f3e3_row49_col4\" class=\"data row49 col4\" >0.919268</td>\n",
       "      <td id=\"T_9f3e3_row49_col5\" class=\"data row49 col5\" >248030</td>\n",
       "      <td id=\"T_9f3e3_row49_col6\" class=\"data row49 col6\" >0.000000</td>\n",
       "      <td id=\"T_9f3e3_row49_col7\" class=\"data row49 col7\" >2.165263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2391f9653a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_LGM = (styled_df_LGM.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score','Fit Time', 'Prediction Time'])\n",
    "                          .format({'Threshold': \"{:g}\"}))\n",
    "\n",
    "styled_df_LGM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede4b98-3785-446c-b4ec-ba04b3c24f48",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc1003f1-6f66-48d6-9976-05f411a0a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.021541725724886718,\n",
       " 'num_leaves': 70,\n",
       " 'n_estimators': 140,\n",
       " 'threshold': 0.48000000000000004}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "511e12d8-9038-4464-8f5d-00171a57a5da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,441260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66858\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 613\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,499995 -> initscore=-0,000020\n",
      "[LightGBM] [Info] Start training from score -0,000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,454978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66728\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,499995 -> initscore=-0,000020\n",
      "[LightGBM] [Info] Start training from score -0,000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,457860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66792\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 616\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,499995 -> initscore=-0,000020\n",
      "[LightGBM] [Info] Start training from score -0,000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,440101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66798\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 614\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,500008 -> initscore=0,000030\n",
      "[LightGBM] [Info] Start training from score 0,000030\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,446198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66812\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 616\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,500008 -> initscore=0,000030\n",
      "[LightGBM] [Info] Start training from score 0,000030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supposition que df_classification_imputed est déjà défini\n",
    "nb_0 = len(application_train[application_train[\"TARGET\"] == 0])\n",
    "nb_1 = len(application_train[application_train[\"TARGET\"] == 1])\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "X = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train[\"TARGET\"]\n",
    "\n",
    "# Stockage des noms des colonnes pour utilisation ultérieure\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Instanciation du modèle\n",
    "clf = LGBMClassifier(**study.best_params, class_weight=class_weights, verbose=1)\n",
    "\n",
    "# Validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    clf.fit(X[train], y.iloc[train])\n",
    "    feature_importances += clf.feature_importances_\n",
    "\n",
    "# Moyenne des importances de caractéristiques sur les plis\n",
    "feature_importances /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e45d1cb-b03d-4620-916b-88f7bb77e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "df_feature_importances_lightGBM = df_feature_importances_lightGBM.sort_values(by='importance', ascending=False)\n",
    "df_feature_importances_lightGBM[\"importance\"] = (df_feature_importances_lightGBM[\"importance\"]/ df_feature_importances_lightGBM[\"importance\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3438bc62-f153-484b-ab57-24b8c7eb42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM.to_csv('df_feature_importances_lightGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7936e544-a7b0-4167-878a-1a5954ae0f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAYS_BIRTH</td>\n",
       "      <td>1.757764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMT_ANNUITY</td>\n",
       "      <td>1.602484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>DAYS_LAST_PHONE_CHANGE</td>\n",
       "      <td>1.461698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMT_CREDIT</td>\n",
       "      <td>1.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DAYS_ID_PUBLISH</td>\n",
       "      <td>1.339545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>ORGANIZATION_TYPE_XNA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>ORGANIZATION_TYPE_Trade: type 5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>PREV_RATE_DOWN_PAYMENT_MEAN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>ORGANIZATION_TYPE_Trade: type 1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>CC_COUNT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             feature  importance\n",
       "5                         DAYS_BIRTH    1.757764\n",
       "3                        AMT_ANNUITY    1.602484\n",
       "73            DAYS_LAST_PHONE_CHANGE    1.461698\n",
       "2                         AMT_CREDIT    1.374741\n",
       "8                    DAYS_ID_PUBLISH    1.339545\n",
       "..                               ...         ...\n",
       "224            ORGANIZATION_TYPE_XNA    0.000000\n",
       "216  ORGANIZATION_TYPE_Trade: type 5    0.000000\n",
       "372      PREV_RATE_DOWN_PAYMENT_MEAN    0.000000\n",
       "212  ORGANIZATION_TYPE_Trade: type 1    0.000000\n",
       "623                         CC_COUNT    0.000000\n",
       "\n",
       "[624 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importances_lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d409ee11-4d70-439a-8548-f17d1f8785ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pourcentage_importance_totale = 95\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "df_sorted = df_feature_importances_lightGBM.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "df_sorted[\"cumulative_importance\"] = df_sorted[\"importance\"].cumsum()\n",
    "\n",
    "# Find the number of features needed to reach 99% of the total importance\n",
    "num_features = df_sorted[df_sorted[\"cumulative_importance\"] <= df_sorted[\"importance\"].sum() * Pourcentage_importance_totale/100].shape[0]\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02e1e6c8-d512-4d6a-8484-9331af709b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les colonnes 'feature' et 'importance' de votre dataframe\n",
    "features = df_feature_importances_lightGBM['feature'].to_numpy()\n",
    "importances = df_feature_importances_lightGBM['importance'].to_numpy()\n",
    "\n",
    "# Création d'un masque pour les importances supérieures à 0\n",
    "mask = importances > 0\n",
    "\n",
    "# Filtrage des features et des importances en utilisant le masque\n",
    "filtered_feature_names = features[mask]\n",
    "filtered_feature_importances = importances[mask]\n",
    "\n",
    "# Supposons que vous vouliez afficher les 10 principales caractéristiques\n",
    "n_features_to_display = 20\n",
    "\n",
    "# Tri des indices filtrés par importance\n",
    "sorted_idx = filtered_feature_importances.argsort()\n",
    "\n",
    "# Limiter aux n principales caractéristiques\n",
    "top_n_idx = sorted_idx[-n_features_to_display:]\n",
    "\n",
    "# Affichage de l'importance des caractéristiques filtrées pour les n principales\n",
    "plt.figure(figsize=(10, len(top_n_idx) * 0.4))\n",
    "bars = plt.barh(filtered_feature_names[top_n_idx], filtered_feature_importances[top_n_idx])\n",
    "\n",
    "# Ajuster les limites de l'axe des y pour réduire l'espace autour des barres\n",
    "plt.ylim(-0.5, len(top_n_idx)-0.5)\n",
    "\n",
    "plt.xlabel(\"Importance des caractéristiques\")\n",
    "plt.title(\"Importance des caractéristiques avec le lightGBM\")\n",
    "\n",
    "# Ajouter les valeurs à côté des barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    \n",
    "    # Déterminez le formatage en fonction de la magnitude\n",
    "    if width < 0.01:\n",
    "        fmt = '{:.4f}'\n",
    "    elif width < 0.1:\n",
    "        fmt = '{:.3f}'\n",
    "    else:\n",
    "        fmt = '{:.2f}'\n",
    "    \n",
    "    plt.text(width + 0.01 * width,  # Ajouter un padding proportionnel à la valeur\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             fmt.format(width),\n",
    "             va='center', ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba55e3a-d726-4646-a8d3-9d50acf072d4",
   "metadata": {},
   "source": [
    "### 2.3.2 Dataset petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b14dcb44-2890-4032-a241-2e198aac052d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 3236.89 secondes\n",
      "{'lr': 0.03202900440110068, 'num_leaves': 50, 'n_estimators': 500, 'threshold': 0.52}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('LightGBM')\n",
    "\n",
    "# Supposition : vous avez déjà votre dataframe 'df_classification' chargé\n",
    "nb_0 = (application_train_petit_clean['TARGET'] == 0).sum()\n",
    "nb_1 = (application_train_petit_clean['TARGET'] == 1).sum()\n",
    "\n",
    "X = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_petit_clean[\"TARGET\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "results = []\n",
    "\n",
    "total_combinations = 50  # définissez cela avant d'appeler study.optimize()\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LGBMClassifier(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators, class_weight=class_weights, verbose=-1)\n",
    "    \n",
    "    # Enregistrement du temps de début pour le fit\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    # Calculer le temps de fit\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    # Enregistrement du temps de début pour la prédiction\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    # Calculer le temps de prédiction\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "    \n",
    "    results.append({\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Learning Rate\", lr)\n",
    "        mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "        mlflow.log_param(\"N Estimators\", n_estimators) \n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        \n",
    "        # Enregistrer les temps dans mlflow\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "# 6. Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f37192a-2b60-4020-9696-714595d5b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4be8457f-2fdf-47f8-a2b1-6bf837d6849d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM.to_csv('results_LGM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f0ba39d8-6895-4c2e-a2f1-9f1238fda0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1f3fa_row0_col5, #T_1f3fa_row26_col3, #T_1f3fa_row48_col4 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1f3fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1f3fa_level0_col0\" class=\"col_heading level0 col0\" >Learning Rate</th>\n",
       "      <th id=\"T_1f3fa_level0_col1\" class=\"col_heading level0 col1\" >Num Leaves</th>\n",
       "      <th id=\"T_1f3fa_level0_col2\" class=\"col_heading level0 col2\" >Threshold</th>\n",
       "      <th id=\"T_1f3fa_level0_col3\" class=\"col_heading level0 col3\" >AUC</th>\n",
       "      <th id=\"T_1f3fa_level0_col4\" class=\"col_heading level0 col4\" >Accuracy</th>\n",
       "      <th id=\"T_1f3fa_level0_col5\" class=\"col_heading level0 col5\" >Business Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row0\" class=\"row_heading level0 row0\" >47</th>\n",
       "      <td id=\"T_1f3fa_row0_col0\" class=\"data row0 col0\" >0.032029</td>\n",
       "      <td id=\"T_1f3fa_row0_col1\" class=\"data row0 col1\" >50</td>\n",
       "      <td id=\"T_1f3fa_row0_col2\" class=\"data row0 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row0_col3\" class=\"data row0 col3\" >0.759112</td>\n",
       "      <td id=\"T_1f3fa_row0_col4\" class=\"data row0 col4\" >0.747894</td>\n",
       "      <td id=\"T_1f3fa_row0_col5\" class=\"data row0 col5\" >162288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row1\" class=\"row_heading level0 row1\" >24</th>\n",
       "      <td id=\"T_1f3fa_row1_col0\" class=\"data row1 col0\" >0.017909</td>\n",
       "      <td id=\"T_1f3fa_row1_col1\" class=\"data row1 col1\" >46</td>\n",
       "      <td id=\"T_1f3fa_row1_col2\" class=\"data row1 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row1_col3\" class=\"data row1 col3\" >0.759633</td>\n",
       "      <td id=\"T_1f3fa_row1_col4\" class=\"data row1 col4\" >0.737293</td>\n",
       "      <td id=\"T_1f3fa_row1_col5\" class=\"data row1 col5\" >162305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row2\" class=\"row_heading level0 row2\" >36</th>\n",
       "      <td id=\"T_1f3fa_row2_col0\" class=\"data row2 col0\" >0.023829</td>\n",
       "      <td id=\"T_1f3fa_row2_col1\" class=\"data row2 col1\" >42</td>\n",
       "      <td id=\"T_1f3fa_row2_col2\" class=\"data row2 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row2_col3\" class=\"data row2 col3\" >0.759436</td>\n",
       "      <td id=\"T_1f3fa_row2_col4\" class=\"data row2 col4\" >0.739555</td>\n",
       "      <td id=\"T_1f3fa_row2_col5\" class=\"data row2 col5\" >162411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row3\" class=\"row_heading level0 row3\" >49</th>\n",
       "      <td id=\"T_1f3fa_row3_col0\" class=\"data row3 col0\" >0.039426</td>\n",
       "      <td id=\"T_1f3fa_row3_col1\" class=\"data row3 col1\" >49</td>\n",
       "      <td id=\"T_1f3fa_row3_col2\" class=\"data row3 col2\" >0.49</td>\n",
       "      <td id=\"T_1f3fa_row3_col3\" class=\"data row3 col3\" >0.758136</td>\n",
       "      <td id=\"T_1f3fa_row3_col4\" class=\"data row3 col4\" >0.721829</td>\n",
       "      <td id=\"T_1f3fa_row3_col5\" class=\"data row3 col5\" >162547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row4\" class=\"row_heading level0 row4\" >18</th>\n",
       "      <td id=\"T_1f3fa_row4_col0\" class=\"data row4 col0\" >0.041528</td>\n",
       "      <td id=\"T_1f3fa_row4_col1\" class=\"data row4 col1\" >53</td>\n",
       "      <td id=\"T_1f3fa_row4_col2\" class=\"data row4 col2\" >0.51</td>\n",
       "      <td id=\"T_1f3fa_row4_col3\" class=\"data row4 col3\" >0.758528</td>\n",
       "      <td id=\"T_1f3fa_row4_col4\" class=\"data row4 col4\" >0.740398</td>\n",
       "      <td id=\"T_1f3fa_row4_col5\" class=\"data row4 col5\" >162611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row5\" class=\"row_heading level0 row5\" >32</th>\n",
       "      <td id=\"T_1f3fa_row5_col0\" class=\"data row5 col0\" >0.024994</td>\n",
       "      <td id=\"T_1f3fa_row5_col1\" class=\"data row5 col1\" >49</td>\n",
       "      <td id=\"T_1f3fa_row5_col2\" class=\"data row5 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row5_col3\" class=\"data row5 col3\" >0.759242</td>\n",
       "      <td id=\"T_1f3fa_row5_col4\" class=\"data row5 col4\" >0.732257</td>\n",
       "      <td id=\"T_1f3fa_row5_col5\" class=\"data row5 col5\" >162880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row6\" class=\"row_heading level0 row6\" >22</th>\n",
       "      <td id=\"T_1f3fa_row6_col0\" class=\"data row6 col0\" >0.017209</td>\n",
       "      <td id=\"T_1f3fa_row6_col1\" class=\"data row6 col1\" >52</td>\n",
       "      <td id=\"T_1f3fa_row6_col2\" class=\"data row6 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row6_col3\" class=\"data row6 col3\" >0.759193</td>\n",
       "      <td id=\"T_1f3fa_row6_col4\" class=\"data row6 col4\" >0.730262</td>\n",
       "      <td id=\"T_1f3fa_row6_col5\" class=\"data row6 col5\" >162980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row7\" class=\"row_heading level0 row7\" >21</th>\n",
       "      <td id=\"T_1f3fa_row7_col0\" class=\"data row7 col0\" >0.019473</td>\n",
       "      <td id=\"T_1f3fa_row7_col1\" class=\"data row7 col1\" >51</td>\n",
       "      <td id=\"T_1f3fa_row7_col2\" class=\"data row7 col2\" >0.51</td>\n",
       "      <td id=\"T_1f3fa_row7_col3\" class=\"data row7 col3\" >0.759131</td>\n",
       "      <td id=\"T_1f3fa_row7_col4\" class=\"data row7 col4\" >0.720312</td>\n",
       "      <td id=\"T_1f3fa_row7_col5\" class=\"data row7 col5\" >163031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row8\" class=\"row_heading level0 row8\" >48</th>\n",
       "      <td id=\"T_1f3fa_row8_col0\" class=\"data row8 col0\" >0.031788</td>\n",
       "      <td id=\"T_1f3fa_row8_col1\" class=\"data row8 col1\" >44</td>\n",
       "      <td id=\"T_1f3fa_row8_col2\" class=\"data row8 col2\" >0.54</td>\n",
       "      <td id=\"T_1f3fa_row8_col3\" class=\"data row8 col3\" >0.759187</td>\n",
       "      <td id=\"T_1f3fa_row8_col4\" class=\"data row8 col4\" >0.762203</td>\n",
       "      <td id=\"T_1f3fa_row8_col5\" class=\"data row8 col5\" >163031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row9\" class=\"row_heading level0 row9\" >46</th>\n",
       "      <td id=\"T_1f3fa_row9_col0\" class=\"data row9 col0\" >0.016861</td>\n",
       "      <td id=\"T_1f3fa_row9_col1\" class=\"data row9 col1\" >56</td>\n",
       "      <td id=\"T_1f3fa_row9_col2\" class=\"data row9 col2\" >0.53</td>\n",
       "      <td id=\"T_1f3fa_row9_col3\" class=\"data row9 col3\" >0.759239</td>\n",
       "      <td id=\"T_1f3fa_row9_col4\" class=\"data row9 col4\" >0.753584</td>\n",
       "      <td id=\"T_1f3fa_row9_col5\" class=\"data row9 col5\" >163051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row10\" class=\"row_heading level0 row10\" >33</th>\n",
       "      <td id=\"T_1f3fa_row10_col0\" class=\"data row10 col0\" >0.044934</td>\n",
       "      <td id=\"T_1f3fa_row10_col1\" class=\"data row10 col1\" >45</td>\n",
       "      <td id=\"T_1f3fa_row10_col2\" class=\"data row10 col2\" >0.53</td>\n",
       "      <td id=\"T_1f3fa_row10_col3\" class=\"data row10 col3\" >0.758831</td>\n",
       "      <td id=\"T_1f3fa_row10_col4\" class=\"data row10 col4\" >0.746957</td>\n",
       "      <td id=\"T_1f3fa_row10_col5\" class=\"data row10 col5\" >163053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row11\" class=\"row_heading level0 row11\" >19</th>\n",
       "      <td id=\"T_1f3fa_row11_col0\" class=\"data row11 col0\" >0.047181</td>\n",
       "      <td id=\"T_1f3fa_row11_col1\" class=\"data row11 col1\" >51</td>\n",
       "      <td id=\"T_1f3fa_row11_col2\" class=\"data row11 col2\" >0.51</td>\n",
       "      <td id=\"T_1f3fa_row11_col3\" class=\"data row11 col3\" >0.757844</td>\n",
       "      <td id=\"T_1f3fa_row11_col4\" class=\"data row11 col4\" >0.741882</td>\n",
       "      <td id=\"T_1f3fa_row11_col5\" class=\"data row11 col5\" >163064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row12\" class=\"row_heading level0 row12\" >30</th>\n",
       "      <td id=\"T_1f3fa_row12_col0\" class=\"data row12 col0\" >0.023664</td>\n",
       "      <td id=\"T_1f3fa_row12_col1\" class=\"data row12 col1\" >54</td>\n",
       "      <td id=\"T_1f3fa_row12_col2\" class=\"data row12 col2\" >0.49</td>\n",
       "      <td id=\"T_1f3fa_row12_col3\" class=\"data row12 col3\" >0.759443</td>\n",
       "      <td id=\"T_1f3fa_row12_col4\" class=\"data row12 col4\" >0.706244</td>\n",
       "      <td id=\"T_1f3fa_row12_col5\" class=\"data row12 col5\" >163078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row13\" class=\"row_heading level0 row13\" >15</th>\n",
       "      <td id=\"T_1f3fa_row13_col0\" class=\"data row13 col0\" >0.035219</td>\n",
       "      <td id=\"T_1f3fa_row13_col1\" class=\"data row13 col1\" >63</td>\n",
       "      <td id=\"T_1f3fa_row13_col2\" class=\"data row13 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row13_col3\" class=\"data row13 col3\" >0.757913</td>\n",
       "      <td id=\"T_1f3fa_row13_col4\" class=\"data row13 col4\" >0.755087</td>\n",
       "      <td id=\"T_1f3fa_row13_col5\" class=\"data row13 col5\" >163165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row14\" class=\"row_heading level0 row14\" >13</th>\n",
       "      <td id=\"T_1f3fa_row14_col0\" class=\"data row14 col0\" >0.034980</td>\n",
       "      <td id=\"T_1f3fa_row14_col1\" class=\"data row14 col1\" >63</td>\n",
       "      <td id=\"T_1f3fa_row14_col2\" class=\"data row14 col2\" >0.45</td>\n",
       "      <td id=\"T_1f3fa_row14_col3\" class=\"data row14 col3\" >0.758517</td>\n",
       "      <td id=\"T_1f3fa_row14_col4\" class=\"data row14 col4\" >0.687034</td>\n",
       "      <td id=\"T_1f3fa_row14_col5\" class=\"data row14 col5\" >163544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row15\" class=\"row_heading level0 row15\" >12</th>\n",
       "      <td id=\"T_1f3fa_row15_col0\" class=\"data row15 col0\" >0.028117</td>\n",
       "      <td id=\"T_1f3fa_row15_col1\" class=\"data row15 col1\" >59</td>\n",
       "      <td id=\"T_1f3fa_row15_col2\" class=\"data row15 col2\" >0.54</td>\n",
       "      <td id=\"T_1f3fa_row15_col3\" class=\"data row15 col3\" >0.758690</td>\n",
       "      <td id=\"T_1f3fa_row15_col4\" class=\"data row15 col4\" >0.750859</td>\n",
       "      <td id=\"T_1f3fa_row15_col5\" class=\"data row15 col5\" >163591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row16\" class=\"row_heading level0 row16\" >27</th>\n",
       "      <td id=\"T_1f3fa_row16_col0\" class=\"data row16 col0\" >0.008852</td>\n",
       "      <td id=\"T_1f3fa_row16_col1\" class=\"data row16 col1\" >48</td>\n",
       "      <td id=\"T_1f3fa_row16_col2\" class=\"data row16 col2\" >0.5</td>\n",
       "      <td id=\"T_1f3fa_row16_col3\" class=\"data row16 col3\" >0.758715</td>\n",
       "      <td id=\"T_1f3fa_row16_col4\" class=\"data row16 col4\" >0.706745</td>\n",
       "      <td id=\"T_1f3fa_row16_col5\" class=\"data row16 col5\" >163869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row17\" class=\"row_heading level0 row17\" >42</th>\n",
       "      <td id=\"T_1f3fa_row17_col0\" class=\"data row17 col0\" >0.022498</td>\n",
       "      <td id=\"T_1f3fa_row17_col1\" class=\"data row17 col1\" >36</td>\n",
       "      <td id=\"T_1f3fa_row17_col2\" class=\"data row17 col2\" >0.51</td>\n",
       "      <td id=\"T_1f3fa_row17_col3\" class=\"data row17 col3\" >0.758094</td>\n",
       "      <td id=\"T_1f3fa_row17_col4\" class=\"data row17 col4\" >0.712966</td>\n",
       "      <td id=\"T_1f3fa_row17_col5\" class=\"data row17 col5\" >163884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row18\" class=\"row_heading level0 row18\" >31</th>\n",
       "      <td id=\"T_1f3fa_row18_col0\" class=\"data row18 col0\" >0.016442</td>\n",
       "      <td id=\"T_1f3fa_row18_col1\" class=\"data row18 col1\" >52</td>\n",
       "      <td id=\"T_1f3fa_row18_col2\" class=\"data row18 col2\" >0.5</td>\n",
       "      <td id=\"T_1f3fa_row18_col3\" class=\"data row18 col3\" >0.758281</td>\n",
       "      <td id=\"T_1f3fa_row18_col4\" class=\"data row18 col4\" >0.706742</td>\n",
       "      <td id=\"T_1f3fa_row18_col5\" class=\"data row18 col5\" >163897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row19\" class=\"row_heading level0 row19\" >23</th>\n",
       "      <td id=\"T_1f3fa_row19_col0\" class=\"data row19 col0\" >0.019783</td>\n",
       "      <td id=\"T_1f3fa_row19_col1\" class=\"data row19 col1\" >54</td>\n",
       "      <td id=\"T_1f3fa_row19_col2\" class=\"data row19 col2\" >0.56</td>\n",
       "      <td id=\"T_1f3fa_row19_col3\" class=\"data row19 col3\" >0.759416</td>\n",
       "      <td id=\"T_1f3fa_row19_col4\" class=\"data row19 col4\" >0.770307</td>\n",
       "      <td id=\"T_1f3fa_row19_col5\" class=\"data row19 col5\" >163943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row20\" class=\"row_heading level0 row20\" >43</th>\n",
       "      <td id=\"T_1f3fa_row20_col0\" class=\"data row20 col0\" >0.029732</td>\n",
       "      <td id=\"T_1f3fa_row20_col1\" class=\"data row20 col1\" >49</td>\n",
       "      <td id=\"T_1f3fa_row20_col2\" class=\"data row20 col2\" >0.55</td>\n",
       "      <td id=\"T_1f3fa_row20_col3\" class=\"data row20 col3\" >0.759025</td>\n",
       "      <td id=\"T_1f3fa_row20_col4\" class=\"data row20 col4\" >0.773055</td>\n",
       "      <td id=\"T_1f3fa_row20_col5\" class=\"data row20 col5\" >164044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row21\" class=\"row_heading level0 row21\" >25</th>\n",
       "      <td id=\"T_1f3fa_row21_col0\" class=\"data row21 col0\" >0.008596</td>\n",
       "      <td id=\"T_1f3fa_row21_col1\" class=\"data row21 col1\" >46</td>\n",
       "      <td id=\"T_1f3fa_row21_col2\" class=\"data row21 col2\" >0.53</td>\n",
       "      <td id=\"T_1f3fa_row21_col3\" class=\"data row21 col3\" >0.756947</td>\n",
       "      <td id=\"T_1f3fa_row21_col4\" class=\"data row21 col4\" >0.733882</td>\n",
       "      <td id=\"T_1f3fa_row21_col5\" class=\"data row21 col5\" >164136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row22\" class=\"row_heading level0 row22\" >41</th>\n",
       "      <td id=\"T_1f3fa_row22_col0\" class=\"data row22 col0\" >0.014902</td>\n",
       "      <td id=\"T_1f3fa_row22_col1\" class=\"data row22 col1\" >42</td>\n",
       "      <td id=\"T_1f3fa_row22_col2\" class=\"data row22 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row22_col3\" class=\"data row22 col3\" >0.756805</td>\n",
       "      <td id=\"T_1f3fa_row22_col4\" class=\"data row22 col4\" >0.722753</td>\n",
       "      <td id=\"T_1f3fa_row22_col5\" class=\"data row22 col5\" >164324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row23\" class=\"row_heading level0 row23\" >1</th>\n",
       "      <td id=\"T_1f3fa_row23_col0\" class=\"data row23 col0\" >0.026561</td>\n",
       "      <td id=\"T_1f3fa_row23_col1\" class=\"data row23 col1\" >56</td>\n",
       "      <td id=\"T_1f3fa_row23_col2\" class=\"data row23 col2\" >0.47</td>\n",
       "      <td id=\"T_1f3fa_row23_col3\" class=\"data row23 col3\" >0.758777</td>\n",
       "      <td id=\"T_1f3fa_row23_col4\" class=\"data row23 col4\" >0.676732</td>\n",
       "      <td id=\"T_1f3fa_row23_col5\" class=\"data row23 col5\" >164387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row24\" class=\"row_heading level0 row24\" >37</th>\n",
       "      <td id=\"T_1f3fa_row24_col0\" class=\"data row24 col0\" >0.040633</td>\n",
       "      <td id=\"T_1f3fa_row24_col1\" class=\"data row24 col1\" >39</td>\n",
       "      <td id=\"T_1f3fa_row24_col2\" class=\"data row24 col2\" >0.55</td>\n",
       "      <td id=\"T_1f3fa_row24_col3\" class=\"data row24 col3\" >0.757869</td>\n",
       "      <td id=\"T_1f3fa_row24_col4\" class=\"data row24 col4\" >0.775824</td>\n",
       "      <td id=\"T_1f3fa_row24_col5\" class=\"data row24 col5\" >164516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row25\" class=\"row_heading level0 row25\" >17</th>\n",
       "      <td id=\"T_1f3fa_row25_col0\" class=\"data row25 col0\" >0.054521</td>\n",
       "      <td id=\"T_1f3fa_row25_col1\" class=\"data row25 col1\" >63</td>\n",
       "      <td id=\"T_1f3fa_row25_col2\" class=\"data row25 col2\" >0.44</td>\n",
       "      <td id=\"T_1f3fa_row25_col3\" class=\"data row25 col3\" >0.751708</td>\n",
       "      <td id=\"T_1f3fa_row25_col4\" class=\"data row25 col4\" >0.718261</td>\n",
       "      <td id=\"T_1f3fa_row25_col5\" class=\"data row25 col5\" >164561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_1f3fa_row26_col0\" class=\"data row26 col0\" >0.018721</td>\n",
       "      <td id=\"T_1f3fa_row26_col1\" class=\"data row26 col1\" >40</td>\n",
       "      <td id=\"T_1f3fa_row26_col2\" class=\"data row26 col2\" >0.57</td>\n",
       "      <td id=\"T_1f3fa_row26_col3\" class=\"data row26 col3\" >0.759645</td>\n",
       "      <td id=\"T_1f3fa_row26_col4\" class=\"data row26 col4\" >0.781133</td>\n",
       "      <td id=\"T_1f3fa_row26_col5\" class=\"data row26 col5\" >164586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row27\" class=\"row_heading level0 row27\" >39</th>\n",
       "      <td id=\"T_1f3fa_row27_col0\" class=\"data row27 col0\" >0.024720</td>\n",
       "      <td id=\"T_1f3fa_row27_col1\" class=\"data row27 col1\" >41</td>\n",
       "      <td id=\"T_1f3fa_row27_col2\" class=\"data row27 col2\" >0.47</td>\n",
       "      <td id=\"T_1f3fa_row27_col3\" class=\"data row27 col3\" >0.759098</td>\n",
       "      <td id=\"T_1f3fa_row27_col4\" class=\"data row27 col4\" >0.674938</td>\n",
       "      <td id=\"T_1f3fa_row27_col5\" class=\"data row27 col5\" >164713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row28\" class=\"row_heading level0 row28\" >34</th>\n",
       "      <td id=\"T_1f3fa_row28_col0\" class=\"data row28 col0\" >0.027329</td>\n",
       "      <td id=\"T_1f3fa_row28_col1\" class=\"data row28 col1\" >48</td>\n",
       "      <td id=\"T_1f3fa_row28_col2\" class=\"data row28 col2\" >0.56</td>\n",
       "      <td id=\"T_1f3fa_row28_col3\" class=\"data row28 col3\" >0.756985</td>\n",
       "      <td id=\"T_1f3fa_row28_col4\" class=\"data row28 col4\" >0.764367</td>\n",
       "      <td id=\"T_1f3fa_row28_col5\" class=\"data row28 col5\" >164715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row29\" class=\"row_heading level0 row29\" >35</th>\n",
       "      <td id=\"T_1f3fa_row29_col0\" class=\"data row29 col0\" >0.010051</td>\n",
       "      <td id=\"T_1f3fa_row29_col1\" class=\"data row29 col1\" >60</td>\n",
       "      <td id=\"T_1f3fa_row29_col2\" class=\"data row29 col2\" >0.48</td>\n",
       "      <td id=\"T_1f3fa_row29_col3\" class=\"data row29 col3\" >0.756118</td>\n",
       "      <td id=\"T_1f3fa_row29_col4\" class=\"data row29 col4\" >0.679437</td>\n",
       "      <td id=\"T_1f3fa_row29_col5\" class=\"data row29 col5\" >165392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row30\" class=\"row_heading level0 row30\" >16</th>\n",
       "      <td id=\"T_1f3fa_row30_col0\" class=\"data row30 col0\" >0.006128</td>\n",
       "      <td id=\"T_1f3fa_row30_col1\" class=\"data row30 col1\" >64</td>\n",
       "      <td id=\"T_1f3fa_row30_col2\" class=\"data row30 col2\" >0.51</td>\n",
       "      <td id=\"T_1f3fa_row30_col3\" class=\"data row30 col3\" >0.753231</td>\n",
       "      <td id=\"T_1f3fa_row30_col4\" class=\"data row30 col4\" >0.710710</td>\n",
       "      <td id=\"T_1f3fa_row30_col5\" class=\"data row30 col5\" >165513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row31\" class=\"row_heading level0 row31\" >11</th>\n",
       "      <td id=\"T_1f3fa_row31_col0\" class=\"data row31 col0\" >0.013720</td>\n",
       "      <td id=\"T_1f3fa_row31_col1\" class=\"data row31 col1\" >57</td>\n",
       "      <td id=\"T_1f3fa_row31_col2\" class=\"data row31 col2\" >0.54</td>\n",
       "      <td id=\"T_1f3fa_row31_col3\" class=\"data row31 col3\" >0.751878</td>\n",
       "      <td id=\"T_1f3fa_row31_col4\" class=\"data row31 col4\" >0.742605</td>\n",
       "      <td id=\"T_1f3fa_row31_col5\" class=\"data row31 col5\" >165866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row32\" class=\"row_heading level0 row32\" >38</th>\n",
       "      <td id=\"T_1f3fa_row32_col0\" class=\"data row32 col0\" >0.062496</td>\n",
       "      <td id=\"T_1f3fa_row32_col1\" class=\"data row32 col1\" >43</td>\n",
       "      <td id=\"T_1f3fa_row32_col2\" class=\"data row32 col2\" >0.49</td>\n",
       "      <td id=\"T_1f3fa_row32_col3\" class=\"data row32 col3\" >0.750664</td>\n",
       "      <td id=\"T_1f3fa_row32_col4\" class=\"data row32 col4\" >0.758564</td>\n",
       "      <td id=\"T_1f3fa_row32_col5\" class=\"data row32 col5\" >166336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row33\" class=\"row_heading level0 row33\" >14</th>\n",
       "      <td id=\"T_1f3fa_row33_col0\" class=\"data row33 col0\" >0.079904</td>\n",
       "      <td id=\"T_1f3fa_row33_col1\" class=\"data row33 col1\" >65</td>\n",
       "      <td id=\"T_1f3fa_row33_col2\" class=\"data row33 col2\" >0.44</td>\n",
       "      <td id=\"T_1f3fa_row33_col3\" class=\"data row33 col3\" >0.748841</td>\n",
       "      <td id=\"T_1f3fa_row33_col4\" class=\"data row33 col4\" >0.727499</td>\n",
       "      <td id=\"T_1f3fa_row33_col5\" class=\"data row33 col5\" >166475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row34\" class=\"row_heading level0 row34\" >45</th>\n",
       "      <td id=\"T_1f3fa_row34_col0\" class=\"data row34 col0\" >0.007265</td>\n",
       "      <td id=\"T_1f3fa_row34_col1\" class=\"data row34 col1\" >37</td>\n",
       "      <td id=\"T_1f3fa_row34_col2\" class=\"data row34 col2\" >0.5</td>\n",
       "      <td id=\"T_1f3fa_row34_col3\" class=\"data row34 col3\" >0.750806</td>\n",
       "      <td id=\"T_1f3fa_row34_col4\" class=\"data row34 col4\" >0.692281</td>\n",
       "      <td id=\"T_1f3fa_row34_col5\" class=\"data row34 col5\" >166738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row35\" class=\"row_heading level0 row35\" >10</th>\n",
       "      <td id=\"T_1f3fa_row35_col0\" class=\"data row35 col0\" >0.011759</td>\n",
       "      <td id=\"T_1f3fa_row35_col1\" class=\"data row35 col1\" >57</td>\n",
       "      <td id=\"T_1f3fa_row35_col2\" class=\"data row35 col2\" >0.54</td>\n",
       "      <td id=\"T_1f3fa_row35_col3\" class=\"data row35 col3\" >0.749294</td>\n",
       "      <td id=\"T_1f3fa_row35_col4\" class=\"data row35 col4\" >0.742384</td>\n",
       "      <td id=\"T_1f3fa_row35_col5\" class=\"data row35 col5\" >166762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row36\" class=\"row_heading level0 row36\" >7</th>\n",
       "      <td id=\"T_1f3fa_row36_col0\" class=\"data row36 col0\" >0.013812</td>\n",
       "      <td id=\"T_1f3fa_row36_col1\" class=\"data row36 col1\" >39</td>\n",
       "      <td id=\"T_1f3fa_row36_col2\" class=\"data row36 col2\" >0.46</td>\n",
       "      <td id=\"T_1f3fa_row36_col3\" class=\"data row36 col3\" >0.755084</td>\n",
       "      <td id=\"T_1f3fa_row36_col4\" class=\"data row36 col4\" >0.648229</td>\n",
       "      <td id=\"T_1f3fa_row36_col5\" class=\"data row36 col5\" >167870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row37\" class=\"row_heading level0 row37\" >4</th>\n",
       "      <td id=\"T_1f3fa_row37_col0\" class=\"data row37 col0\" >0.082019</td>\n",
       "      <td id=\"T_1f3fa_row37_col1\" class=\"data row37 col1\" >44</td>\n",
       "      <td id=\"T_1f3fa_row37_col2\" class=\"data row37 col2\" >0.48</td>\n",
       "      <td id=\"T_1f3fa_row37_col3\" class=\"data row37 col3\" >0.745058</td>\n",
       "      <td id=\"T_1f3fa_row37_col4\" class=\"data row37 col4\" >0.767004</td>\n",
       "      <td id=\"T_1f3fa_row37_col5\" class=\"data row37 col5\" >168711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row38\" class=\"row_heading level0 row38\" >28</th>\n",
       "      <td id=\"T_1f3fa_row38_col0\" class=\"data row38 col0\" >0.005287</td>\n",
       "      <td id=\"T_1f3fa_row38_col1\" class=\"data row38 col1\" >54</td>\n",
       "      <td id=\"T_1f3fa_row38_col2\" class=\"data row38 col2\" >0.52</td>\n",
       "      <td id=\"T_1f3fa_row38_col3\" class=\"data row38 col3\" >0.741841</td>\n",
       "      <td id=\"T_1f3fa_row38_col4\" class=\"data row38 col4\" >0.718118</td>\n",
       "      <td id=\"T_1f3fa_row38_col5\" class=\"data row38 col5\" >168871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row39\" class=\"row_heading level0 row39\" >6</th>\n",
       "      <td id=\"T_1f3fa_row39_col0\" class=\"data row39 col0\" >0.034442</td>\n",
       "      <td id=\"T_1f3fa_row39_col1\" class=\"data row39 col1\" >68</td>\n",
       "      <td id=\"T_1f3fa_row39_col2\" class=\"data row39 col2\" >0.4</td>\n",
       "      <td id=\"T_1f3fa_row39_col3\" class=\"data row39 col3\" >0.759242</td>\n",
       "      <td id=\"T_1f3fa_row39_col4\" class=\"data row39 col4\" >0.616858</td>\n",
       "      <td id=\"T_1f3fa_row39_col5\" class=\"data row39 col5\" >169102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row40\" class=\"row_heading level0 row40\" >44</th>\n",
       "      <td id=\"T_1f3fa_row40_col0\" class=\"data row40 col0\" >0.011904</td>\n",
       "      <td id=\"T_1f3fa_row40_col1\" class=\"data row40 col1\" >53</td>\n",
       "      <td id=\"T_1f3fa_row40_col2\" class=\"data row40 col2\" >0.48</td>\n",
       "      <td id=\"T_1f3fa_row40_col3\" class=\"data row40 col3\" >0.743669</td>\n",
       "      <td id=\"T_1f3fa_row40_col4\" class=\"data row40 col4\" >0.656535</td>\n",
       "      <td id=\"T_1f3fa_row40_col5\" class=\"data row40 col5\" >170574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row41\" class=\"row_heading level0 row41\" >40</th>\n",
       "      <td id=\"T_1f3fa_row41_col0\" class=\"data row41 col0\" >0.090121</td>\n",
       "      <td id=\"T_1f3fa_row41_col1\" class=\"data row41 col1\" >46</td>\n",
       "      <td id=\"T_1f3fa_row41_col2\" class=\"data row41 col2\" >0.53</td>\n",
       "      <td id=\"T_1f3fa_row41_col3\" class=\"data row41 col3\" >0.746144</td>\n",
       "      <td id=\"T_1f3fa_row41_col4\" class=\"data row41 col4\" >0.798869</td>\n",
       "      <td id=\"T_1f3fa_row41_col5\" class=\"data row41 col5\" >171395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row42\" class=\"row_heading level0 row42\" >2</th>\n",
       "      <td id=\"T_1f3fa_row42_col0\" class=\"data row42 col0\" >0.001834</td>\n",
       "      <td id=\"T_1f3fa_row42_col1\" class=\"data row42 col1\" >36</td>\n",
       "      <td id=\"T_1f3fa_row42_col2\" class=\"data row42 col2\" >0.55</td>\n",
       "      <td id=\"T_1f3fa_row42_col3\" class=\"data row42 col3\" >0.733978</td>\n",
       "      <td id=\"T_1f3fa_row42_col4\" class=\"data row42 col4\" >0.778646</td>\n",
       "      <td id=\"T_1f3fa_row42_col5\" class=\"data row42 col5\" >174719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row43\" class=\"row_heading level0 row43\" >0</th>\n",
       "      <td id=\"T_1f3fa_row43_col0\" class=\"data row43 col0\" >0.002545</td>\n",
       "      <td id=\"T_1f3fa_row43_col1\" class=\"data row43 col1\" >60</td>\n",
       "      <td id=\"T_1f3fa_row43_col2\" class=\"data row43 col2\" >0.49</td>\n",
       "      <td id=\"T_1f3fa_row43_col3\" class=\"data row43 col3\" >0.731855</td>\n",
       "      <td id=\"T_1f3fa_row43_col4\" class=\"data row43 col4\" >0.641862</td>\n",
       "      <td id=\"T_1f3fa_row43_col5\" class=\"data row43 col5\" >175370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row44\" class=\"row_heading level0 row44\" >3</th>\n",
       "      <td id=\"T_1f3fa_row44_col0\" class=\"data row44 col0\" >0.002137</td>\n",
       "      <td id=\"T_1f3fa_row44_col1\" class=\"data row44 col1\" >51</td>\n",
       "      <td id=\"T_1f3fa_row44_col2\" class=\"data row44 col2\" >0.49</td>\n",
       "      <td id=\"T_1f3fa_row44_col3\" class=\"data row44 col3\" >0.728964</td>\n",
       "      <td id=\"T_1f3fa_row44_col4\" class=\"data row44 col4\" >0.627879</td>\n",
       "      <td id=\"T_1f3fa_row44_col5\" class=\"data row44 col5\" >177587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row45\" class=\"row_heading level0 row45\" >29</th>\n",
       "      <td id=\"T_1f3fa_row45_col0\" class=\"data row45 col0\" >0.062865</td>\n",
       "      <td id=\"T_1f3fa_row45_col1\" class=\"data row45 col1\" >48</td>\n",
       "      <td id=\"T_1f3fa_row45_col2\" class=\"data row45 col2\" >0.6</td>\n",
       "      <td id=\"T_1f3fa_row45_col3\" class=\"data row45 col3\" >0.746629</td>\n",
       "      <td id=\"T_1f3fa_row45_col4\" class=\"data row45 col4\" >0.845021</td>\n",
       "      <td id=\"T_1f3fa_row45_col5\" class=\"data row45 col5\" >179581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row46\" class=\"row_heading level0 row46\" >20</th>\n",
       "      <td id=\"T_1f3fa_row46_col0\" class=\"data row46 col0\" >0.001068</td>\n",
       "      <td id=\"T_1f3fa_row46_col1\" class=\"data row46 col1\" >31</td>\n",
       "      <td id=\"T_1f3fa_row46_col2\" class=\"data row46 col2\" >0.58</td>\n",
       "      <td id=\"T_1f3fa_row46_col3\" class=\"data row46 col3\" >0.734246</td>\n",
       "      <td id=\"T_1f3fa_row46_col4\" class=\"data row46 col4\" >0.818252</td>\n",
       "      <td id=\"T_1f3fa_row46_col5\" class=\"data row46 col5\" >181784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row47\" class=\"row_heading level0 row47\" >8</th>\n",
       "      <td id=\"T_1f3fa_row47_col0\" class=\"data row47 col0\" >0.002223</td>\n",
       "      <td id=\"T_1f3fa_row47_col1\" class=\"data row47 col1\" >48</td>\n",
       "      <td id=\"T_1f3fa_row47_col2\" class=\"data row47 col2\" >0.47</td>\n",
       "      <td id=\"T_1f3fa_row47_col3\" class=\"data row47 col3\" >0.727523</td>\n",
       "      <td id=\"T_1f3fa_row47_col4\" class=\"data row47 col4\" >0.489311</td>\n",
       "      <td id=\"T_1f3fa_row47_col5\" class=\"data row47 col5\" >194203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row48\" class=\"row_heading level0 row48\" >9</th>\n",
       "      <td id=\"T_1f3fa_row48_col0\" class=\"data row48 col0\" >0.003738</td>\n",
       "      <td id=\"T_1f3fa_row48_col1\" class=\"data row48 col1\" >70</td>\n",
       "      <td id=\"T_1f3fa_row48_col2\" class=\"data row48 col2\" >0.59</td>\n",
       "      <td id=\"T_1f3fa_row48_col3\" class=\"data row48 col3\" >0.732995</td>\n",
       "      <td id=\"T_1f3fa_row48_col4\" class=\"data row48 col4\" >0.895895</td>\n",
       "      <td id=\"T_1f3fa_row48_col5\" class=\"data row48 col5\" >214117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f3fa_level0_row49\" class=\"row_heading level0 row49\" >5</th>\n",
       "      <td id=\"T_1f3fa_row49_col0\" class=\"data row49 col0\" >0.002983</td>\n",
       "      <td id=\"T_1f3fa_row49_col1\" class=\"data row49 col1\" >43</td>\n",
       "      <td id=\"T_1f3fa_row49_col2\" class=\"data row49 col2\" >0.41</td>\n",
       "      <td id=\"T_1f3fa_row49_col3\" class=\"data row49 col3\" >0.728763</td>\n",
       "      <td id=\"T_1f3fa_row49_col4\" class=\"data row49 col4\" >0.154101</td>\n",
       "      <td id=\"T_1f3fa_row49_col5\" class=\"data row49 col5\" >263205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17b573a24c0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_LGM = (styled_df_LGM.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score'])\n",
    "                          .format({'Threshold': \"{:g}\"}))\n",
    "\n",
    "styled_df_LGM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3fc0d-9410-47b7-b089-c245c5edc655",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ffdd9d8-82e4-40d8-a260-e639a74c510b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.03202900440110068,\n",
       " 'num_leaves': 50,\n",
       " 'n_estimators': 500,\n",
       " 'threshold': 0.52}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0dea5b9d-1170-4bfb-b8e7-94f33fca5319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12466\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 241\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12478\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 242\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12446\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12484\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 242\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12454\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 244\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supposition que df_classification_imputed est déjà défini\n",
    "nb_0 = len(application_train_petit_clean[application_train_petit_clean[\"TARGET\"] == 0])\n",
    "nb_1 = len(application_train_petit_clean[application_train_petit_clean[\"TARGET\"] == 1])\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "X = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_petit_clean[\"TARGET\"]\n",
    "\n",
    "# Stockage des noms des colonnes pour utilisation ultérieure\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Instanciation du modèle\n",
    "clf = LGBMClassifier(**study.best_params, class_weight=class_weights, verbose=1)\n",
    "\n",
    "# Validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    clf.fit(X[train], y.iloc[train])\n",
    "    feature_importances += clf.feature_importances_\n",
    "\n",
    "# Moyenne des importances de caractéristiques sur les plis\n",
    "feature_importances /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0d49cf47-5511-405a-997f-724c87b855b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "df_feature_importances_lightGBM = df_feature_importances_lightGBM.sort_values(by='importance', ascending=False)\n",
    "df_feature_importances_lightGBM[\"importance\"] = (df_feature_importances_lightGBM[\"importance\"]/ df_feature_importances_lightGBM[\"importance\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b241321e-1feb-43c3-90d1-259ec20ef931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM.to_csv('df_feature_importances_lightGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9be08a5-0c0a-4217-af14-5c390e335edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EXT_SOURCE_3</td>\n",
       "      <td>4.930612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>EXT_SOURCE_2</td>\n",
       "      <td>4.847347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_BIRTH</td>\n",
       "      <td>4.595918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>4.183673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DAYS_ID_PUBLISH</td>\n",
       "      <td>4.118367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>CODE_GENDER_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>ORGANIZATION_TYPE_Industry: type 8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>FLAG_DOCUMENT_20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>ORGANIZATION_TYPE_nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>NAME_INCOME_TYPE_Student</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                feature  importance\n",
       "29                         EXT_SOURCE_3    4.930612\n",
       "28                         EXT_SOURCE_2    4.847347\n",
       "6                            DAYS_BIRTH    4.595918\n",
       "8                     DAYS_REGISTRATION    4.183673\n",
       "9                       DAYS_ID_PUBLISH    4.118367\n",
       "..                                  ...         ...\n",
       "112                     CODE_GENDER_nan    0.000000\n",
       "207  ORGANIZATION_TYPE_Industry: type 8    0.000000\n",
       "96                     FLAG_DOCUMENT_20    0.000000\n",
       "240               ORGANIZATION_TYPE_nan    0.000000\n",
       "132            NAME_INCOME_TYPE_Student    0.000000\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importances_lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cb5b9991-08fe-48ee-9f57-7d61312550f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pourcentage_importance_totale = 95\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "df_sorted = df_feature_importances_lightGBM.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "df_sorted[\"cumulative_importance\"] = df_sorted[\"importance\"].cumsum()\n",
    "\n",
    "# Find the number of features needed to reach 99% of the total importance\n",
    "num_features = df_sorted[df_sorted[\"cumulative_importance\"] <= df_sorted[\"importance\"].sum() * Pourcentage_importance_totale/100].shape[0]\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72d45bf9-a274-4f86-be33-01f740f32d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les colonnes 'feature' et 'importance' de votre dataframe\n",
    "features = df_feature_importances_lightGBM['feature'].to_numpy()\n",
    "importances = df_feature_importances_lightGBM['importance'].to_numpy()\n",
    "\n",
    "# Création d'un masque pour les importances supérieures à 0\n",
    "mask = importances > 0\n",
    "\n",
    "# Filtrage des features et des importances en utilisant le masque\n",
    "filtered_feature_names = features[mask]\n",
    "filtered_feature_importances = importances[mask]\n",
    "\n",
    "# Supposons que vous vouliez afficher les 10 principales caractéristiques\n",
    "n_features_to_display = 20\n",
    "\n",
    "# Tri des indices filtrés par importance\n",
    "sorted_idx = filtered_feature_importances.argsort()\n",
    "\n",
    "# Limiter aux n principales caractéristiques\n",
    "top_n_idx = sorted_idx[-n_features_to_display:]\n",
    "\n",
    "# Affichage de l'importance des caractéristiques filtrées pour les n principales\n",
    "plt.figure(figsize=(10, len(top_n_idx) * 0.4))\n",
    "bars = plt.barh(filtered_feature_names[top_n_idx], filtered_feature_importances[top_n_idx])\n",
    "\n",
    "# Ajuster les limites de l'axe des y pour réduire l'espace autour des barres\n",
    "plt.ylim(-0.5, len(top_n_idx)-0.5)\n",
    "\n",
    "plt.xlabel(\"Importance des caractéristiques\")\n",
    "plt.title(\"Importance des caractéristiques avec le lightGBM\")\n",
    "\n",
    "# Ajouter les valeurs à côté des barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    \n",
    "    # Déterminez le formatage en fonction de la magnitude\n",
    "    if width < 0.01:\n",
    "        fmt = '{:.4f}'\n",
    "    elif width < 0.1:\n",
    "        fmt = '{:.3f}'\n",
    "    else:\n",
    "        fmt = '{:.2f}'\n",
    "    \n",
    "    plt.text(width + 0.01 * width,  # Ajouter un padding proportionnel à la valeur\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             fmt.format(width),\n",
    "             va='center', ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fd0ec-435a-421d-b6fc-52f15831cf9f",
   "metadata": {},
   "source": [
    "### 2.3.3 Dataset petit encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a788350a-46bd-4c87-89b4-c6fc0633881f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2825639940.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    application_train_petit_encoded = pd.read_csv('C:\\Users\\anais\\Documents\\Data Science\\Projets\\Projet 7\\Données\\application_test_petit_encoded.csv')\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "### Importantion des données : \n",
    "application_train_petit_encoded = pd.read_csv('C:\\Users\\anais\\Documents\\Data Science\\Projets\\Projet 7\\Données\\application_test_petit_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "681a21f7-3b03-4d21-9257-dcf502b27d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 2296.24 secondes\n",
      "{'lr': 0.02127915254605793, 'num_leaves': 58, 'n_estimators': 481, 'threshold': 0.52}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('LightGBM')\n",
    "\n",
    "# Supposition : vous avez déjà votre dataframe 'df_classification' chargé\n",
    "nb_0 = (application_train_petit_encoded['TARGET'] == 0).sum()\n",
    "nb_1 = (application_train_petit_encoded['TARGET'] == 1).sum()\n",
    "\n",
    "X = application_train_petit_encoded.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_petit_encoded[\"TARGET\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "results = []\n",
    "\n",
    "total_combinations = 50  # définissez cela avant d'appeler study.optimize()\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LGBMClassifier(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators, class_weight=class_weights, verbose=-1)\n",
    "    \n",
    "    # Enregistrement du temps de début pour le fit\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    # Calculer le temps de fit\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    # Enregistrement du temps de début pour la prédiction\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    # Calculer le temps de prédiction\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "    \n",
    "    results.append({\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Learning Rate\", lr)\n",
    "        mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "        mlflow.log_param(\"N Estimators\", n_estimators) \n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        \n",
    "        # Enregistrer les temps dans mlflow\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "# 6. Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeeaf55f-8ec2-4e30-bf08-5c417a048c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM = results_df.sort_values(by='Business Score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab6701b0-74f1-4e8f-9870-ab4a81de9a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "styled_df_LGM.to_csv('results_LGM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1965c6f-51f9-473e-8e81-e265b2f6dcaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0440f_row0_col5, #T_0440f_row3_col3, #T_0440f_row40_col4 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0440f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0440f_level0_col0\" class=\"col_heading level0 col0\" >Learning Rate</th>\n",
       "      <th id=\"T_0440f_level0_col1\" class=\"col_heading level0 col1\" >Num Leaves</th>\n",
       "      <th id=\"T_0440f_level0_col2\" class=\"col_heading level0 col2\" >Threshold</th>\n",
       "      <th id=\"T_0440f_level0_col3\" class=\"col_heading level0 col3\" >AUC</th>\n",
       "      <th id=\"T_0440f_level0_col4\" class=\"col_heading level0 col4\" >Accuracy</th>\n",
       "      <th id=\"T_0440f_level0_col5\" class=\"col_heading level0 col5\" >Business Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row0\" class=\"row_heading level0 row0\" >45</th>\n",
       "      <td id=\"T_0440f_row0_col0\" class=\"data row0 col0\" >0.021279</td>\n",
       "      <td id=\"T_0440f_row0_col1\" class=\"data row0 col1\" >58</td>\n",
       "      <td id=\"T_0440f_row0_col2\" class=\"data row0 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row0_col3\" class=\"data row0 col3\" >0.702497</td>\n",
       "      <td id=\"T_0440f_row0_col4\" class=\"data row0 col4\" >0.720598</td>\n",
       "      <td id=\"T_0440f_row0_col5\" class=\"data row0 col5\" >184849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row1\" class=\"row_heading level0 row1\" >44</th>\n",
       "      <td id=\"T_0440f_row1_col0\" class=\"data row1 col0\" >0.023916</td>\n",
       "      <td id=\"T_0440f_row1_col1\" class=\"data row1 col1\" >57</td>\n",
       "      <td id=\"T_0440f_row1_col2\" class=\"data row1 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row1_col3\" class=\"data row1 col3\" >0.702277</td>\n",
       "      <td id=\"T_0440f_row1_col4\" class=\"data row1 col4\" >0.714661</td>\n",
       "      <td id=\"T_0440f_row1_col5\" class=\"data row1 col5\" >184918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row2\" class=\"row_heading level0 row2\" >23</th>\n",
       "      <td id=\"T_0440f_row2_col0\" class=\"data row2 col0\" >0.021340</td>\n",
       "      <td id=\"T_0440f_row2_col1\" class=\"data row2 col1\" >49</td>\n",
       "      <td id=\"T_0440f_row2_col2\" class=\"data row2 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row2_col3\" class=\"data row2 col3\" >0.702823</td>\n",
       "      <td id=\"T_0440f_row2_col4\" class=\"data row2 col4\" >0.720266</td>\n",
       "      <td id=\"T_0440f_row2_col5\" class=\"data row2 col5\" >185167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row3\" class=\"row_heading level0 row3\" >21</th>\n",
       "      <td id=\"T_0440f_row3_col0\" class=\"data row3 col0\" >0.012676</td>\n",
       "      <td id=\"T_0440f_row3_col1\" class=\"data row3 col1\" >48</td>\n",
       "      <td id=\"T_0440f_row3_col2\" class=\"data row3 col2\" >0.5</td>\n",
       "      <td id=\"T_0440f_row3_col3\" class=\"data row3 col3\" >0.702986</td>\n",
       "      <td id=\"T_0440f_row3_col4\" class=\"data row3 col4\" >0.688762</td>\n",
       "      <td id=\"T_0440f_row3_col5\" class=\"data row3 col5\" >185306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row4\" class=\"row_heading level0 row4\" >48</th>\n",
       "      <td id=\"T_0440f_row4_col0\" class=\"data row4 col0\" >0.036569</td>\n",
       "      <td id=\"T_0440f_row4_col1\" class=\"data row4 col1\" >57</td>\n",
       "      <td id=\"T_0440f_row4_col2\" class=\"data row4 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row4_col3\" class=\"data row4 col3\" >0.701693</td>\n",
       "      <td id=\"T_0440f_row4_col4\" class=\"data row4 col4\" >0.728016</td>\n",
       "      <td id=\"T_0440f_row4_col5\" class=\"data row4 col5\" >185495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row5\" class=\"row_heading level0 row5\" >11</th>\n",
       "      <td id=\"T_0440f_row5_col0\" class=\"data row5 col0\" >0.009319</td>\n",
       "      <td id=\"T_0440f_row5_col1\" class=\"data row5 col1\" >39</td>\n",
       "      <td id=\"T_0440f_row5_col2\" class=\"data row5 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row5_col3\" class=\"data row5 col3\" >0.702562</td>\n",
       "      <td id=\"T_0440f_row5_col4\" class=\"data row5 col4\" >0.708585</td>\n",
       "      <td id=\"T_0440f_row5_col5\" class=\"data row5 col5\" >185534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row6\" class=\"row_heading level0 row6\" >14</th>\n",
       "      <td id=\"T_0440f_row6_col0\" class=\"data row6 col0\" >0.011339</td>\n",
       "      <td id=\"T_0440f_row6_col1\" class=\"data row6 col1\" >46</td>\n",
       "      <td id=\"T_0440f_row6_col2\" class=\"data row6 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row6_col3\" class=\"data row6 col3\" >0.702866</td>\n",
       "      <td id=\"T_0440f_row6_col4\" class=\"data row6 col4\" >0.701889</td>\n",
       "      <td id=\"T_0440f_row6_col5\" class=\"data row6 col5\" >185548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row7\" class=\"row_heading level0 row7\" >15</th>\n",
       "      <td id=\"T_0440f_row7_col0\" class=\"data row7 col0\" >0.011228</td>\n",
       "      <td id=\"T_0440f_row7_col1\" class=\"data row7 col1\" >48</td>\n",
       "      <td id=\"T_0440f_row7_col2\" class=\"data row7 col2\" >0.5</td>\n",
       "      <td id=\"T_0440f_row7_col3\" class=\"data row7 col3\" >0.702802</td>\n",
       "      <td id=\"T_0440f_row7_col4\" class=\"data row7 col4\" >0.688157</td>\n",
       "      <td id=\"T_0440f_row7_col5\" class=\"data row7 col5\" >185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row8\" class=\"row_heading level0 row8\" >41</th>\n",
       "      <td id=\"T_0440f_row8_col0\" class=\"data row8 col0\" >0.010757</td>\n",
       "      <td id=\"T_0440f_row8_col1\" class=\"data row8 col1\" >49</td>\n",
       "      <td id=\"T_0440f_row8_col2\" class=\"data row8 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row8_col3\" class=\"data row8 col3\" >0.702637</td>\n",
       "      <td id=\"T_0440f_row8_col4\" class=\"data row8 col4\" >0.702065</td>\n",
       "      <td id=\"T_0440f_row8_col5\" class=\"data row8 col5\" >185692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row9\" class=\"row_heading level0 row9\" >37</th>\n",
       "      <td id=\"T_0440f_row9_col0\" class=\"data row9 col0\" >0.020603</td>\n",
       "      <td id=\"T_0440f_row9_col1\" class=\"data row9 col1\" >41</td>\n",
       "      <td id=\"T_0440f_row9_col2\" class=\"data row9 col2\" >0.5</td>\n",
       "      <td id=\"T_0440f_row9_col3\" class=\"data row9 col3\" >0.702561</td>\n",
       "      <td id=\"T_0440f_row9_col4\" class=\"data row9 col4\" >0.688671</td>\n",
       "      <td id=\"T_0440f_row9_col5\" class=\"data row9 col5\" >185694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row10\" class=\"row_heading level0 row10\" >42</th>\n",
       "      <td id=\"T_0440f_row10_col0\" class=\"data row10 col0\" >0.011779</td>\n",
       "      <td id=\"T_0440f_row10_col1\" class=\"data row10 col1\" >48</td>\n",
       "      <td id=\"T_0440f_row10_col2\" class=\"data row10 col2\" >0.5</td>\n",
       "      <td id=\"T_0440f_row10_col3\" class=\"data row10 col3\" >0.702964</td>\n",
       "      <td id=\"T_0440f_row10_col4\" class=\"data row10 col4\" >0.686145</td>\n",
       "      <td id=\"T_0440f_row10_col5\" class=\"data row10 col5\" >185696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row11\" class=\"row_heading level0 row11\" >31</th>\n",
       "      <td id=\"T_0440f_row11_col0\" class=\"data row11 col0\" >0.009929</td>\n",
       "      <td id=\"T_0440f_row11_col1\" class=\"data row11 col1\" >47</td>\n",
       "      <td id=\"T_0440f_row11_col2\" class=\"data row11 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row11_col3\" class=\"data row11 col3\" >0.702661</td>\n",
       "      <td id=\"T_0440f_row11_col4\" class=\"data row11 col4\" >0.698966</td>\n",
       "      <td id=\"T_0440f_row11_col5\" class=\"data row11 col5\" >185717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row12\" class=\"row_heading level0 row12\" >27</th>\n",
       "      <td id=\"T_0440f_row12_col0\" class=\"data row12 col0\" >0.036436</td>\n",
       "      <td id=\"T_0440f_row12_col1\" class=\"data row12 col1\" >54</td>\n",
       "      <td id=\"T_0440f_row12_col2\" class=\"data row12 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row12_col3\" class=\"data row12 col3\" >0.701542</td>\n",
       "      <td id=\"T_0440f_row12_col4\" class=\"data row12 col4\" >0.728921</td>\n",
       "      <td id=\"T_0440f_row12_col5\" class=\"data row12 col5\" >185802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row13\" class=\"row_heading level0 row13\" >28</th>\n",
       "      <td id=\"T_0440f_row13_col0\" class=\"data row13 col0\" >0.024903</td>\n",
       "      <td id=\"T_0440f_row13_col1\" class=\"data row13 col1\" >43</td>\n",
       "      <td id=\"T_0440f_row13_col2\" class=\"data row13 col2\" >0.49</td>\n",
       "      <td id=\"T_0440f_row13_col3\" class=\"data row13 col3\" >0.702181</td>\n",
       "      <td id=\"T_0440f_row13_col4\" class=\"data row13 col4\" >0.679268</td>\n",
       "      <td id=\"T_0440f_row13_col5\" class=\"data row13 col5\" >185820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row14\" class=\"row_heading level0 row14\" >39</th>\n",
       "      <td id=\"T_0440f_row14_col0\" class=\"data row14 col0\" >0.014654</td>\n",
       "      <td id=\"T_0440f_row14_col1\" class=\"data row14 col1\" >46</td>\n",
       "      <td id=\"T_0440f_row14_col2\" class=\"data row14 col2\" >0.48</td>\n",
       "      <td id=\"T_0440f_row14_col3\" class=\"data row14 col3\" >0.702715</td>\n",
       "      <td id=\"T_0440f_row14_col4\" class=\"data row14 col4\" >0.660223</td>\n",
       "      <td id=\"T_0440f_row14_col5\" class=\"data row14 col5\" >185848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row15\" class=\"row_heading level0 row15\" >22</th>\n",
       "      <td id=\"T_0440f_row15_col0\" class=\"data row15 col0\" >0.011849</td>\n",
       "      <td id=\"T_0440f_row15_col1\" class=\"data row15 col1\" >44</td>\n",
       "      <td id=\"T_0440f_row15_col2\" class=\"data row15 col2\" >0.49</td>\n",
       "      <td id=\"T_0440f_row15_col3\" class=\"data row15 col3\" >0.702846</td>\n",
       "      <td id=\"T_0440f_row15_col4\" class=\"data row15 col4\" >0.666651</td>\n",
       "      <td id=\"T_0440f_row15_col5\" class=\"data row15 col5\" >185853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row16\" class=\"row_heading level0 row16\" >43</th>\n",
       "      <td id=\"T_0440f_row16_col0\" class=\"data row16 col0\" >0.009258</td>\n",
       "      <td id=\"T_0440f_row16_col1\" class=\"data row16 col1\" >39</td>\n",
       "      <td id=\"T_0440f_row16_col2\" class=\"data row16 col2\" >0.53</td>\n",
       "      <td id=\"T_0440f_row16_col3\" class=\"data row16 col3\" >0.701461</td>\n",
       "      <td id=\"T_0440f_row16_col4\" class=\"data row16 col4\" >0.720139</td>\n",
       "      <td id=\"T_0440f_row16_col5\" class=\"data row16 col5\" >185863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row17\" class=\"row_heading level0 row17\" >35</th>\n",
       "      <td id=\"T_0440f_row17_col0\" class=\"data row17 col0\" >0.031438</td>\n",
       "      <td id=\"T_0440f_row17_col1\" class=\"data row17 col1\" >46</td>\n",
       "      <td id=\"T_0440f_row17_col2\" class=\"data row17 col2\" >0.49</td>\n",
       "      <td id=\"T_0440f_row17_col3\" class=\"data row17 col3\" >0.698603</td>\n",
       "      <td id=\"T_0440f_row17_col4\" class=\"data row17 col4\" >0.710108</td>\n",
       "      <td id=\"T_0440f_row17_col5\" class=\"data row17 col5\" >186038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row18\" class=\"row_heading level0 row18\" >8</th>\n",
       "      <td id=\"T_0440f_row18_col0\" class=\"data row18 col0\" >0.056061</td>\n",
       "      <td id=\"T_0440f_row18_col1\" class=\"data row18 col1\" >36</td>\n",
       "      <td id=\"T_0440f_row18_col2\" class=\"data row18 col2\" >0.54</td>\n",
       "      <td id=\"T_0440f_row18_col3\" class=\"data row18 col3\" >0.700615</td>\n",
       "      <td id=\"T_0440f_row18_col4\" class=\"data row18 col4\" >0.753167</td>\n",
       "      <td id=\"T_0440f_row18_col5\" class=\"data row18 col5\" >186273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row19\" class=\"row_heading level0 row19\" >24</th>\n",
       "      <td id=\"T_0440f_row19_col0\" class=\"data row19 col0\" >0.025814</td>\n",
       "      <td id=\"T_0440f_row19_col1\" class=\"data row19 col1\" >56</td>\n",
       "      <td id=\"T_0440f_row19_col2\" class=\"data row19 col2\" >0.53</td>\n",
       "      <td id=\"T_0440f_row19_col3\" class=\"data row19 col3\" >0.701615</td>\n",
       "      <td id=\"T_0440f_row19_col4\" class=\"data row19 col4\" >0.744548</td>\n",
       "      <td id=\"T_0440f_row19_col5\" class=\"data row19 col5\" >186365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row20\" class=\"row_heading level0 row20\" >46</th>\n",
       "      <td id=\"T_0440f_row20_col0\" class=\"data row20 col0\" >0.021984</td>\n",
       "      <td id=\"T_0440f_row20_col1\" class=\"data row20 col1\" >58</td>\n",
       "      <td id=\"T_0440f_row20_col2\" class=\"data row20 col2\" >0.55</td>\n",
       "      <td id=\"T_0440f_row20_col3\" class=\"data row20 col3\" >0.702538</td>\n",
       "      <td id=\"T_0440f_row20_col4\" class=\"data row20 col4\" >0.760168</td>\n",
       "      <td id=\"T_0440f_row20_col5\" class=\"data row20 col5\" >186417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row21\" class=\"row_heading level0 row21\" >49</th>\n",
       "      <td id=\"T_0440f_row21_col0\" class=\"data row21 col0\" >0.034059</td>\n",
       "      <td id=\"T_0440f_row21_col1\" class=\"data row21 col1\" >58</td>\n",
       "      <td id=\"T_0440f_row21_col2\" class=\"data row21 col2\" >0.47</td>\n",
       "      <td id=\"T_0440f_row21_col3\" class=\"data row21 col3\" >0.701835</td>\n",
       "      <td id=\"T_0440f_row21_col4\" class=\"data row21 col4\" >0.654198</td>\n",
       "      <td id=\"T_0440f_row21_col5\" class=\"data row21 col5\" >186457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row22\" class=\"row_heading level0 row22\" >26</th>\n",
       "      <td id=\"T_0440f_row22_col0\" class=\"data row22 col0\" >0.006997</td>\n",
       "      <td id=\"T_0440f_row22_col1\" class=\"data row22 col1\" >62</td>\n",
       "      <td id=\"T_0440f_row22_col2\" class=\"data row22 col2\" >0.55</td>\n",
       "      <td id=\"T_0440f_row22_col3\" class=\"data row22 col3\" >0.701307</td>\n",
       "      <td id=\"T_0440f_row22_col4\" class=\"data row22 col4\" >0.752793</td>\n",
       "      <td id=\"T_0440f_row22_col5\" class=\"data row22 col5\" >186604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row23\" class=\"row_heading level0 row23\" >3</th>\n",
       "      <td id=\"T_0440f_row23_col0\" class=\"data row23 col0\" >0.005881</td>\n",
       "      <td id=\"T_0440f_row23_col1\" class=\"data row23 col1\" >40</td>\n",
       "      <td id=\"T_0440f_row23_col2\" class=\"data row23 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row23_col3\" class=\"data row23 col3\" >0.699302</td>\n",
       "      <td id=\"T_0440f_row23_col4\" class=\"data row23 col4\" >0.685331</td>\n",
       "      <td id=\"T_0440f_row23_col5\" class=\"data row23 col5\" >186882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row24\" class=\"row_heading level0 row24\" >32</th>\n",
       "      <td id=\"T_0440f_row24_col0\" class=\"data row24 col0\" >0.005477</td>\n",
       "      <td id=\"T_0440f_row24_col1\" class=\"data row24 col1\" >38</td>\n",
       "      <td id=\"T_0440f_row24_col2\" class=\"data row24 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row24_col3\" class=\"data row24 col3\" >0.698239</td>\n",
       "      <td id=\"T_0440f_row24_col4\" class=\"data row24 col4\" >0.700633</td>\n",
       "      <td id=\"T_0440f_row24_col5\" class=\"data row24 col5\" >186915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row25\" class=\"row_heading level0 row25\" >12</th>\n",
       "      <td id=\"T_0440f_row25_col0\" class=\"data row25 col0\" >0.014982</td>\n",
       "      <td id=\"T_0440f_row25_col1\" class=\"data row25 col1\" >40</td>\n",
       "      <td id=\"T_0440f_row25_col2\" class=\"data row25 col2\" >0.56</td>\n",
       "      <td id=\"T_0440f_row25_col3\" class=\"data row25 col3\" >0.702062</td>\n",
       "      <td id=\"T_0440f_row25_col4\" class=\"data row25 col4\" >0.762720</td>\n",
       "      <td id=\"T_0440f_row25_col5\" class=\"data row25 col5\" >187037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row26\" class=\"row_heading level0 row26\" >38</th>\n",
       "      <td id=\"T_0440f_row26_col0\" class=\"data row26 col0\" >0.046022</td>\n",
       "      <td id=\"T_0440f_row26_col1\" class=\"data row26 col1\" >33</td>\n",
       "      <td id=\"T_0440f_row26_col2\" class=\"data row26 col2\" >0.53</td>\n",
       "      <td id=\"T_0440f_row26_col3\" class=\"data row26 col3\" >0.699012</td>\n",
       "      <td id=\"T_0440f_row26_col4\" class=\"data row26 col4\" >0.749769</td>\n",
       "      <td id=\"T_0440f_row26_col5\" class=\"data row26 col5\" >187038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row27\" class=\"row_heading level0 row27\" >4</th>\n",
       "      <td id=\"T_0440f_row27_col0\" class=\"data row27 col0\" >0.048103</td>\n",
       "      <td id=\"T_0440f_row27_col1\" class=\"data row27 col1\" >47</td>\n",
       "      <td id=\"T_0440f_row27_col2\" class=\"data row27 col2\" >0.48</td>\n",
       "      <td id=\"T_0440f_row27_col3\" class=\"data row27 col3\" >0.697978</td>\n",
       "      <td id=\"T_0440f_row27_col4\" class=\"data row27 col4\" >0.694188</td>\n",
       "      <td id=\"T_0440f_row27_col5\" class=\"data row27 col5\" >187104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row28\" class=\"row_heading level0 row28\" >20</th>\n",
       "      <td id=\"T_0440f_row28_col0\" class=\"data row28 col0\" >0.016040</td>\n",
       "      <td id=\"T_0440f_row28_col1\" class=\"data row28 col1\" >36</td>\n",
       "      <td id=\"T_0440f_row28_col2\" class=\"data row28 col2\" >0.57</td>\n",
       "      <td id=\"T_0440f_row28_col3\" class=\"data row28 col3\" >0.702681</td>\n",
       "      <td id=\"T_0440f_row28_col4\" class=\"data row28 col4\" >0.776947</td>\n",
       "      <td id=\"T_0440f_row28_col5\" class=\"data row28 col5\" >187949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row29\" class=\"row_heading level0 row29\" >34</th>\n",
       "      <td id=\"T_0440f_row29_col0\" class=\"data row29 col0\" >0.008377</td>\n",
       "      <td id=\"T_0440f_row29_col1\" class=\"data row29 col1\" >43</td>\n",
       "      <td id=\"T_0440f_row29_col2\" class=\"data row29 col2\" >0.47</td>\n",
       "      <td id=\"T_0440f_row29_col3\" class=\"data row29 col3\" >0.702028</td>\n",
       "      <td id=\"T_0440f_row29_col4\" class=\"data row29 col4\" >0.625168</td>\n",
       "      <td id=\"T_0440f_row29_col5\" class=\"data row29 col5\" >188293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row30\" class=\"row_heading level0 row30\" >25</th>\n",
       "      <td id=\"T_0440f_row30_col0\" class=\"data row30 col0\" >0.018459</td>\n",
       "      <td id=\"T_0440f_row30_col1\" class=\"data row30 col1\" >50</td>\n",
       "      <td id=\"T_0440f_row30_col2\" class=\"data row30 col2\" >0.46</td>\n",
       "      <td id=\"T_0440f_row30_col3\" class=\"data row30 col3\" >0.702948</td>\n",
       "      <td id=\"T_0440f_row30_col4\" class=\"data row30 col4\" >0.619791</td>\n",
       "      <td id=\"T_0440f_row30_col5\" class=\"data row30 col5\" >188532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row31\" class=\"row_heading level0 row31\" >36</th>\n",
       "      <td id=\"T_0440f_row31_col0\" class=\"data row31 col0\" >0.006149</td>\n",
       "      <td id=\"T_0440f_row31_col1\" class=\"data row31 col1\" >51</td>\n",
       "      <td id=\"T_0440f_row31_col2\" class=\"data row31 col2\" >0.54</td>\n",
       "      <td id=\"T_0440f_row31_col3\" class=\"data row31 col3\" >0.696089</td>\n",
       "      <td id=\"T_0440f_row31_col4\" class=\"data row31 col4\" >0.734705</td>\n",
       "      <td id=\"T_0440f_row31_col5\" class=\"data row31 col5\" >188561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row32\" class=\"row_heading level0 row32\" >18</th>\n",
       "      <td id=\"T_0440f_row32_col0\" class=\"data row32 col0\" >0.003032</td>\n",
       "      <td id=\"T_0440f_row32_col1\" class=\"data row32 col1\" >44</td>\n",
       "      <td id=\"T_0440f_row32_col2\" class=\"data row32 col2\" >0.52</td>\n",
       "      <td id=\"T_0440f_row32_col3\" class=\"data row32 col3\" >0.693229</td>\n",
       "      <td id=\"T_0440f_row32_col4\" class=\"data row32 col4\" >0.699175</td>\n",
       "      <td id=\"T_0440f_row32_col5\" class=\"data row32 col5\" >188722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row33\" class=\"row_heading level0 row33\" >47</th>\n",
       "      <td id=\"T_0440f_row33_col0\" class=\"data row33 col0\" >0.061489</td>\n",
       "      <td id=\"T_0440f_row33_col1\" class=\"data row33 col1\" >64</td>\n",
       "      <td id=\"T_0440f_row33_col2\" class=\"data row33 col2\" >0.54</td>\n",
       "      <td id=\"T_0440f_row33_col3\" class=\"data row33 col3\" >0.696833</td>\n",
       "      <td id=\"T_0440f_row33_col4\" class=\"data row33 col4\" >0.774077</td>\n",
       "      <td id=\"T_0440f_row33_col5\" class=\"data row33 col5\" >189317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row34\" class=\"row_heading level0 row34\" >33</th>\n",
       "      <td id=\"T_0440f_row34_col0\" class=\"data row34 col0\" >0.003411</td>\n",
       "      <td id=\"T_0440f_row34_col1\" class=\"data row34 col1\" >52</td>\n",
       "      <td id=\"T_0440f_row34_col2\" class=\"data row34 col2\" >0.51</td>\n",
       "      <td id=\"T_0440f_row34_col3\" class=\"data row34 col3\" >0.691665</td>\n",
       "      <td id=\"T_0440f_row34_col4\" class=\"data row34 col4\" >0.680299</td>\n",
       "      <td id=\"T_0440f_row34_col5\" class=\"data row34 col5\" >189355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row35\" class=\"row_heading level0 row35\" >13</th>\n",
       "      <td id=\"T_0440f_row35_col0\" class=\"data row35 col0\" >0.083685</td>\n",
       "      <td id=\"T_0440f_row35_col1\" class=\"data row35 col1\" >36</td>\n",
       "      <td id=\"T_0440f_row35_col2\" class=\"data row35 col2\" >0.56</td>\n",
       "      <td id=\"T_0440f_row35_col3\" class=\"data row35 col3\" >0.697155</td>\n",
       "      <td id=\"T_0440f_row35_col4\" class=\"data row35 col4\" >0.784531</td>\n",
       "      <td id=\"T_0440f_row35_col5\" class=\"data row35 col5\" >190731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row36\" class=\"row_heading level0 row36\" >17</th>\n",
       "      <td id=\"T_0440f_row36_col0\" class=\"data row36 col0\" >0.007640</td>\n",
       "      <td id=\"T_0440f_row36_col1\" class=\"data row36 col1\" >60</td>\n",
       "      <td id=\"T_0440f_row36_col2\" class=\"data row36 col2\" >0.5</td>\n",
       "      <td id=\"T_0440f_row36_col3\" class=\"data row36 col3\" >0.689803</td>\n",
       "      <td id=\"T_0440f_row36_col4\" class=\"data row36 col4\" >0.658433</td>\n",
       "      <td id=\"T_0440f_row36_col5\" class=\"data row36 col5\" >190853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row37\" class=\"row_heading level0 row37\" >2</th>\n",
       "      <td id=\"T_0440f_row37_col0\" class=\"data row37 col0\" >0.037503</td>\n",
       "      <td id=\"T_0440f_row37_col1\" class=\"data row37 col1\" >52</td>\n",
       "      <td id=\"T_0440f_row37_col2\" class=\"data row37 col2\" >0.59</td>\n",
       "      <td id=\"T_0440f_row37_col3\" class=\"data row37 col3\" >0.699296</td>\n",
       "      <td id=\"T_0440f_row37_col4\" class=\"data row37 col4\" >0.799182</td>\n",
       "      <td id=\"T_0440f_row37_col5\" class=\"data row37 col5\" >191873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row38\" class=\"row_heading level0 row38\" >7</th>\n",
       "      <td id=\"T_0440f_row38_col0\" class=\"data row38 col0\" >0.027433</td>\n",
       "      <td id=\"T_0440f_row38_col1\" class=\"data row38 col1\" >55</td>\n",
       "      <td id=\"T_0440f_row38_col2\" class=\"data row38 col2\" >0.45</td>\n",
       "      <td id=\"T_0440f_row38_col3\" class=\"data row38 col3\" >0.698535</td>\n",
       "      <td id=\"T_0440f_row38_col4\" class=\"data row38 col4\" >0.577565</td>\n",
       "      <td id=\"T_0440f_row38_col5\" class=\"data row38 col5\" >192784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row39\" class=\"row_heading level0 row39\" >9</th>\n",
       "      <td id=\"T_0440f_row39_col0\" class=\"data row39 col0\" >0.009038</td>\n",
       "      <td id=\"T_0440f_row39_col1\" class=\"data row39 col1\" >31</td>\n",
       "      <td id=\"T_0440f_row39_col2\" class=\"data row39 col2\" >0.43</td>\n",
       "      <td id=\"T_0440f_row39_col3\" class=\"data row39 col3\" >0.700234</td>\n",
       "      <td id=\"T_0440f_row39_col4\" class=\"data row39 col4\" >0.534440</td>\n",
       "      <td id=\"T_0440f_row39_col5\" class=\"data row39 col5\" >196844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row40\" class=\"row_heading level0 row40\" >10</th>\n",
       "      <td id=\"T_0440f_row40_col0\" class=\"data row40 col0\" >0.099521</td>\n",
       "      <td id=\"T_0440f_row40_col1\" class=\"data row40 col1\" >41</td>\n",
       "      <td id=\"T_0440f_row40_col2\" class=\"data row40 col2\" >0.6</td>\n",
       "      <td id=\"T_0440f_row40_col3\" class=\"data row40 col3\" >0.695122</td>\n",
       "      <td id=\"T_0440f_row40_col4\" class=\"data row40 col4\" >0.823258</td>\n",
       "      <td id=\"T_0440f_row40_col5\" class=\"data row40 col5\" >196860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row41\" class=\"row_heading level0 row41\" >29</th>\n",
       "      <td id=\"T_0440f_row41_col0\" class=\"data row41 col0\" >0.018889</td>\n",
       "      <td id=\"T_0440f_row41_col1\" class=\"data row41 col1\" >49</td>\n",
       "      <td id=\"T_0440f_row41_col2\" class=\"data row41 col2\" >0.43</td>\n",
       "      <td id=\"T_0440f_row41_col3\" class=\"data row41 col3\" >0.699055</td>\n",
       "      <td id=\"T_0440f_row41_col4\" class=\"data row41 col4\" >0.535446</td>\n",
       "      <td id=\"T_0440f_row41_col5\" class=\"data row41 col5\" >197039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row42\" class=\"row_heading level0 row42\" >0</th>\n",
       "      <td id=\"T_0440f_row42_col0\" class=\"data row42 col0\" >0.021000</td>\n",
       "      <td id=\"T_0440f_row42_col1\" class=\"data row42 col1\" >65</td>\n",
       "      <td id=\"T_0440f_row42_col2\" class=\"data row42 col2\" >0.45</td>\n",
       "      <td id=\"T_0440f_row42_col3\" class=\"data row42 col3\" >0.691611</td>\n",
       "      <td id=\"T_0440f_row42_col4\" class=\"data row42 col4\" >0.547206</td>\n",
       "      <td id=\"T_0440f_row42_col5\" class=\"data row42 col5\" >198358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row43\" class=\"row_heading level0 row43\" >5</th>\n",
       "      <td id=\"T_0440f_row43_col0\" class=\"data row43 col0\" >0.004642</td>\n",
       "      <td id=\"T_0440f_row43_col1\" class=\"data row43 col1\" >31</td>\n",
       "      <td id=\"T_0440f_row43_col2\" class=\"data row43 col2\" >0.54</td>\n",
       "      <td id=\"T_0440f_row43_col3\" class=\"data row43 col3\" >0.669571</td>\n",
       "      <td id=\"T_0440f_row43_col4\" class=\"data row43 col4\" >0.781049</td>\n",
       "      <td id=\"T_0440f_row43_col5\" class=\"data row43 col5\" >201143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row44\" class=\"row_heading level0 row44\" >30</th>\n",
       "      <td id=\"T_0440f_row44_col0\" class=\"data row44 col0\" >0.013319</td>\n",
       "      <td id=\"T_0440f_row44_col1\" class=\"data row44 col1\" >69</td>\n",
       "      <td id=\"T_0440f_row44_col2\" class=\"data row44 col2\" >0.58</td>\n",
       "      <td id=\"T_0440f_row44_col3\" class=\"data row44 col3\" >0.684944</td>\n",
       "      <td id=\"T_0440f_row44_col4\" class=\"data row44 col4\" >0.821680</td>\n",
       "      <td id=\"T_0440f_row44_col5\" class=\"data row44 col5\" >201602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row45\" class=\"row_heading level0 row45\" >1</th>\n",
       "      <td id=\"T_0440f_row45_col0\" class=\"data row45 col0\" >0.002526</td>\n",
       "      <td id=\"T_0440f_row45_col1\" class=\"data row45 col1\" >66</td>\n",
       "      <td id=\"T_0440f_row45_col2\" class=\"data row45 col2\" >0.47</td>\n",
       "      <td id=\"T_0440f_row45_col3\" class=\"data row45 col3\" >0.676301</td>\n",
       "      <td id=\"T_0440f_row45_col4\" class=\"data row45 col4\" >0.531475</td>\n",
       "      <td id=\"T_0440f_row45_col5\" class=\"data row45 col5\" >203974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row46\" class=\"row_heading level0 row46\" >40</th>\n",
       "      <td id=\"T_0440f_row46_col0\" class=\"data row46 col0\" >0.002152</td>\n",
       "      <td id=\"T_0440f_row46_col1\" class=\"data row46 col1\" >54</td>\n",
       "      <td id=\"T_0440f_row46_col2\" class=\"data row46 col2\" >0.55</td>\n",
       "      <td id=\"T_0440f_row46_col3\" class=\"data row46 col3\" >0.672304</td>\n",
       "      <td id=\"T_0440f_row46_col4\" class=\"data row46 col4\" >0.813438</td>\n",
       "      <td id=\"T_0440f_row46_col5\" class=\"data row46 col5\" >204728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row47\" class=\"row_heading level0 row47\" >19</th>\n",
       "      <td id=\"T_0440f_row47_col0\" class=\"data row47 col0\" >0.001011</td>\n",
       "      <td id=\"T_0440f_row47_col1\" class=\"data row47 col1\" >52</td>\n",
       "      <td id=\"T_0440f_row47_col2\" class=\"data row47 col2\" >0.48</td>\n",
       "      <td id=\"T_0440f_row47_col3\" class=\"data row47 col3\" >0.668431</td>\n",
       "      <td id=\"T_0440f_row47_col4\" class=\"data row47 col4\" >0.535352</td>\n",
       "      <td id=\"T_0440f_row47_col5\" class=\"data row47 col5\" >206410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row48\" class=\"row_heading level0 row48\" >6</th>\n",
       "      <td id=\"T_0440f_row48_col0\" class=\"data row48 col0\" >0.001571</td>\n",
       "      <td id=\"T_0440f_row48_col1\" class=\"data row48 col1\" >58</td>\n",
       "      <td id=\"T_0440f_row48_col2\" class=\"data row48 col2\" >0.53</td>\n",
       "      <td id=\"T_0440f_row48_col3\" class=\"data row48 col3\" >0.664543</td>\n",
       "      <td id=\"T_0440f_row48_col4\" class=\"data row48 col4\" >0.818874</td>\n",
       "      <td id=\"T_0440f_row48_col5\" class=\"data row48 col5\" >208332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0440f_level0_row49\" class=\"row_heading level0 row49\" >16</th>\n",
       "      <td id=\"T_0440f_row49_col0\" class=\"data row49 col0\" >0.004371</td>\n",
       "      <td id=\"T_0440f_row49_col1\" class=\"data row49 col1\" >46</td>\n",
       "      <td id=\"T_0440f_row49_col2\" class=\"data row49 col2\" >0.4</td>\n",
       "      <td id=\"T_0440f_row49_col3\" class=\"data row49 col3\" >0.695070</td>\n",
       "      <td id=\"T_0440f_row49_col4\" class=\"data row49 col4\" >0.440549</td>\n",
       "      <td id=\"T_0440f_row49_col5\" class=\"data row49 col5\" >210111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x181f17458e0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur maximale d'une série en vert.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    Mettez en surbrillance la valeur minimale d'une série en vert.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: green' if v else '' for v in is_min]\n",
    "\n",
    "styled_df_LGM = (styled_df_LGM.style.apply(highlight_max, subset=['AUC', 'Accuracy'])\n",
    "                          .apply(highlight_min, subset=['Business Score'])\n",
    "                          .format({'Threshold': \"{:g}\"}))\n",
    "\n",
    "styled_df_LGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778760ba-6b6c-4814-8457-88a2fc4458c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4e698a-0d28-4a77-8b8a-4fcac3581d5b",
   "metadata": {},
   "source": [
    "#### Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "accde7e1-861e-4c43-b2c6-21a5f772c5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.02127915254605793,\n",
       " 'num_leaves': 58,\n",
       " 'n_estimators': 481,\n",
       " 'threshold': 0.52}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8e241af-55a7-4605-87de-51a9b144c8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3316\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3320\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 232\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3320\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3320\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 232\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: threshold\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supposition que df_classification_imputed est déjà défini\n",
    "nb_0 = len(application_train_petit_encoded[application_train_petit_encoded[\"TARGET\"] == 0])\n",
    "nb_1 = len(application_train_petit_encoded[application_train_petit_encoded[\"TARGET\"] == 1])\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "X = application_train_petit_encoded.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_petit_encoded[\"TARGET\"]\n",
    "\n",
    "# Stockage des noms des colonnes pour utilisation ultérieure\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Instanciation du modèle\n",
    "clf = LGBMClassifier(**study.best_params, class_weight=class_weights, verbose=1)\n",
    "\n",
    "# Validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "feature_importances = np.zeros(X.shape[1])\n",
    "\n",
    "for train, test in cv.split(X, y):\n",
    "    clf.fit(X[train], y.iloc[train])\n",
    "    feature_importances += clf.feature_importances_\n",
    "\n",
    "# Moyenne des importances de caractéristiques sur les plis\n",
    "feature_importances /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8530e37d-0804-4260-8c7e-5fa0c25b23f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "df_feature_importances_lightGBM = df_feature_importances_lightGBM.sort_values(by='importance', ascending=False)\n",
    "df_feature_importances_lightGBM[\"importance\"] = (df_feature_importances_lightGBM[\"importance\"]/ df_feature_importances_lightGBM[\"importance\"].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "605ef7dd-1b24-4dfc-acaa-ca0a98edbf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feature_importances_lightGBM.to_csv('df_feature_importances_lightGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b2656b82-9fc8-49d6-bf95-d41fe5264408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>DAYS_LAST_PHONE_CHANGE</td>\n",
       "      <td>7.249517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DAYS_ID_PUBLISH</td>\n",
       "      <td>7.228362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>7.175110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_BIRTH</td>\n",
       "      <td>6.817668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMT_ANNUITY</td>\n",
       "      <td>6.768793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>ORGANIZATION_TYPE_Industry: type 8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>FLOORSMAX_MODE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ENTRANCES_MODE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ELEVATORS_MODE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>NAME_INCOME_TYPE_Businessman</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                feature  importance\n",
       "77               DAYS_LAST_PHONE_CHANGE    7.249517\n",
       "9                       DAYS_ID_PUBLISH    7.228362\n",
       "8                     DAYS_REGISTRATION    7.175110\n",
       "6                            DAYS_BIRTH    6.817668\n",
       "3                           AMT_ANNUITY    6.768793\n",
       "..                                  ...         ...\n",
       "196  ORGANIZATION_TYPE_Industry: type 8    0.000000\n",
       "51                       FLOORSMAX_MODE    0.000000\n",
       "50                       ENTRANCES_MODE    0.000000\n",
       "49                       ELEVATORS_MODE    0.000000\n",
       "122        NAME_INCOME_TYPE_Businessman    0.000000\n",
       "\n",
       "[245 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importances_lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d418578b-fc4e-4d7e-8ccb-35b2874819e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pourcentage_importance_totale = 95\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "df_sorted = df_feature_importances_lightGBM.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Calculate the cumulative importance\n",
    "df_sorted[\"cumulative_importance\"] = df_sorted[\"importance\"].cumsum()\n",
    "\n",
    "# Find the number of features needed to reach 99% of the total importance\n",
    "num_features = df_sorted[df_sorted[\"cumulative_importance\"] <= df_sorted[\"importance\"].sum() * Pourcentage_importance_totale/100].shape[0]\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a2ffe56-2b94-4816-97d9-d3ab9f406eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABA4AAAK+CAYAAAAixFwPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1gUV/s38O/CwoJUC7q0CIqAFBuWRGLsggKWYCOiIJbYUPJYscQaQBNLYkGNC0jsXWMUlSgaW9QYjF2jIhZQowIqCgLz/uHL/Bx3l2LD8v1c11zPs2fuOXPPsBjmnjNnZIIgCCAiIiIiIiIi0kCnrBMgIiIiIiIioncXCwdEREREREREpBULB0RERERERESkFQsHRERERERERKQVCwdEREREREREpBULB0RERERERESkFQsHRERERERERKQVCwdEREREREREpBULB0RERETvqblz5+KXX34p6zSIiOgDx8IBERERvXEpKSmQyWSIi4sr61Q+GHPnzsWUKVPw6aeflnibpKQkyGQyJCUlvZYc4uLiIJPJkJKS8lr6o5Kzs7NDcHDwa+uvWbNmaNasmfj5VX5nC7f94YcfXlt+RFS2WDggIqL3WuGFy7Fjx8o6lZe2YMECXlCTKCIiAps2bSoy5ujRo/j222/x66+/okaNGu9ETkQvY9u2bZg0aZLW9Tk5OZg7dy4+//xzlC9fHvr6+rCyskL79u2xcuVK5Ofni7GFBYvnF1NTU9SpUwfz5s2TxALPiiUymUzr79CuXbvEftatW/dajpfofcXCARERURlj4YCeV5KL9NOnT2P9+vWlGm0AAF988QUeP36ML7744rXk1LNnTzx+/BhVq1YtVX/07qtatSoeP36Mnj17vtH9bNu2DZMnT9a47s6dO/D09MTQoUNhbGyM8ePHY9GiRQgNDcWjR4/w1VdfISIiQm27gIAA/PLLL/jll18QGRkJa2trhIaGYsyYMWqxBgYG+Pfff3HkyBG1dcuXL4eBgcGrHyTRB0Be1gkQERF9rLKzs1GuXLmyToNeUl5eHgoKCqCvr//W913aIepPnjyBvr4+dHR0XuuFkK6uLnR1dV9bf/TukMlkZX7R3LNnT/z9999Yv349vvzyS8m68PBwHDt2DOfPn1fbrl69eggMDBQ/Dxo0CI0aNcKKFSvw/fffS2KrV6+OvLw8rFy5Eg0bNhTbnzx5go0bN8LHxwfr169/zUdG9P7hiAMiIvrgBAcHw9jYGKmpqfD19YWxsTGsra0xf/58AMDJkyfRokULGBkZoWrVqlixYoVk+8LHH/bt24evv/4aFStWhKmpKXr16oX79++r7W/BggVwdXWFQqGAlZUVBg8ejIyMDElMs2bN4Obmhr/++gtffPEFypUrh7Fjx8LOzg6nT5/G3r17xSGxhc8Z37t3DyNGjIC7uzuMjY1hamqKtm3b4sSJE5K+C59bX7NmDb777jvY2NjAwMAALVu2xL///quW759//ol27dqhfPnyMDIyQq1atfDjjz9KYs6dO4fOnTujQoUKMDAwQP369bFly5YSnf+MjAwEBwfDzMwM5ubmCAoKUjsfpdnP06dPMXnyZNSoUQMGBgaoWLEiPv/8c+zatatEuXzzzTews7ODQqGAjY0NevXqhf/++w8AkJubi2+//RYeHh4wMzODkZERmjRpgj179kj6ef6Z7Tlz5qB69epQKBQ4c+ZMifsAgIKCAvz4449wd3eHgYEBLCws4O3tLT5qI5PJ8OjRIyxdulT8PjxfJLhx4wZCQkJQpUoVKBQKuLq6IiYmRrKPwu/DqlWrMH78eFhbW6NcuXLIysrSOMfBxYsX4e/vD6VSCQMDA9jY2KB79+7IzMwsNidNcxwIgoBp06bBxsYG5cqVQ/PmzXH69Gm1Z/InTZoEmUymdo60zZuwfft2NGnSBEZGRjAxMYGPjw9Onz4tiUlPT0fv3r1hY2MDhUIBS0tLdOjQodg5GP755x8EBwejWrVqMDAwgFKpREhICO7evSvGrFu3DjKZDHv37lXbftGiRZDJZDh16pTYVtLfoeK+o6WRkZGBsLAw2NraQqFQwMHBAdOnT0dBQUGp+9I2x8HatWvh4uICAwMDuLm5YePGjQgODoadnZ3GfhYvXiz+vjRo0ABHjx4V1wUHB4v/Lj//eAEAHDp0CDt27ED//v3VigaF6tevjx49ehR7LDKZDFWqVIFcrvmeaUBAAFavXi05T7/++iuys7PRtWvXYvsn+hhwxAEREX2Q8vPz0bZtW3zxxReYMWMGli9fjiFDhsDIyAjjxo1Djx498OWXX2LhwoXo1asXPvvsM9jb20v6GDJkCMzNzTFp0iScP38e0dHRuHr1qnjxBTy7+Jk8eTJatWqFgQMHinFHjx7FgQMHoKenJ/Z39+5dtG3bFt27d0dgYCCqVKmCZs2aITQ0FMbGxhg3bhwAoEqVKgCAy5cvY9OmTejSpQvs7e1x69YtLFq0CE2bNsWZM2dgZWUlyTcqKgo6OjoYMWIEMjMzMWPGDPTo0QN//vmnGLNr1y74+vrC0tISw4YNg1KpxNmzZ7F161YMGzYMwLNh8J6enrC2tsaYMWNgZGSENWvWoGPHjli/fj06deqk9bwLgoAOHTpg//79GDBgAGrWrImNGzciKChILbak+5k0aRIiIyPRt29fNGzYEFlZWTh27BiOHz+O1q1ba83l4cOHaNKkCc6ePYuQkBDUq1cP//33H7Zs2YLr16+jUqVKyMrKwpIlSxAQEIB+/frhwYMHUKlU8PLywpEjR1CnTh1Jn7GxsXjy5An69+8PhUKBChUqlKqPPn36IC4uDm3btkXfvn2Rl5eHP/74A4cPH0b9+vXxyy+/iMfZv39/AM/uiALArVu38Omnn0Imk2HIkCGwsLDA9u3b0adPH2RlZSEsLEyS69SpU6Gvr48RI0YgJydH48iI3NxceHl5IScnB6GhoVAqlbhx4wa2bt2KjIwMmJmZFZmTJt9++y2mTZuGdu3aoV27djh+/DjatGmD3NxcrdsU55dffkFQUBC8vLwwffp0ZGdnIzo6Gp9//jn+/vtv8aLV398fp0+fRmhoKOzs7HD79m3s2rULqampWi9sgWe/F5cvX0bv3r2hVCpx+vRpLF68GKdPn8bhw4chk8ng4+MDY2NjrFmzBk2bNpVsv3r1ari6usLNzQ1Ayb/bJfmOllR2djaaNm2KGzdu4Ouvv8Ynn3yCgwcPIjw8HGlpaZgzZ06pzrkmv/32G7p16wZ3d3dERkbi/v376NOnD6ytrTXGr1ixAg8ePMDXX38NmUyGGTNm4Msvv8Tly5ehp6eHr7/+Gjdv3sSuXbvU3g7y66+/AoBk5EBJZWdni4WXrKwsbN++HQkJCQgPD9cY/9VXX2HSpElISkpCixYtxNxbtmyJypUrl3r/RB8kgYiI6D0WGxsrABCOHj0qtgUFBQkAhIiICLHt/v37gqGhoSCTyYRVq1aJ7efOnRMACBMnTlTr08PDQ8jNzRXbZ8yYIQAQNm/eLAiCINy+fVvQ19cX2rRpI+Tn54tx8+bNEwAIMTExYlvTpk0FAMLChQvVjsHV1VVo2rSpWvuTJ08k/QqCIFy5ckVQKBTClClTxLY9e/YIAISaNWsKOTk5YvuPP/4oABBOnjwpCIIg5OXlCfb29kLVqlWF+/fvS/otKCgQ/3/Lli0Fd3d34cmTJ5L1jRs3FmrUqKGW5/M2bdokABBmzJghtuXl5QlNmjQRAAixsbGl3k/t2rUFHx+fIverybfffisAEDZs2KC2rvB48/LyJOdMEJ59V6pUqSKEhISIbVeuXBEACKampsLt27cl8SXtY/fu3QIAYejQoVrzEQRBMDIyEoKCgtRi+vTpI1haWgr//fefpL179+6CmZmZkJ2dLQjC/30fqlWrJrYVKly3Z88eQRAE4e+//xYACGvXrlXb3/O05VT4u3LlyhVBEP7vd8LHx0dyTGPHjhUASPqYOHGioOlP0Rf7fPDggWBubi7069dPEpeeni6YmZmJ7ffv3xcACN9//32Rx6LJi+dJEARh5cqVAgBh3759YltAQIBQuXJlIS8vT2xLS0sTdHR0JL+TJf1ul+Q7qk3VqlUl53Pq1KmCkZGRcOHCBUncmDFjBF1dXSE1NbXI/po2bSr5d6jwO//876y7u7tgY2MjPHjwQGxLSkoSAAhVq1ZV27ZixYrCvXv3xPbNmzcLAIRff/1VbBs8eLDG70GnTp0EAEJGRoak/fHjx8KdO3fE5fl/ywr3q2kZOHCg2jlt2rSp4OrqKgiCINSvX1/o06ePIAjPvkv6+vrC0qVLxd+Z4n5HiD50fFSBiIg+WH379hX/v7m5OZycnGBkZCQZeurk5ARzc3NcvnxZbfv+/ftLRgwMHDgQcrkc27ZtAwAkJiYiNzcXYWFh0NH5v/+k9uvXD6ampvjtt98k/SkUCvTu3bvE+SsUCrHf/Px83L17F8bGxnBycsLx48fV4nv37i25q9ykSRMAEI/t77//xpUrVxAWFgZzc3PJtoUjKO7du4fdu3eja9euePDgAf777z/8999/uHv3Lry8vHDx4kXcuHFDa87btm2DXC7HwIEDxTZdXV2EhoZK4kqzH3Nzc5w+fRoXL14s6akDAKxfvx61a9fWOEKi8Hh1dXXFc1ZQUIB79+4hLy8P9evX13iO/f39YWFhIWkraR/r16+HTCbDxIkTteajjSAIWL9+Pfz8/CAIgni+/vvvP3h5eSEzM1Mt36CgIBgaGhbZr5mZGQBgx44dyM7OLjK2JAp/J0JDQyXH9OJoiNLYtWsXMjIyEBAQIDluXV1dNGrUSHwkxNDQEPr6+khKStL4SFFRnj9PT548wX///SdOPPn8ee3WrRtu374tedRj3bp1KCgoQLdu3QCU7rtdku9oSa1duxZNmjRB+fLlJeepVatWyM/Px759+0rV34tu3ryJkydPolevXjA2NhbbmzZtCnd3d43bdOvWDeXLlxc/v/hvUlGysrIAQLIvAFi4cCEsLCzE5fPPP1fbtn///ti1axd27dqF9evXY/DgwVi0aBH+97//ad3fV199hQ0bNiA3Nxfr1q2Drq5ukaOriD42fFSBiIg+SIXPjz/PzMwMNjY2an+Qm5mZabzQePEVXcbGxrC0tBSfl7569SqAZ8WH5+nr66NatWri+kLW1talmkiv8Hn4BQsW4MqVK5JXiVWsWFEt/pNPPpF8LvyDvfDYLl26BADicGpN/v33XwiCgAkTJmDChAkaY27fvq11aPLVq1dhaWmp9sf+i+eoNPuZMmUKOnToAEdHR7i5ucHb2xs9e/ZErVq1tB4H8Ox4/f39i4wBgKVLl2LmzJk4d+4cnj59Kra/+OiKtraS9nHp0iVYWVmhQoUKxeb0ojt37iAjIwOLFy/G4sWLNcbcvn27RLm+GPO///0Ps2bNwvLly9GkSRO0b98egYGBYlGhNAq/8y/+7lhYWEguIEujsGBUOIT8RaampgCeFdqmT5+O4cOHo0qVKvj000/h6+uLXr16QalUFrmPe/fuYfLkyVi1apXaeSyc6wEAvL29YWZmhtWrV6Nly5YAnj2mUKdOHTg6OgIo3Xe7pN/Rkrh48SL++ecftX/3nt/nqyj82To4OKitc3Bw0FhoK+7fpKKYmJgAePY4x/PfRX9/f/HfsOHDh6u9YhF49v1r1aqV+PnLL7+ETCbDnDlzEBISorHQ0b17d4wYMQLbt2/H8uXL4evrK+ZARCwcEBHRB0rbTO/a2gVBeJPpAECxd39fFBERgQkTJiAkJARTp05FhQoVoKOjg7CwMI2Tnb2OYyvsd8SIEfDy8tIYo+nCobRKs58vvvgCly5dwubNm7Fz504sWbIEs2fPxsKFCyWjSl7GsmXLEBwcjI4dO2LkyJGoXLkydHV1ERkZKRZanqfpZ1jaPl5G4fkKDAzUOF8EALVCSkm/bzNnzkRwcLB4focOHYrIyEgcPnwYNjY2r5Z4EbTdUX/xQrDw2H/55ReNBYDnJ7wLCwuDn58fNm3ahB07dmDChAmIjIzE7t27UbduXa25dO3aFQcPHsTIkSNRp04dGBsbo6CgAN7e3pLfNYVCgY4dO2Ljxo1YsGABbt26hQMHDkheCfi2fodeVFBQgNatW2PUqFEa1xcWNt6mV/k3ydnZGQBw6tQpeHp6iu22trawtbUFAHF0RUm0bNkS8+bNw759+zQWDiwtLdGsWTPMnDkTBw4c4JsUiF7AwgEREZEWFy9eRPPmzcXPDx8+RFpaGtq1awcA4rvrz58/j2rVqolxubm5uHLliuSOV1G0XUCtW7cOzZs3h0qlkrRnZGSUatK0QoUT2p06dUprboXHoaenV+L8n1e1alX8/vvvePjwoWTUwYuvTCvtfipUqIDevXujd+/eePjwIb744gtMmjSpyMJB9erVJbPca7Ju3TpUq1YNGzZskPwcND1O8Kp9VK9eHTt27MC9e/eKHHWg6ftgYWEBExMT5Ofnv9TPpTju7u5wd3fH+PHjcfDgQXh6emLhwoWYNm2a1pw0KfyduHjxouR34s6dO2p3mQvvPmdkZEgenXlxpE7h97Zy5colOvbq1atj+PDhGD58OC5evIg6depg5syZWLZsmcb4+/fv4/fff8fkyZPx7bffiu3aHo3p1q0bli5dit9//x1nz56FIAjiYwpA6b7bJfmOllT16tXx8OHDN/L9AP7vZ6vpTS2a2kpK23fL19cXUVFRWL58uaRw8LLy8vIAPPt3XJuvvvoKffv2hbm5ufjvPBE9wzkOiIiItFi8eLFk2Hl0dDTy8vLQtm1bAECrVq2gr6+Pn376SXIHTaVSITMzEz4+PiXaj5GRkcbXFerq6qrdmVu7dm2RcwwUpV69erC3t8ecOXPU9le4n8qVK6NZs2ZYtGgR0tLS1Pq4c+dOkfto164d8vLyEB0dLbbl5+dj7ty5krjS7Of5V+IBzx4ZcXBwQE5OTpG5+Pv748SJE9i4caPausLjLbwj+vx5/vPPP3Ho0KEi+35eSfvw9/eHIAiYPHmy1nwAzd8HXV1d+Pv7Y/369RovNIv7uWiTlZUlXlAVcnd3h46OjuT8avuOvqhVq1bQ09PD3LlzJcekaUb/woLA88/eF7728XleXl4wNTVFRESE5PexUOGxZ2dn48mTJ2r7MDExKfK7ounnpy1n4NkxVqhQAatXr8bq1avRsGFDyWMhpflul+Q7WlJdu3YVX2H4ooyMDLWfc2lZWVnBzc0N8fHxkovvvXv34uTJky/dr5GRkZjj8zw9PdG6dWssXrwYmzdv1rhtac5R4VsaateurTWmc+fOmDhxIhYsWFCqx8qIPgYccUBERKRFbm4uWrZsia5du+L8+fNYsGABPv/8c7Rv3x7As7vA4eHhmDx5Mry9vdG+fXsxrkGDBiV+jZiHhweio6Mxbdo0ODg4oHLlymjRogV8fX0xZcoU9O7dG40bN8bJkyexfPlyyZ3c0tDR0UF0dDT8/PxQp04d9O7dG5aWljh37hxOnz4tXnDMnz8fn3/+Odzd3dGvXz9Uq1YNt27dwqFDh3D9+nWcOHFC6z78/Pzg6emJMWPGICUlBS4uLtiwYYPkOfFCJd2Pi4sLmjVrBg8PD1SoUAHHjh3DunXrMGTIkCKPd+TIkVi3bh26dOmCkJAQeHh44N69e9iyZQsWLlyI2rVrw9fXFxs2bECnTp3g4+ODK1euYOHChXBxcSnyzuTzStpH8+bN0bNnT/z000+4ePGiOAz+jz/+QPPmzcXj8fDwQGJiImbNmgUrKyvY29ujUaNGiIqKwp49e9CoUSP069cPLi4uuHfvHo4fP47ExETcu3evRPk+b/fu3RgyZAi6dOkCR0dH5OXl4ZdffhELFYW05fQiCwsLjBgxApGRkfD19UW7du3w999/Y/v27WqjZNq0aYNPPvkEffr0wciRI6Grq4uYmBhYWFggNTVVjDM1NUV0dDR69uyJevXqoXv37mLMb7/9Bk9PT8ybNw8XLlwQf19dXFwgl8uxceNG3Lp1C927d9d6DkxNTcXXtj59+hTW1tbYuXMnrly5ojFeT08PX375JVatWoVHjx7hhx9+UIsp6Xe7JN/Rkho5ciS2bNkCX19fBAcHw8PDA48ePcLJkyexbt06pKSkvNRIpedFRESgQ4cO8PT0RO/evXH//n3MmzcPbm5uJf59eZGHhwcAYOjQofDy8oKurq7481q2bBm8vb3RsWNHtG3bFq1atUL58uWRnp6OxMRE7Nu3TyzkPu/48ePiCJMHDx7g999/x/r169G4cWO0adNGay5mZmaYNGnSSx0H0Qfvrb7DgYiI6DXT9jpGIyMjtdjnX731vKpVq0pe91fY5969e4X+/fsL5cuXF4yNjYUePXoId+/eVdt+3rx5grOzs6CnpydUqVJFGDhwoNrrDrXtWxCevVbOx8dHMDExEQCIr0R78uSJMHz4cMHS0lIwNDQUPD09hUOHDqm9Nk3b68I0vU5NEARh//79QuvWrQUTExPByMhIqFWrljB37lxJzKVLl4RevXoJSqVS0NPTE6ytrQVfX19h3bp1Go/heXfv3hV69uwpmJqaCmZmZkLPnj3F1/69mEtJ9jNt2jShYcOGgrm5uWBoaCg4OzsL3333neRVmUXlMmTIEMHa2lrQ19cXbGxshKCgIPGVhgUFBUJERIRQtWpVQaFQCHXr1hW2bt0qBAUFaXy9nKZX/ZW0D0F49urG77//XnB2dhb09fUFCwsLoW3btsJff/0lxpw7d0744osvBENDQ7VXGN66dUsYPHiwYGtrK+jp6QlKpVJo2bKlsHjxYjGmqNfHvfg6xsuXLwshISFC9erVBQMDA6FChQpC8+bNhcTERMl22nJ68dWJgiAI+fn5wuTJk8XvbbNmzYRTp06pvT5QEAThr7/+Eho1aiTo6+sLn3zyiTBr1iyNfRbm7uXlJZiZmQkGBgZC9erVheDgYOHYsWOCIAjCf//9JwwePFhwdnYWjIyMBDMzM6FRo0bCmjVr1M7Di65fvy506tRJMDc3F8zMzIQuXboIN2/eVHtVa6Fdu3YJAASZTCZcu3ZNY58l/R0q7juqjabz+eDBAyE8PFxwcHAQ9PX1hUqVKgmNGzcWfvjhh2J/X0ryOkZBEIRVq1YJzs7OgkKhENzc3IQtW7YI/v7+grOzs9q2mn5fXjyneXl5QmhoqGBhYSHIZDK1VzM+fvxYmDNnjvDZZ58JpqamglwuF5RKpeDr6yssX75c8mpMTa9jlMvlQrVq1YSRI0dKXiNZeMza/l0uxNcxEj0jE4S3MBsUERHReyQuLg69e/fG0aNHUb9+/bJOh+iDYGdnh2bNmiEuLq6sU6HXrE6dOrCwsMCuXbvKOhUiekM4xwERERERERXr6dOnanMlJCUl4cSJE2jWrFnZJEVEbwXnOCAiIiIiomLduHEDrVq1QmBgIKysrHDu3DksXLgQSqUSAwYMKOv0iOgNYuGAiIiIiIiKVb58eXh4eGDJkiW4c+cOjIyM4OPjg6ioKFSsWLGs0yOiN4hzHBARERERERGRVpzjgIiIiIiIiIi0YuGAiIiIiIiIiLTiHAdEH4mCggLcvHkTJiYmkMlkZZ0OERERERGVEUEQ8ODBA1hZWUFHp/jxBCwcEH0kbt68CVtb27JOg4iIiIiI3hHXrl2DjY1NsXEsHBB9JExMTAA8+8fB1NS0jLMhIiIiIqKykpWVBVtbW/EaoTgsHBB9JAofTzA1NWXhgIiIiIiISvwIMwsHRB8Zt4k7oKMoV9ZpEBERERF9NFKifMo6hVfCtyoQERERERERkVYsHBARERERERGRViwcEBEREREREZFWLBwQERERERERkVYsHBARERERERGRViwcEBEREREREZFWLBwQERERERERlTE7OzvIZDK1ZfDgwRrjf/75ZzRp0gTly5dH+fLl0apVKxw5ckQSExwcrNaft7d3qXOTv9QREREREREREdFrc/ToUeTn54ufT506hdatW6NLly4a45OSkhAQEIDGjRvDwMAA06dPR5s2bXD69GlYW1uLcd7e3oiNjRU/KxSKUuf23o04eL5ioqenhypVqqB169aIiYlBQUGBWryXlxd0dXVx9OhRAEBOTg5cXV3Rv39/tdhRo0bB3t4eDx48QH5+PqKiouDs7AxDQ0NUqFABjRo1wpIlS0qcZ8eOHYuNu379OvT19eHm5qZx/d69e9GiRQtUqFAB5cqVQ40aNRAUFITc3FyN1aPnFzs7u2L336xZMzHewMAALi4uWLBggbg+Li4O5ubmGreVyWTYtGmTpG3r1q1o2rQpTExMUK5cOTRo0ABxcXGSmJSUFMhkMlSuXBkPHjyQrKtTpw4mTZqkMb/nlwEDBhR7bIX27NmDdu3aoWLFiihXrhxcXFwwfPhw3LhxA8CzXziZTIaMjAy1be3s7DBnzhy19sjISOjq6uL7779XWxcXF6exkpeRkQGZTIakpCS1/Hx9fWFhYQEDAwNUr14d3bp1w759+8SYwhw1Lenp6SU+F0RERERE9G6ysLCAUqkUl61bt6J69epo2rSpxvjly5dj0KBBqFOnDpydnbFkyRIUFBTg999/l8QpFApJv+XLly91bu9d4QB4VjFJS0tDSkoKtm/fjubNm2PYsGHw9fVFXl6eGJeamoqDBw9iyJAhiImJAfDspMXHxyMuLg47duwQYw8fPozZs2cjLi4OJiYmmDx5MmbPno2pU6fizJkz2LNnD/r376/x4vJVxMXFoWvXrsjKysKff/4pWXfmzBl4e3ujfv362LdvH06ePIm5c+dCX18f+fn5+PHHH5GWliYuABAbGyt+LiyWFKdfv35IS0vDmTNn0LVrVwwePBgrV64s9bHMnTsXHTp0gKenJ/7880/8888/6N69OwYMGIARI0aoxT948AA//PBDifN7fpkxY0aJclq0aBFatWoFpVKJ9evX48yZM1i4cCEyMzMxc+bMUh9joZiYGIwaNUr8Xr1ILpcjMTERe/bsKbKfBQsWoGXLlqhYsSJWr16N8+fPY+PGjWjcuDG++eYbtfjz58+rnYvKlSu/9HEQEREREdG7Jzc3F8uWLUNISAhkMlmJtsnOzsbTp09RoUIFSXtSUhIqV64MJycnDBw4EHfv3i11Pu/lowqFFRMAsLa2Rr169fDpp5+iZcuWiIuLQ9++fQE8u4j29fXFwIED8emnn2LWrFkwNDSEh4cHxo0bhz59+uDUqVMwMDBA7969ERoaKlZztmzZgkGDBkmGhdSuXfu1HocgCIiNjcWCBQtgY2MDlUqFRo0aiet37twJpVIpuUiuXr26eCfb0NAQZmZmkj7Nzc3Fc1NS5cqVE7eZNGkSVqxYgS1btiAgIKDEfVy7dg3Dhw9HWFgYIiIixPbhw4dDX18fQ4cORZcuXSTHFxoailmzZmHw4MFFXvw+n19pXL9+HUOHDsXQoUMxe/Zssd3Ozg5ffPHFSxeB9u7di8ePH2PKlCmIj4/HwYMH0bhxY0mMkZERunbtijFjxqgVhAqlpqYiLCwMYWFhmDVrlmRdrVq1MHToULVtKleurHUECBERERERfRg2bdqEjIwMBAcHl3ib0aNHw8rKCq1atRLbvL298eWXX8Le3h6XLl3C2LFj0bZtW8lN9JJ4L0ccaNKiRQvUrl0bGzZsAPB/F+WBgYFwdnaGg4MD1q1bJ8aPGzcOSqUSQ4cOxfjx4yGTySQXvEqlErt378adO3feWM579uxBdnY2WrVqhcDAQKxatQqPHj2S5JCWliYZsv42GBoaIjc3t1TbrFu3Dk+fPtU4suDrr7+GsbGx2iiGgIAAODg4YMqUKa+UrzZr165Fbm4uRo0apXH9y16Aq1QqBAQEQE9PDwEBAVCpVBrjJk2ahJMnT0q+d89bv349nj59qjW/klYWtcnJyUFWVpZkISIiIiKid59KpULbtm1hZWVVovioqCisWrUKGzduhIGBgdjevXt3tG/fHu7u7ujYsSO2bt2Ko0eP4o8//ihVPh9M4QAAnJ2dkZKSAgBITExEdnY2vLy8AACBgYGSCzy5XI74+HisXbsWc+fORXx8vOQEz5o1C3fu3IFSqUStWrUwYMAAbN++/bXmq1Kp0L17d+jq6sLNzQ3VqlXD2rVrxfVdunRBQEAAmjZtCktLS3Tq1Anz5s17YxeA+fn5WLZsGf755x+0aNFCbM/MzISxsbHa8rwLFy7AzMwMlpaWav3q6+ujWrVquHDhgqRdJpMhKioKixcvxqVLl7TmtWDBArV9L1++vNjjuXjxIkxNTTXmpImNjY3aflJTUyUxWVlZWLduHQIDAwE8+16tWbMGDx8+VOvPysoKw4YNw7hx4ySP0BS6cOECTE1NJaMp1q9fL9n/yZMni8zR1dVV6/FERkbCzMxMXGxtbUt0HoiIiIiIqOxcvXoViYmJ4kj64vzwww+IiorCzp07UatWrSJjq1WrhkqVKuHy5culyumDKhwIgiDepY2JiUG3bt0glz97GiMgIAAHDhyQXKC6uLjA398frVu3Rv369SV9ubi44NSpUzh8+DBCQkJw+/Zt+Pn5lfiHV5yMjAxs2LBBvAAF1Isburq6iI2NxfXr1zFjxgxYW1sjIiICrq6u4pwGr0PhhbmhoSH69euHb775BgMHDhTXm5iYIDk5WW15Hby8vPD5559jwoQJWmN69Oihtu/27dsX2/fz34eS+OOPP9T282KFb+XKlahevbr42EqdOnVQtWpVrF69WmOfo0ePxp07d7TOhfBifl5eXkhOTsZvv/2GR48eSWZV1ZTjtm3btB5PeHg4MjMzxeXatWvFngMiIiIiIipbsbGxqFy5Mnx8fIqNnTFjBqZOnYqEhAS1a1pNrl+/jrt375b6UfD3co4Dbc6ePQt7e3vcu3cPGzduxNOnTxEdHS2uz8/PR0xMDL777juxTS6Xi8WFF+no6KBBgwZo0KABwsLCsGzZMvTs2RPjxo2Dvb39K+W6YsUKPHnyRPLMvyAIKCgowIULF+Do6Ci2W1tbo2fPnujZsyemTp0KR0dHLFy4EJMnT36lHAr16NED48aNg6GhISwtLaGjI60n6ejowMHBocg+HB0dkZmZiZs3b6pdbOfm5uLSpUto3ry5xm2joqLw2WefYeTIkRrXm5mZFbv/onJKS0sr0agDe3t7tccXXvxuqFQqnD59WtJeUFCAmJgY9OnTR61Pc3NzhIeHY/LkyfD19ZWsq1GjBjIzM5Geni7+4hobG8PBwUHrd1JTjtooFIqXetUKERERERGVjYKCAsTGxiIoKEjtmqBXr16wtrZGZGQkAGD69On49ttvsWLFCtjZ2YlvWyscnfzw4UNMnjwZ/v7+UCqVuHTpEkaNGgUHBwe0bNmyVHl9MCMOdu/ejZMnT8Lf3x/Lly+HjY0NTpw4Ibk7O3PmTMTFxandxS0pFxcXAJDMQ/CyVCoVhg8fLsnvxIkTaNKkida70wBQvnx5WFpavpYcChVemFtbW6sVDUrK398fenp6Gt9UsHDhQjx69EjrZIsNGzbEl19+iTFjxrzUvrXp3Lkz9PX1tb6BobSTI548eRLHjh1DUlKS5OeWlJSEQ4cO4dy5cxq3Cw0NhY6ODn788Ue1/PT09DB9+vRS5UFERERERB+mxMREpKamIiQkRG1damqqZOR5dHQ0cnNz0blzZ1haWopL4ZvrdHV18c8//6B9+/ZwdHREnz594OHhgT/++KPUNxjfyxEHOTk5SE9PR35+Pm7duoWEhARERkbC19cXvXr1goeHBzp37gw3NzfJdra2tggPD0dCQkKxwz46d+4MT09PNG7cGEqlEleuXEF4eDgcHR3h7OxcojwzMzPVhvRXrFgRd+/exfHjx7F8+XK1vgICAjBlyhRMmzYNKpUKycnJ6NSpE6pXr44nT54gPj4ep0+fxty5c0uUw9vyySefYMaMGRg+fDgMDAzQs2dP6OnpYfPmzRg7diyGDx8uGV3xou+++w6urq4a77RnZ2eL1bNCCoWi2PeP2traYvbs2RgyZAiysrLQq1cv2NnZ4fr164iPj4exsXGpXsmoUqnQsGFDfPHFF2rrGjRoAJVKhe+//15tnYGBASZPnozBgwdL2j/55BPMnDkTw4YNw7179xAcHCyOmFm2bBmAZ7/sz7t9+zaePHkiaatYsSL09PRKfBxERERERPRuatOmDQRB0LguKSlJ8rlwfj9tDA0Ntb49obTz5r2XIw4SEhJgaWkJOzs7eHt7Y8+ePfjpp5+wefNm8c69v7+/2nZmZmZo2bKl1lnwn+fl5YVff/0Vfn5+cHR0RFBQEJydnbFz506tw8hflJSUhLp160qWyZMnQ6VSwcXFRWMBolOnTrh9+za2bduGhg0b4uHDhxgwYABcXV3RtGlTHD58GJs2bRJfG/kuCQsLw8aNG/HHH3+gfv36cHNzw4oVKxAdHS1WvbRxdHRESEiI2kUxAPz888+SCpqlpWWJXxU5aNAg7Ny5Ezdu3ECnTp3g7OyMvn37wtTUVOMbILQpfI+qpu8V8GzERXx8PJ4+fapxfVBQEKpVq6bWHhoaip07d+LOnTvo3LkzatSogXbt2uHKlStISEiAu7u7JN7JyUntXPz1118lPg4iIiIiIqLSkgnayhlE9EHJysp69naFsDXQUZQr63SIiIiIiD4aKVHFT3T4NhVeG2RmZsLU1LTY+PdyxAERERERERERvR0sHLyE1NRUcaZKTUtqampZpwjg2av7isrzfRcREaH12Nq2bVvW6REREREREX0Q3svJEcualZWV2qSHL65/F9SvX7/IPN93AwYMQNeuXTWuMzQ0fMvZEBERERERfZhYOHgJcrkcDg4OZZ1GsQwNDd+LPF9WhQoVUKFChbJOg4iIiIiI6IPGRxWIiIiIiIiISCuOOCD6yJya7FWimVOJiIiIiIgAjjggIiIiIiIioiKwcEBEREREREREWrFwQERERERERERasXBARERERERERFqxcEBEREREREREWvGtCkQfGbeJO6CjKFfWaRARERERffBSonzKOoXXgiMOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiMqInZ0dZDKZ2jJ48GCN8T///DOaNGmC8uXLo3z58mjVqhWOHDkiiZk0aRKcnZ1hZGQkxvz5558vnSMLB0RERERERERl5OjRo0hLSxOXXbt2AQC6dOmiMT4pKQkBAQHYs2cPDh06BFtbW7Rp0wY3btwQYxwdHTFv3jycPHkS+/fvh52dHdq0aYM7d+68VI4sHHzggoODxYqVnp4eqlSpgtatWyMmJgYFBQVq8V5eXtDV1cXRo0cBADk5OXB1dUX//v3VYkeNGgV7e3s8ePAA+fn5iIqKgrOzMwwNDVGhQgU0atQIS5YsKXGeHTt2fOm8tXm+emdkZIR69eph7dq1WvdbKCkpCTKZDBkZGQCAuLg4SfXP2NgYHh4e2LBhg2S7Zs2aISwsTGs+MpkMmzZtEj/v3bsXLVq0QIUKFVCuXDnUqFEDQUFByM3N1ZjHi8c2Z86ckp4KIiIiIiJ6B1lYWECpVIrL1q1bUb16dTRt2lRj/PLlyzFo0CDUqVMHzs7OWLJkCQoKCvD777+LMV999RVatWqFatWqwdXVFbNmzUJWVhb++eefl8qRhYOPgLe3N9LS0pCSkoLt27ejefPmGDZsGHx9fZGXlyfGpaam4uDBgxgyZAhiYmIAAAqFAvHx8YiLi8OOHTvE2MOHD2P27NmIi4uDiYkJJk+ejNmzZ2Pq1Kk4c+YM9uzZg/79+2u84H3deRdnypQpSEtLw99//40GDRqgW7duOHjwYKnzMTU1FauAf//9N7y8vNC1a1ecP3++1H0BwJkzZ+Dt7Y369etj3759OHnyJObOnQt9fX3k5+e/VJ9ERERERPT+ys3NxbJlyxASEgKZTFaibbKzs/H06VNUqFBBa5+LFy+GmZkZateu/VJ5yV9qK3qvKBQKKJVKAIC1tTXq1auHTz/9FC1btkRcXBz69u0LAIiNjYWvry8GDhyITz/9FLNmzYKhoSE8PDwwbtw49OnTB6dOnYKBgQF69+6N0NBQsQq2ZcsWDBo0SDKc5mW/lKXNuzgmJiZi9W7+/PlYtmwZfv31VzRu3LhU+chkMjEfpVKJadOm4YcffsA///wDJyen0h0cgJ07d0KpVGLGjBliW/Xq1eHt7V3qvjTJyclBTk6O+DkrK+u19EtERERERG/Gpk2bkJGRgeDg4BJvM3r0aFhZWaFVq1aS9q1bt6J79+7Izs6GpaUldu3ahUqVKr1UXhxx8JFq0aIFateuLQ61FwQBsbGxCAwMhLOzMxwcHLBu3Toxfty4cVAqlRg6dCjGjx8PmUyGiIgIcb1SqcTu3btf+pmZl827tORyOfT09MRHAV5Wfn4+li5dCgCoV6/eS/WhVCqRlpaGffv2vVIu2kRGRsLMzExcbG1t38h+iIiIiIjo9VCpVGjbti2srKxKFB8VFYVVq1Zh48aNMDAwkKxr3rw5kpOTcfDgQXh7e6Nr1664ffv2S+XFwsFHzNnZGSkpKQCAxMREZGdnw8vLCwAQGBgIlUolxsrlcsTHx2Pt2rWYO3cu4uPjJV/MWbNm4c6dO1AqlahVqxYGDBiA7du3v/G8SyM3NxeRkZHIzMxEixYtSr19ZmYmjI2NYWxsDH19fQwcOBCLFy9G9erVS90X8Gyyk4CAADRt2hSWlpbo1KkT5s2bp3FkgI2NjbjvwiU1NbXI/sPDw5GZmSku165de6k8iYiIiIjozbt69SoSExNLPLL6hx9+QFRUFHbu3IlatWqprTcyMoKDgwM+/fRTqFQqyOVyyTVeabBw8BETBEF8biYmJgbdunWDXP7s6ZWAgAAcOHAAly5dEuNdXFzg7++P1q1bo379+pK+XFxccOrUKRw+fBghISG4ffs2/Pz8Svylf9m8S2L06NEwNjZGuXLlMH36dERFRcHHx6fU+zUxMUFycjKSk5Px999/IyIiAgMGDMCvv/5a6r4AQFdXF7Gxsbh+/TpmzJgBa2trREREwNXVFWlpaZLYP/74Q9x34VJcFVKhUMDU1FSyEBERERHRuyk2NhaVK1cu0bXKjBkzMHXqVCQkJKhdm2lTUFAgeZS5NFg4+IidPXsW9vb2uHfvHjZu3IgFCxZALpdDLpfD2toaeXl54iSJhQrXa6Kjo4MGDRogLCwMGzZsQFxcHFQqFa5cufJG8i6pkSNHIjk5GdevX8f9+/cxevRocZ2pqSkyMzPVtsnIyICuri6MjIzENh0dHTg4OMDBwQG1atXC//73PzRr1gzTp09/peOxtrZGz549MW/ePJw+fRpPnjzBwoULJTH29vbivgsXbT8HIiIiIiJ6vxQUFCA2NhZBQUFqf+f36tUL4eHh4ufp06djwoQJiImJgZ2dHdLT05Geno6HDx8CAB49eoSxY8fi8OHDuHr1Kv766y+EhITgxo0bWl/xWBwWDj5Su3fvxsmTJ+Hv74/ly5fDxsYGJ06ckNzRnjlzJuLi4l56hn8XFxcAz764byLvkqpUqRIcHBygVCrVRio4OTnh9OnTapW348ePw97eHnp6ekX2rauri8ePH5f8AIpRvnx5WFpavtZzRkRERERE77bExESkpqYiJCREbV1qaqpkRHJ0dDRyc3PRuXNnWFpaissPP/wA4Nk1yrlz5+Dv7w9HR0f4+fnh7t27+OOPP+Dq6vpS+fGW5UcgJycH6enpyM/Px61bt5CQkIDIyEj4+vqiV69e8PDwQOfOneHm5ibZztbWFuHh4UhISCh2uEznzp3h6emJxo0bQ6lU4sqVKwgPD4ejoyOcnZ3fSN6vQ48ePTBlyhT06tULo0aNgpmZGfbt24c5c+ZI3nYAPHtEIj09HQDw+PFj7Nq1Czt27MC3334ribtz5w6Sk5MlbZaWlqhSpYqkbdGiRUhOTkanTp1QvXp1PHnyBPHx8Th9+jTmzp37Wo6PiIiIiIjefW3atIEgCBrXJSUlST4XN9+bgYHBS08mrw0LBx+BhIQEWFpaQi6Xo3z58qhduzZ++uknBAUF4e+//8aJEyfw888/q21nZmaGli1bQqVSFVs48PLywsqVK8XJB5VKJVq0aIFJkya99JD6ovLW0Xk9g2XMzc3xxx9/YMyYMWjfvj0yMzPh4OCAWbNmoU+fPpLYrKwsWFpaAng2f0DVqlUxZcoUyaMPALBixQqsWLFC0jZ16lSMHz9e0tawYUPs378fAwYMwM2bN2FsbAxXV1ds2rRJfM0lERERERFRWZMJ2soaRPRBycrKevZaxrA10FGUK+t0iIiIiIg+eClRpZ+U/W0ovDbIzMws0STqnOOAiIiIiIiIiLRi4YDeuNTUVBgbG2tdUlNTX6rf5cuXa+3zZSf9ICIiIiIiIinOcUBvnJWVldpkgS+ufxnt27dHo0aNNK4r7m0IREREREREVDIsHNAbJ5fL4eDg8Nr7NTExgYmJyWvvl4iIiIiIiP4PH1UgIiIiIiIiIq044oDoI3NqsleJZk4lIiIiIiICOOKAiIiIiIiIiIrAwgERERERERERacXCARERERERERFpxcIBEREREREREWnFwgERERERERERacW3KhB9ZNwm7oCOolxZp0FERERE9EFJifIp6xTeGI44ICIiIiIiIiKtWDggIiIiIiIiIq1YOCAiIiIiIiIirVg4ICIiIiIiIiKtWDggIiIiIiIiIq1YOCAiIiIiIiIirVg4ICIiIiIiInrD7OzsIJPJ1JbBgwdrjD99+jT8/f3F7ebMmaMWk5+fjwkTJsDe3h6GhoaoXr06pk6dCkEQXmvuLBy8Q4KDg8Uvj56eHqpUqYLWrVsjJiYGBQUFavFeXl7Q1dXF0aNHAQA5OTlwdXVF//791WJHjRoFe3t7PHjwAPn5+YiKioKzszMMDQ1RoUIFNGrUCEuWLHmpPO3t7TFq1Cg8efJEEqfpl0Imk2HVqlVijCAI+Pnnn/HZZ5/B1NQUxsbGcHV1xbBhw/Dvv/+KcZMmTUKdOnXEz9nZ2QgPD0f16tVhYGAACwsLNG3aFJs3b0ZKSorWfRcucXFxSEpKkrRZWFigXbt2OHnypMbjfvF8l3Y/GRkZYl/5+fmYPXs23N3dYWBggPLly6Nt27Y4cOCAZJ9xcXGQyWTw9vaWtGdkZEAmkyEpKalEPzMiIiIiIipbR48eRVpamrjs2rULANClSxeN8dnZ2ahWrRqioqKgVCo1xkyfPh3R0dGYN28ezp49i+nTp2PGjBmYO3fua82dhYN3jLe3N9LS0pCSkoLt27ejefPmGDZsGHx9fZGXlyfGpaam4uDBgxgyZAhiYmIAAAqFAvHx8YiLi8OOHTvE2MOHD2P27NmIi4uDiYkJJk+ejNmzZ2Pq1Kk4c+YM9uzZg/79+0subEua5+XLlzF79mwsWrQIEydOVIuLjY2V/HKkpaWhY8eOAJ4VDb766isMHToU7dq1w86dO3HmzBmoVCoYGBhg2rRpWvc/YMAAbNiwAXPnzsW5c+eQkJCAzp074+7du7C1tZXsb/jw4XB1dZW0devWTezr/PnzSEtLw44dO5CTkwMfHx/k5uZK9qfpfJd2P4UEQUD37t0xZcoUDBs2DGfPnkVSUhJsbW3RrFkzbNq0SRIvl8uRmJiIPXv2lPTHQ0RERERE7xgLCwsolUpx2bp1K6pXr46mTZtqjG/QoAG+//57dO/eHQqFQmPMwYMH0aFDB/j4+MDOzg6dO3dGmzZtcOTIkdeau/y19kavTKFQiNUka2tr1KtXD59++ilatmyJuLg49O3bF8CzC3JfX18MHDgQn376KWbNmgVDQ0N4eHhg3Lhx6NOnD06dOgUDAwP07t0boaGh4hdyy5YtGDRokKSyVbt27ZfO09bWFq1atcKuXbswffp0SZy5ubnW6tjq1auxatUqbN68Ge3btxfbP/nkE3z66adFDq/ZsmULfvzxR7Rr1w7As2E/Hh4e4vrn92lsbAy5XK41j8qVK4t5hoWFoX379jh37hxq1aolxmg736XZT6E1a9Zg3bp12LJlC/z8/MT2xYsX4+7du+jbty9at24NIyMjAICRkRG6du2KMWPG4M8//yyybyIiIiIievfl5uZi2bJl+N///geZTPbS/TRu3BiLFy/GhQsX4OjoiBMnTmD//v2YNWvWa8yWIw7eCy1atEDt2rWxYcMGAM/uWMfGxiIwMBDOzs5wcHDAunXrxPhx48ZBqVRi6NChGD9+PGQyGSIiIsT1SqUSu3fvxp07d15LfqdOncLBgwehr69fqu1WrlwJJycnSdHgeUX9AimVSmzbtg0PHjwo1T6LkpmZKT5G8fyxFHe+S2vFihVwdHSUFA0KDR8+HHfv3hWHLRWaNGkSTp48War95uTkICsrS7IQEREREVHZ27RpEzIyMhAcHPxK/YwZMwbdu3eHs7Mz9PT0ULduXYSFhaFHjx6vJ9H/j4WD94SzszNSUlIAAImJicjOzoaXlxcAIDAwECqVSoyVy+WIj4/H2rVrMXfuXMTHx8PAwEBcP2vWLNy5cwdKpRK1atXCgAEDsH379lLls3XrVhgbG8PAwADu7u64ffs2Ro4cqRYXEBAAY2NjyZKamgoAuHDhApycnCTxYWFhYpyNjY3W/S9evBgHDx5ExYoV0aBBA3zzzTdq8wOUlI2NDYyNjWFubo4VK1agffv2cHZ2FtcXd75L68KFC6hZs6bGdYXtFy5ckLRbWVlh2LBhGDdunOSRlaJERkbCzMxMXGxtbV86ZyIiIiIien1UKhXatm0LKyurV+pnzZo1WL58OVasWIHjx49j6dKl+OGHH7B06dLXlOkzLBy8JwRBEO/Ax8TEoFu3bpDLnz1pEhAQgAMHDuDSpUtivIuLC/z9/dG6dWvUr19f0peLiwtOnTqFw4cPIyQkBLdv34afn5/4GERJNG/eHMnJyfjzzz8RFBSE3r17w9/fXy1u9uzZSE5OlixF/XKMGzcOycnJ+Pbbb/Hw4UOtcV988QUuX76M33//HZ07d8bp06fRpEkTTJ06tcTHUOiPP/7AX3/9hbi4ODg6OmLhwoWS9SU536X1MrOcjh49Gnfu3BHnWChOeHg4MjMzxeXatWul3icREREREb1eV69eRWJiYqmuv7QZOXKkOOrA3d0dPXv2xDfffIPIyMjXkOn/YeHgPXH27FnY29vj3r172LhxIxYsWAC5XA65XA5ra2vk5eWpXVAWrtdER0cHDRo0QFhYGDZs2IC4uDioVCpcuXKlRPkYGRnBwcEBtWvXRkxMDP7880+Nd+GVSiUcHBwkS2FONWrUwPnz5yXxFhYWcHBwQOXKlYvNQU9PD02aNMHo0aOxc+dOTJkyBVOnTlWb2LA49vb2cHJyQlBQEPr27SuZ0LA057ukHB0dcfbsWY3rCtsdHR3V1pmbmyM8PByTJ09GdnZ2sftRKBQwNTWVLEREREREVLZiY2NRuXJl+Pj4vHJf2dnZ0NGRXtbr6upqfCvfq2Dh4D2we/dunDx5Ev7+/li+fDlsbGxw4sQJyV38mTNnIi4uDvn5+S+1DxcXFwDAo0ePSr2tjo4Oxo4di/Hjx+Px48cl3i4gIADnz5/H5s2bS71PTVxcXJCXl6f2WsjSGDx4ME6dOoWNGzcCwBs53927d8fFixfx66+/qq2bOXMmKlasiNatW2vcNjQ0FDo6Ovjxxx9LvV8iIiIiIipbBQUFiI2NRVBQkNpN3l69eiE8PFz8nJubK15/5Obm4saNG0hOTpa8tt7Pzw/fffcdfvvtN6SkpGDjxo2YNWsWOnXq9Frz5lsV3jE5OTlIT09Hfn4+bt26hYSEBERGRsLX1xe9evWCh4cHOnfuDDc3N8l2tra2CA8PR0JCQrGVq86dO8PT0xONGzeGUqnElStXEB4eDkdHR8mz/aXRpUsXjBw5EvPnz8eIESPE9oyMDKSnp0tiTUxMYGRkhO7du2PDhg3o3r07wsPD4eXlhSpVquDq1atYvXo1dHV1te6vWbNmCAgIQP369VGxYkWcOXMGY8eORfPmzV/pznq5cuXQr18/TJw4ER07doRKpXrl8/2i7t27Y+3atQgKCsL333+Pli1bIisrC/Pnz8eWLVuwdu1a8Y0KLzIwMMDkyZMxePDglz5GIiIiIiIqG4mJiUhNTUVISIjautTUVMnogZs3b6Ju3bri5x9++AE//PADmjZtiqSkJADA3LlzMWHCBAwaNAi3b9+GlZUVvv76a3z77bevNW+OOHjHJCQkwNLSEnZ2dvD29saePXvw008/YfPmzUhOTsaJEyc0ziVgZmaGli1blmjSPi8vL/z666/w8/ODo6MjgoKC4OzsjJ07d2p9tKE4crkcQ4YMwYwZMySjFnr37g1LS0vJMnfuXADP3pqwevVqzJkzB9u2bUPLli3h5OSEkJAQ2NraYv/+/UUew9KlS9GmTRvUrFkToaGh8PLywpo1a14q/+cNGTIEZ8+exYwZM17L+X6RTCbDmjVrMHbsWMyePRtOTk5o0qQJrl69iqSkJHTs2LHI7YOCglCtWrVS75eIiIiIiMpWmzZtIAiCxkeTk5KSEBcXJ362s7ODIAhqS2HRAHh2U3bOnDm4evUqHj9+jEuXLmHatGmlfuNdcWTCy8zSRkTvnaysrGdvVwhbAx1FubJOh4iIiIjog5IS9epzFrwthdcGmZmZJRqxzREHRERERERERKQVCwckkZqaCmNjY61LampqWadIREREREREbxEnRyQJKysrJCcnF7meiIiIiIiIPh4sHJCEXC6Hg4NDWadBRERERERE7wg+qkBEREREREREWrFwQERERERERERa8VEFoo/MqcleJXrlChEREREREcARB0RERERERERUBBYOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrvlWB6CPjNnEHdBTlyjoNIiIiIqKXlhLlU9YpfFQ44oCIiIiIiIiItGLhgIiIiIiIiIi0YuGAiIiIiIiIiLRi4YCIiIiIiIiItGLhgIiIiIiIiIi0YuGAiIiIiIiIiLRi4YCIiIiIiIg+ODdu3EBgYCAqVqwIQ0NDuLu749ixY0Vus3z5ctSuXRvlypWDpaUlQkJCcPfuXXH9zz//jCZNmqB8+fIoX748WrVqhSNHjrzpQylzLBxQmQoODoZMJoNMJoOenh6qVKmC1q1bIyYmBgUFBWrxXl5e0NXVxdGjRwEAOTk5cHV1Rf/+/dViR40aBXt7ezx48AD5+fmIioqCs7MzDA0NUaFCBTRq1AhLliwpdZ4ymQwVK1aEt7c3/vnnH0mcTCbDpk2bJJ8LF1NTUzRo0ACbN28GADRr1kyy/sWlWbNmAAA7OzvMmTNHLadJkyahTp06JcqfiIiIiOhjcv/+fXh6ekJPTw/bt2/HmTNnMHPmTJQvX17rNgcOHECvXr3Qp08fnD59GmvXrsWRI0fQr18/MSYpKQkBAQHYs2cPDh06BFtbW7Rp0wY3btx4G4dVZuRlnQCRt7c3YmNjkZ+fj1u3biEhIQHDhg3DunXrsGXLFsjlz76mqampOHjwIIYMGYKYmBg0aNAACoUC8fHx+Oyzz+Dv7w8vLy8AwOHDhzF79mwkJibCxMQE3377LRYtWoR58+ahfv36yMrKwrFjx3D//v1S5wkA6enpGD9+PHx9fZGamlrkdrGxsfD29kZWVhYWLFiAzp074/jx49iwYQNyc3MBANeuXUPDhg2RmJgIV1dXAIC+vn6pzyUREREREQHTp0+Hra2t+Pc7ANjb2xe5zaFDh2BnZ4ehQ4eK8V9//TWmT58uxixfvlyyzZIlS7B+/Xr8/vvv6NWr12s8gncLRxxQmVMoFFAqlbC2tka9evUwduxYbN68Gdu3b0dcXJwYFxsbC19fXwwcOBArV67E48ePAQAeHh4YN24c+vTpg4yMDDx58gS9e/dGaGgomjZtCgDYsmULBg0ahC5dusDe3h61a9dGnz59MGLEiFLnqVQqUadOHYwZMwbXrl3DnTt3itzO3NwcSqUSjo6OmDp1KvLy8rBnzx5UqFBB7M/CwgIAULFiRbGtQoUKpTyTREREREQEPPv7v379+ujSpQsqV66MunXr4ueffy5ym88++wzXrl3Dtm3bIAgCbt26hXXr1qFdu3Zat8nOzsbTp08/+L/dWTigd1KLFi1Qu3ZtbNiwAQAgCAJiY2MRGBgIZ2dnODg4YN26dWL8uHHjoFQqMXToUIwfPx4ymQwRERHieqVSid27dxd7kV9SDx8+xLJly+Dg4ICKFSuWaJu8vDyoVCoAb2c0QU5ODrKysiQLEREREdHH4PLly4iOjkaNGjWwY8cODBw4EEOHDsXSpUu1buPp6Ynly5ejW7du0NfXh1KphJmZGebPn691m9GjR8PKygqtWrV6E4fxzuCjCvTOcnZ2FucQSExMRHZ2tvgoQmBgIFQqFXr27AkAkMvliI+Ph4eHBwoKCnDgwAEYGBiIfc2aNQudO3eGUqmEq6srGjdujA4dOqBt27Ylzmfr1q0wNjYGADx69AiWlpbYunUrdHSKrr8FBARAV1cXjx8/RkFBAezs7NC1a9dSnYvRo0dj/Pjxkrbc3Fy4uLho3SYyMhKTJ08u1X6IiIiIiD4EBQUFqF+/vngzsW7dujh16hQWLlyIoKAgjducOXMGw4YNw7fffgsvLy+kpaVh5MiRGDBggHgD8HlRUVFYtWoVkpKSJNceHyKOOKB3liAIkMlkAICYmBh069ZNnO8gICAABw4cwKVLl8R4FxcX+Pv7o3Xr1qhfv76kLxcXF5w6dQqHDx9GSEgIbt++DT8/P/Tt27fE+TRv3hzJyclITk7GkSNH4OXlhbZt2+Lq1atFbjd79mwkJydj+/btcHFxwZIlS0o9lGnkyJHivguXAQMGFLlNeHg4MjMzxeXatWul2icRERER0fvK0tJS7SZbzZo1i5yfLDIyEp6enhg5ciRq1aoFLy8vLFiwADExMUhLS5PE/vDDD4iKisLOnTtRq1atN3IM7xIWDuiddfbsWdjb2+PevXvYuHEjFixYALlcDrlcDmtra+Tl5SEmJkayTeF6TXR0dNCgQQOEhYVhw4YNiIuLg0qlwpUrV0qUj5GRERwcHODg4IAGDRpgyZIlePToUbHPSimVSjg4OKBNmzaIjY1Ft27dcPv27ZKdhP+vUqVK4r4Ll+KKDwqFAqamppKFiIiIiOhj4OnpifPnz0vaLly4gKpVq2rdJjs7W200sa6uLoBnNzULzZgxA1OnTkVCQoLaDcsPFQsH9E7avXs3Tp48CX9/fyxfvhw2NjY4ceKE5I77zJkzERcXh/z8/JfaR2EF8tGjRy+1vUwmg46OjjhJY0k0bNgQHh4e+O67715qn0REREREVLxvvvkGhw8fRkREBP7991+sWLECixcvxuDBg8WY8PBwyZsQ/Pz8sGHDBkRHR+Py5cs4cOAAhg4dioYNG8LKygrAs7c1TJgwATExMbCzs0N6ejrS09Px8OHDt36MbxPnOKAyl5OTg/T0dMnrGCMjI+Hr64tevXrBw8MDnTt3hpubm2Q7W1tbhIeHIyEhAT4+PkXuo3PnzvD09ETjxo2hVCpx5coVhIeHw9HREc7OzqXKE3j2Xth58+bh4cOH8PPzK9XxhoWFoVOnThg1ahSsra1LtS0RERERERWvQYMG2LhxI8LDwzFlyhTY29tjzpw56NGjhxiTlpYmeXQhODgYDx48wLx58zB8+HCYm5ujRYsWktcxRkdHIzc3F507d5bsb+LEiZg0adIbP66ywsIBlbmEhARYWlpCLpejfPnyqF27Nn766ScEBQXh77//xokTJzQ+DmBmZoaWLVtCpVIVWzjw8vLCypUrERkZiczMTCiVSrRo0QKTJk3S+miDtjwBwMTEBM7Ozli7di2aNWtWquP19vaGvb09vvvuOyxYsKBU2xIRERERUcn4+vrC19dX6/rnX/1eKDQ0FKGhoVq3SUlJeQ2ZvX9kwvMPaxDRBysrKwtmZmawDVsDHUW5sk6HiIiIiOilpUQVfeOQilZ4bZCZmVmiudA4xwERERERERERacXCAX30UlNTYWxsrHUp6pUtREREREREHzrOcUAfPSsrKyQnJxe5noiIiIiI6GPFwgF99ORyORwcHMo6DSIiIiIioncSH1UgIiIiIiIiIq044oDoI3NqsleJZk4lIiIiIiICOOKAiIiIiIiIiIrAwgERERERERERacXCARERERERERFpxcIBEREREREREWnFwgERERERERERacXCARERERERERFpxdcxEn1k3CbugI6iXFmnQURERERUIilRPmWdwkePIw6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiI6L1248YNBAYGomLFijA0NIS7uzuOHTumNT44OBgymUxtcXV1FWPs7Ow0xgwePPhtHNI7ha9jJCIiIiIiovfW/fv34enpiebNm2P79u2wsLDAxYsXUb58ea3b/Pjjj4iKihI/5+XloXbt2ujSpYvYdvToUeTn54ufT506hdatW0tiPhYccUCvzaFDh6CrqwsfH+l7VlNSUiCTyaCrq4sbN25I1qWlpUEul0MmkyElJQWTJk3SWNV7fnnVfJ7PqXLlynjw4IFkXZ06dTBp0iTxc7NmzSCTybBq1SpJ3Jw5c2BnZyd+njRpEurUqaN1X8nJyQCApKQkyGQyZGRkaK10Fi6WlpZwdXVF//791fodNWoU7O3t1fInIiIiIvqYTJ8+Hba2toiNjUXDhg1hb2+PNm3aoHr16lq3MTMzg1KpFJdjx47h/v376N27txhjYWEhidm6dSuqV6+Opk2bvo3DeqewcECvjUqlQmhoKPbt24ebN2+qrbe2tkZ8fLykbenSpbC2thY/jxgxAmlpaeJiY2ODKVOmSNpeVz4A8ODBA/zwww/F9mVgYIDx48fj6dOnJd5/Sfz4449qxxYbGyt+/ueffxAfH4+4uDjs2LFD3O7w4cOYPXs24uLiYGJi8lpzIiIiIiJ6n2zZsgX169dHly5dULlyZdStWxc///xzqfpQqVRo1aoVqlatqnF9bm4uli1bhpCQkFLdzPxQsHBAr8XDhw+xevVqDBw4ED4+PoiLi1OLCQoKQmxsrKQtNjYWQUFB4mdjY2NJVU9XVxcmJiaStteVDwCEhoZi1qxZuH37dpH9BQQEICMjo9T/ABXnxUonAJibm4ufLSws4OHhgXHjxqFPnz7IyMjAkydP0Lt3b4SGhn6U1U4iIiIiouddvnwZ0dHRqFGjBnbs2IGBAwdi6NChWLp0aYm2v3nzJrZv346+fftqjdm0aZM4YvhjxMIBvRZr1qyBs7MznJycEBgYiJiYGAiCIIlp37497t+/j/379wMA9u/fj/v378PPz69M8gGeFQQcHBwwZcqUIvszNTXFuHHjMGXKFDx69Oi151uccePGQalUYujQoRg/fjxkMhkiIiKK3CYnJwdZWVmShYiIiIjoQ1NQUIB69eohIiICdevWRf/+/dGvXz8sXLiwRNsvXboU5ubm6Nixo9YYlUqFtm3bwsrK6jVl/X5h4YBeC5VKhcDAQACAt7c3MjMzsXfvXkmMnp6eeBEPADExMQgMDISenl6Z5AMAMpkMUVFRWLx4MS5dulRkn4MGDYKBgQFmzZr12vMtjlwuR3x8PNauXYu5c+ciPj4eBgYGRW4TGRkJMzMzcbG1tX1L2RIRERERvT2WlpZwcXGRtNWsWROpqanFbisIAmJiYtCzZ0/o6+trjLl69SoSExOLHJHwoWPhgF7Z+fPnceTIEQQEBAB4dpHbrVs3qFQqtdiQkBCsXbsW6enpWLt2LUJCQso0HwDw8vLC559/jgkTJhTZr0KhwJQpU/DDDz/gv//+e+15F8fFxQX+/v5o3bo16tevX2x8eHg4MjMzxeXatWtvIUsiIiIiorfL09MT58+fl7RduHBB63wFz9u7dy/+/fdf9OnTR2tMbGwsKleurHHS9Y8FCwf0ylQqFfLy8mBlZQW5XA65XI7o6GisX78emZmZklh3d3c4OzsjICAANWvWhJubW5nmUygqKgqrV6/G33//XWTfgYGBqFq1KqZNm6a2ztTUVGP/GRkZAJ7NZ/CqCo+nJBQKBUxNTSULEREREdGH5ptvvsHhw4cRERGBf//9FytWrMDixYsxePBgMSY8PBy9evVS21alUqFRo0Zar0sKCgrEedlK+nf4h4iFA3oleXl5iI+Px8yZM5GcnCwuJ06cgJWVFVauXKm2TUhICJKSkt7IaIOXyQcAGjZsiC+//BJjxowpsn8dHR1ERkYiOjoaKSkpknVOTk64fv06bt26JWk/fvw4DAwM8Mknn7zSsRERERERkboGDRpg48aNWLlyJdzc3DB16lTMmTMHPXr0EGPS0tLUHl3IzMzE+vXrixxtkJiYiNTU1Ddy7fI++XhLJvRabN26Fffv30efPn3U7qj7+/tDpVLB29tb0t6vXz906dIF5ubmZZLPgAEDNG773XffwdXVtdhKoo+PDxo1aoRFixahSpUqYruXlxecnJwQEBCAadOmQalU4vjx4xg/fjyGDRsGXV3dVz9AIiIiIiJS4+vrC19fX63rNb1lzczMDNnZ2UX226ZNG42TrH9sOOKAXknh+041DcP39/fHsWPH1Gbzl8vlqFSp0hsZ6lOSfP755x+N2zo6OiIkJARPnjwpdj/Tp09Xi5PL5di5cyc++eQTBAQEwM3NDRMnTsSwYcMwderUlzsgIiIiIiKiMiYTWD4h+ihkZWU9e7tC2BroKMqVdTpERERERCWSEvXxTkr4phReG2RmZpZoLjSOOCAiIiIiIiIirVg4oPdKamoqjI2NtS4leVcrERERERERlRwnR6T3ipWVFZKTk4tcT0RERERERK8PCwf0XpHL5XBwcCjrNIiIiIiIiD4afFSBiIiIiIiIiLTiiAOij8ypyV4lmjmViIiIiIgI4IgDIiIiIiIiIioCCwdEREREREREpBULB0RERERERESkFQsHRERERERERKQVCwdEREREREREpBXfqkD0kXGbuAM6inJlnQYRERHRK0mJ8inrFIg+GhxxQERERERERERasXBARERERERERFqxcEBEREREREREWrFwQERERERERERasXBARERERERERFqxcEBEREREREREWrFwQEREREREH5RJkyZBJpNJFmdnZ63xcXFxavEGBgZqcWfPnkX79u1hZmYGIyMjNGjQAKmpqW/yUIjeCfKyToCIiIiIiOh1c3V1RWJiovhZLi/60sfU1BTnz58XP8tkMsn6S5cu4fPPP0efPn0wefJkmJqa4vTp0xoLDEQfGo44+IgFBweLFVU9PT1UqVIFrVu3RkxMDAoKCtTivby8oKuri6NHjwIAcnJy4Orqiv79+6vFjho1Cvb29njw4AHy8/MRFRUFZ2dnGBoaokKFCmjUqBGWLFlS6jyfX7y9vcUYOzs7yGQyrFq1Sm17V1dXyGQyxMXFqcXLZDIYGRmhXr16WLt2rbh+0qRJqFOnjtac8vPzMXv2bLi7u8PAwADly5dH27ZtceDAAQDA3r17oaenh/3790u2e/ToEapVq4YRI0YAAJo1a6bx2AYMGCBu83y7kZERatSogeDgYPz1118lOn9EREREHyO5XA6lUikulSpVKjJeJpNJ4qtUqSJZP27cOLRr1w4zZsxA3bp1Ub16dbRv3x6VK1d+k4dB9E5g4eAj5+3tjbS0NKSkpGD79u1o3rw5hg0bBl9fX+Tl5YlxqampOHjwIIYMGYKYmBgAgEKhQHx8POLi4rBjxw4x9vDhw5g9ezbi4uJgYmKCyZMnY/bs2Zg6dSrOnDmDPXv2oH///sjIyCh1ns8vK1eulMTY2toiNjZW0nb48GGkp6fDyMhIrc8pU6YgLS0Nf//9Nxo0aIBu3brh4MGDxeYiCAK6d++OKVOmYNiwYTh79iySkpJga2uLZs2aYdOmTWjatClCQ0MRHByMR48eiduOGjUKhoaGmDZtmtjWr18/tWObMWOGZJ+xsbFIS0vD6dOnMX/+fDx8+BCNGjVCfHx8ic4fERER0cfm4sWLsLKyQrVq1dCjR49iHyl4+PAhqlatCltbW3To0AGnT58W1xUUFOC3336Do6MjvLy8ULlyZTRq1AibNm16w0dB9G5g4eAjp1AooFQqYW1tjXr16mHs2LHYvHkztm/fLrlDHxsbC19fXwwcOBArV67E48ePAQAeHh4YN24c+vTpg4yMDDx58gS9e/dGaGgomjZtCgDYsmULBg0ahC5dusDe3h61a9dGnz59xLvupcnz+aV8+fKSmB49emDv3r24du2a2BYTE4MePXpoHJpmYmICpVIJR0dHzJ8/H4aGhvj111+LzWXNmjVYt24d4uPj0bdvX/GYFi9ejPbt26Nv37549OgRIiIioK+vj9GjRwMA9uzZgyVLliA+Pl4ypK1cuXJqx2ZqairZp7m5OZRKJezs7NCmTRusW7cOPXr0wJAhQ3D//v0Sn0ciIiKij0GjRo0QFxeHhIQEREdH48qVK2jSpAkePHigMd7JyQkxMTHYvHkzli1bhoKCAjRu3BjXr18HANy+fRsPHz5EVFQUvL29sXPnTnTq1Alffvkl9u7d+zYPjahMsHBAalq0aIHatWtjw4YNAJ7dYY+NjUVgYCCcnZ3h4OCAdevWifHjxo2DUqnE0KFDMX78eMhkMkRERIjrlUoldu/ejTt37rzRvKtUqQIvLy8sXboUAJCdnY3Vq1cjJCSk2G3lcjn09PSQm5tbbOyKFSvg6OgIPz8/tXXDhw/H3bt3sWvXLhgYGCA+Ph6LFy/G5s2bERISgrFjx8LDw6P0B6fBN998gwcPHmDXrl0a1+fk5CArK0uyEBEREX0M2rZtiy5duqBWrVrw8vLCtm3bkJGRgTVr1miM/+yzz9CrVy/UqVMHTZs2xYYNG2BhYYFFixYBgPgYb4cOHfDNN9+gTp06GDNmDHx9fbFw4cK3dlxEZYWFA9LI2dkZKSkpAIDExERkZ2fDy8sLABAYGAiVSiXGyuVyxMfHY+3atZg7d67aHfVZs2bhzp07UCqVqFWrFgYMGIDt27eXKp+tW7fC2NhYsjxfnCgUEhKCuLg4CIKAdevWoXr16kXOVQAAubm5iIyMRGZmJlq0aFFsLhcuXEDNmjU1ritsv3DhAgCgfv36CA8Px5dffomKFSti3LhxatssWLBA7diWL19ebB6FMwMX/pxeFBkZCTMzM3GxtbUttk8iIiKiD5G5uTkcHR3x77//liheT08PdevWFeMrVaoEuVwOFxcXSVzNmjX5VgX6KLBwQBoJgiDOJBsTE4Nu3bqJw/0DAgJw4MABXLp0SYx3cXGBv78/Wrdujfr160v6cnFxwalTp3D48GGEhITg9u3b8PPzQ9++fUucT/PmzZGcnCxZnp9AsJCPjw8ePnyIffv2ISYmpsjRBqNHj4axsTHKlSuH6dOnIyoqCj4+PiXKRxCEEuc+YcIEFBQUYMyYMRofmejRo4fasbVv377EObw442+h8PBwZGZmisvzj3AQERERfUwePnyIS5cuwdLSskTx+fn5OHnypBivr6+PBg0aSN66ADy7WVS1atXXni/Ru4avYySNzp49C3t7e9y7dw8bN27E06dPER0dLa7Pz89HTEwMvvvuO7FNLpdrfc2Njo4OGjRogAYNGiAsLAzLli1Dz549MW7cONjb2xebj5GRERwcHIqNk8vl6NmzJyZOnIg///wTGzdu1Bo7cuRIBAcHw9jYGFWqVNF6Af4iR0dHnD17VuO6wnZHR0dJTs//74vMzMxKdGza9qXt/CkUCigUilL3S0RERPS+GzFiBPz8/FC1alXcvHkTEydOhK6uLgICAgAAvXr1grW1NSIjIwE8mzT7008/hYODAzIyMvD999/j6tWrkhtdI0eORLdu3fDFF1+gefPmSEhIwK+//oqkpKSyOESit4ojDkjN7t27cfLkSfj7+2P58uWwsbHBiRMnJHfEZ86cibi4OOTn57/UPgqHeT3/xoHXJSQkBHv37kWHDh3UJlB8XqVKleDg4AClUlniogEAdO/eHRcvXtQ4keLMmTNRsWJFtG7d+qVyL405c+bA1NQUrVq1euP7IiIiInqfXL9+HQEBAXByckLXrl1RsWJFHD58GBYWFgCevTEsLS1NjL9//z769euHmjVrol27dsjKysLBgwcljyZ06tQJCxcuxIwZM+Du7o4lS5Zg/fr1+Pzzz9/68RG9bRxx8JHLyclBeno68vPzcevWLSQkJCAyMhK+vr7o1asXPDw80LlzZ7i5uUm2s7W1RXh4OBISEood3t+5c2d4enqicePGUCqVuHLlCsLDw+Ho6Cg+p1/SPJ8nl8s1vo+3Zs2a+O+//1CuXLkS9a3N48ePkZycLGkzMTFB9+7dsXbtWgQFBeH7779Hy5YtkZWVhfnz52PLli1Yu3atxtc/apOdna12bAqFQlL0yMjIQHp6OnJycnDhwgUsWrQImzZtQnx8PMzNzV/lMImIiIg+OKtWrSpy/YujBGbPno3Zs2cX229ISEiJJt4m+tCwcPCRS0hIgKWlJeRyOcqXL4/atWvjp59+QlBQEP7++2+cOHECP//8s9p2ZmZmaNmyJVQqVbGFAy8vL6xcuVKcgFCpVKJFixaYNGmS1uH72vJ8npOTE86dO6cxvmLFiiXqtygXLlxA3bp1JW0tW7ZEYmIi1qxZgzlz5mD27NkYNGgQDAwM8NlnnyEpKQmenp6l2s/PP/+sdo69vLyQkJAgfu7duzcAwMDAANbW1vj8889x5MgR1KtX7yWPjoiIiIiIqGRkQmlmeSOi91ZWVtaztyuErYGO4tVGYxARERGVtZSokk1qTUTqCq8NMjMzYWpqWmw85zggIiIiIiIiIq1YOKAylZqaCmNjY60L34tLRERERERUtjjHAZUpKysrtQkIX1xPREREREREZYeFAypTcrkcDg4OZZ0GERERERERacFHFYiIiIiIiIhIK444IPrInJrsVaKZU4mIiIiIiACOOCAiIiIiIiKiIrBwQERERERERERasXBARERERERERFqxcEBEREREREREWrFwQERERERERERa8a0KRB8Zt4k7oKMoV9ZpEBER0VuWEuVT1ikQ0XuKIw6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrFg6IiIiIiD5ikyZNgkwmkyzOzs5FbrN27Vo4OzvDwMAA7u7u2LZtm2R9cHCwWp/e3t5v8jCI6A2Sl3UCRERERERUtlxdXZGYmCh+lsu1XyYcPHgQAQEBiIyMhK+vL1asWIGOHTvi+PHjcHNzE+O8vb0RGxsrflYoFG8meSJ64zjigN66Q4cOQVdXFz4+PpL2lJQUyGQy6Orq4saNG5J1aWlpkMvlkMlkSElJ0VgZf3EpifT0dISGhqJatWpQKBSwtbWFn58ffv/9dzHGzs5O7LNcuXJwd3fHkiVLJP0kJSVpzSM9PR2AtJovl8tRqVIlfPHFF5gzZw5ycnIk/TVr1gxhYWHiOSlqiYuLK+mpJyIiItJILpdDqVSKS6VKlbTG/vjjj/D29sbIkSNRs2ZNTJ06FfXq1cO8efMkcQqFQtJn+fLl3/RhENEbwsIBvXUqlQqhoaHYt28fbt68qbbe2toa8fHxkralS5fC2tpa/DxixAikpaWJi42NDaZMmSJpK05KSgo8PDywe/dufP/99zh58iQSEhLQvHlzDB48WBJb2PepU6cQGBiIfv36Yfv27Wp9nj9/XpJDWloaKleuLK53dXVFWloaUlNTsWfPHnTp0gWRkZFo3LgxHjx4oNafra2tpK/hw4eLfRQu3bp1K/ZYiYiIiIpy8eJFWFlZoVq1aujRowdSU1O1xh46dAitWrWStHl5eeHQoUOStqSkJFSuXBlOTk4YOHAg7t69+0ZyJ6I3j48q0Fv18OFDrF69GseOHUN6ejri4uIwduxYSUxQUBBiY2MRHh4utsXGxiIoKAhTp04FABgbG8PY2Fhcr6urCxMTEyiVyhLnMmjQIMhkMhw5cgRGRkZiu6urK0JCQiSxz/c9evRozJgxA7t27ULbtm0lcZUrV4a5ubnWfRZW8wHAysoK7u7uaN26NWrXro3p06dj2rRpknhdXV3JMRkbG0v6KEpOTo5kJENWVlax2xAREdHHp1GjRoiLi4OTkxPS0tIwefJkNGnSBKdOnYKJiYlafHp6OqpUqSJpq1KlijjKEnj2mMKXX34Je3t7XLp0CWPHjkXbtm3FkadE9H7hiAN6q9asWQNnZ2c4OTkhMDAQMTExEARBEtO+fXvcv38f+/fvBwDs378f9+/fh5+f32vL4969e0hISMDgwYMlRYNC2i7+CwoKsH79ety/fx/6+vqvJRdnZ2e0bdsWGzZseC39FYqMjISZmZm42Nravtb+iYiI6MPQtm1bdOnSBbVq1YKXlxe2bduGjIwMrFmz5qX77N69O9q3bw93d3d07NgRW7duxdGjR5GUlPT6Eieit4aFA3qrVCoVAgMDATyrRGdmZmLv3r2SGD09PbGoAAAxMTEIDAyEnp7ea8vj33//hSAIxc4YXGj06NEwNjaGQqFA586dUb58efTt21ctzsbGRhwNYWxsDFdX1xL17+zsjJSUlNIcQrHCw8ORmZkpLteuXXut/RMREdGHydzcHI6Ojvj33381rlcqlbh165ak7datW0WOiKxWrRoqVaqktU8ierexcEBvzfnz53HkyBEEBAQAeDZsv1u3blCpVGqxISEhWLt2LdLT07F27Vq1Rwde1YujHIozcuRIJCcnY/fu3WjUqBFmz54NBwcHtbg//vgDycnJ4vLiq4mKyqekEzqWlEKhgKmpqWQhIiIiKs7Dhw9x6dIlWFpaalz/2WefSSaSBoBdu3bhs88+09rn9evXcffuXa19EtG7jXMc0FujUqmQl5cHKysrsU0QBCgUCrVZeN3d3eHs7IyAgADUrFkTbm5uSE5Ofm251KhRAzKZDOfOnStRfKVKleDg4AAHBwesXbsW7u7uqF+/PlxcXCRx9vb2Rc5xoM3Zs2dhb29f6u2IiIiIXtWIESPg5+eHqlWr4ubNm5g4cSJ0dXXFmz29evWCtbU1IiMjAQDDhg1D06ZNMXPmTPj4+GDVqlU4duwYFi9eDOBZ4WHy5Mnw9/eHUqnEpUuXMGrUKDg4OMDLy6vMjpOIXh5HHNBbkZeXh/j4eMycOVNyR/7EiROwsrLCypUr1bYJCQlBUlLSax9tAAAVKlSAl5cX5s+fj0ePHqmtz8jI0Lqtra0tunXrJpm88VWcO3cOCQkJ8Pf3fy39EREREZXG9evXERAQACcnJ3Tt2hUVK1bE4cOHYWFhAQBITU2VvLGqcePGWLFiBRYvXozatWtj3bp12LRpE9zc3AA8m9z5n3/+Qfv27eHo6Ig+ffrAw8MDf/zxBxQKRZkcIxG9Go44oLdi69atuH//Pvr06QMzMzPJOn9/f6hUKnh7e0va+/Xrhy5durzUHfySmD9/Pjw9PdGwYUNMmTIFtWrVQl5eHnbt2oXo6GicPXtW67bDhg2Dm5sbjh07hvr164vtt2/fxpMnTySxFStWFOdnyMvLQ3p6OgoKCnD37l0kJSVh2rRpqFOnDkaOHPlGjpOIiIioKKtWrSpyvaYJDbt06YIuXbpojDc0NMSOHTteR2pE9I5g4YDeCpVKhVatWqkVDYBnhYMZM2aovS5QLpejUqVKbyynatWq4fjx4/juu+8wfPhwpKWlwcLCAh4eHoiOji5yWxcXF7Rp0wbffvutZB4DJycntdhDhw7h008/BQCcPn0alpaW0NXVhZmZGVxcXBAeHo6BAweyAk9ERERERO8kmVDaWeKI6L2UlZX17LWMYWugoyhX1ukQERHRW5YS5VPWKRDRO6Lw2iAzM7NEk6hzjgMiIiIiIiIi0oqFA/ogpaamwtjYWOuSmppa1ikSERERERG9FzjHAX2QrKysinx94/OvhCQiIiIiIiLtWDigD5JcLoeDg0NZp0FERERERPTe46MKRERERERERKQVRxwQfWROTfYq0cypREREREREAEccEBEREREREVERWDggIiIiIiIiIq1YOCAiIiIiIiIirVg4ICIiIiIiIiKtWDggIiIiIiIiIq34VgWij4zbxB3QUZQr6zSIiIhIi5Qon7JOgYhIgiMOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIiIiEgrFg6IiIiIiIiISCsWDoiIiIiIiIhIKxYOiIiIiIjeM1FRUZDJZAgLCysybs6cOXBycoKhoSFsbW3xzTff4MmTJ+L66Oho1KpVC6ampjA1NcVnn32G7du3v+Hsieh9w8LBO+rQoUPQ1dWFj4/0Pb4pKSmQyWTQ1dXFjRs3JOvS0tIgl8shk8mQkpKCSZMmQSaTFbkUJzg4GB07dpR8lslkiIqKksRt2rRJrT9BELB48WI0atQIxsbGMDc3R/369TFnzhxkZ2eLcffu3UNYWBiqVq0KfX19WFlZISQkBKmpqWq5yGQyDBgwQC3PwYMHQyaTITg4WC3+xcXb27vIY05KSir2vCUlJQEAHj9+jIkTJ8LR0REKhQKVKlVCly5dcPr0abE/Ozu7Ivt6PmcvLy/o6uri6NGjanm9+LMgIiKij9PRo0exaNEi1KpVq8i4FStWYMyYMZg4cSLOnj0LlUqF1atXY+zYsWKMjY0NoqKi8Ndff+HYsWNo0aIFOnToIPlbhoiIhYN3lEqlQmhoKPbt24ebN2+qrbe2tkZ8fLykbenSpbC2thY/jxgxAmlpaeJiY2ODKVOmSNpehoGBAaZPn4779+8XGdezZ0+EhYWhQ4cO2LNnD5KTkzFhwgRs3rwZO3fuBPCsaPDpp58iMTERCxcuxL///otVq1bh33//RYMGDXD58mVJn7a2tli1ahUeP34stj158gQrVqzAJ598opaDt7e35HjT0tKwcuXKIvNu3LixJL5r165q/TRu3Bg5OTlo1aoVYmJiMG3aNFy4cAHbtm1DXl4eGjVqhMOHDwN49h/3wu3Wr18PADh//rzY9uOPPwIAUlNTcfDgQQwZMgQxMTHF/BSIiIjoY/Tw4UP06NEDP//8M8qXL19k7MGDB+Hp6YmvvvoKdnZ2aNOmDQICAnDkyBExxs/PD+3atUONGjXg6OiI7777DsbGxuLfMUREAAsH76SHDx9i9erVGDhwIHx8fBAXF6cWExQUhNjYWElbbGwsgoKCxM/GxsZQKpXioqurCxMTE0nby2jVqhWUSiUiIyO1xqxZswbLly/HypUrMXbsWDRo0AB2dnbo0KEDdu/ejebNmwMAxo0bh5s3byIxMRFt27bFJ598gi+++AI7duyAnp4eBg8eLOm3Xr16sLW1xYYNG8S2DRs24JNPPkHdunXV8lAoFJLjVSqVxf5HVl9fXxJvaGio1o++vj7mzJmDQ4cOYevWrejatSuqVq2Khg0bYv369ahZsyb69OkDQRBgYWEhblehQgUAQOXKlcU2MzMzAM9+fr6+vhg4cCBWrlwpKY4QERERAc9GWfr4+KBVq1bFxjZu3Bh//fWXWCi4fPkytm3bhnbt2mmMz8/Px6pVq/Do0SN89tlnrzVvInq/sXDwDlqzZg2cnZ3h5OSEwMBAxMTEQBAESUz79u1x//597N+/HwCwf/9+3L9/H35+fm88P11dXURERGDu3Lm4fv26xpjly5fDyckJHTp0UFsnk8lgZmaGgoICrFq1Cj169FArYhgaGmLQoEHYsWMH7t27J1kXEhIiKZrExMSgd+/er+HISmfFihVo3bo1ateuLWnX0dHBN998gzNnzuDEiRMl6ksQBMTGxiIwMBDOzs5wcHDAunXrXim/nJwcZGVlSRYiIiJ6f61atQrHjx8v8ubN87766itMmTIFn3/+OfT09FC9enU0a9ZM8qgCAJw8eRLGxsZQKBQYMGAANm7cCBcXlzdxCET0nmLh4B2kUqkQGBgI4NlQ+8zMTOzdu1cSo6enJxYVgGcXz4GBgdDT03srOXbq1Al16tTBxIkTNa6/ePEinJyciuzjzp07yMjIQM2aNTWur1mzJgRBwL///itpDwwMxP79+3H16lVcvXoVBw4cEM/Xi7Zu3QpjY2PJEhERUYIjLN6FCxeKzL0wpiQSExORnZ0NLy8vAM+OUaVSvVJ+kZGRMDMzExdbW9tX6o+IiIjKzrVr1zBs2DAsX74cBgYGJdomKSkJERERWLBgAY4fP44NGzbgt99+w9SpUyVxTk5OSE5Oxp9//omBAwciKCgIZ86ceROHQUTvKXlZJ0BS58+fx5EjR7Bx40YAgFwuR7du3aBSqdCsWTNJbEhICBo3boyIiAisXbsWhw4dQl5e3lvLdfr06WjRogVGjBihtu7FERJFKU0sAFhYWIiPcAiCAB8fH1SqVEljbPPmzREdHS1pK3xc4HUobe7axMTEoFu3bpDLn/1KBgQEYOTIkbh06RKqV6/+Un2Gh4fjf//7n/g5KyuLxQMiIqL31F9//YXbt2+jXr16Ylt+fj727duHefPmIScnB7q6upJtJkyYgJ49e6Jv374AAHd3dzx69Aj9+/fHuHHjoKPz7B6ivr4+HBwcAAAeHh44evQofvzxRyxatOgtHR0RvetYOHjHqFQq5OXlwcrKSmwTBAEKhQLz5s2TxLq7u8PZ2RkBAQGoWbMm3NzckJyc/NZy/eKLL+Dl5YXw8HDJmwEAwNHREefOnStyewsLC5ibm+Ps2bMa1589exYymUz8D9nzQkJCMGTIEADA/Pnzte7DyMhI4/avg6OjY5G5F8YU5969e9i4cSOePn0qKXLk5+cjJiYG33333Uvlp1AooFAoXmpbIiIiere0bNkSJ0+elLT17t0bzs7OGD16tFrRAACys7PF4kChwriibn4UFBQgJyfnNWRNRB8KPqrwDsnLy0N8fDxmzpyJ5ORkcTlx4gSsrKw0vg0gJCQESUlJCAkJKYOMn71D+Ndff8WhQ4ck7V999RUuXLiAzZs3q20jCAIyMzOho6ODrl27YsWKFUhPT5fEPH78GAsWLICXl5fGEQLe3t7Izc3F06dPxeH9b1v37t2RmJioNo9BQUEBZs+eDRcXF7X5DzRZvnw5bGxscOLECcnPfebMmYiLi0N+fv6bOgQiIiJ6T5iYmMDNzU2yGBkZoWLFinBzcwMA9OrVC+Hh4eI2fn5+iI6OxqpVq3DlyhXs2rULEyZMgJ+fn1hACA8Px759+5CSkoKTJ08iPDwcSUlJ6NGjR5kcJxG9mzji4B2ydetW3L9/H3369BFn2i/k7+8PlUoFb29vSXu/fv3QpUsXmJubv8VM/4+7uzt69OiBn376SdLetWtXbNy4EQEBARg/fjzatGkDCwsLnDx5ErNnz0ZoaCg6duyIiIgI/P7772jdujVmzJgBNzc3XLlyBePHj8fTp0+1jibQ1dUV7+prqrAXysnJUStKyOVyrY82lMY333yDzZs3w8/PDzNnzkSjRo1w69YtRERE4OzZs0hMTIRMJiu2H5VKhc6dO4v/0S9ka2uL8PBwJCQkwMfHBwCQmZmpNqqkYsWKfASBiIiIkJqaKhlhMH78eMhkMowfPx43btyAhYUF/Pz8JKMZb9++jV69eiEtLQ1mZmaoVasWduzYgdatW5fFIRDRO4qFg3eISqVCq1at1IoGwLPCwYwZM9Rmxn9dF8GvYsqUKVi9erWkTSaTYcWKFVi8eLE43F4ul6NGjRro1auXOEqgYsWKOHz4MKZMmYKvv/4a6enpqFChAtq2bYtly5bhk08+0bpfU1PTYnNLSEiApaWlpM3JyanYxyhKwsDAALt370ZERATGjh2Lq1evwsTEBM2bN8fhw4fVCgGa/PXXXzhx4gR+/vlntXVmZmZo2bIlVCqVWDhISkpSe+1knz59sGTJklc+HiIiInq/JCUlFflZLpdj4sSJWiezBvDKkzET0cdBJryu2d2I6J2WlZX17O0KYWugoyhX1ukQERGRFilRPmWdAhF94AqvDTIzM0t0Q5ZzHBARERERERGRViwcfMRSU1NhbGysdUlNTS3rFN+Y5cuXaz1uV1fXsk6PiIiIiIjoncE5Dj5iVlZWRb6+8flXQn5o2rdvj0aNGmlcp6en95azISIiIiIienexcPARk8vlcHBwKOs0yoSJiQlMTEzKOg0iIiIiIqJ3Hh9VICIiIiIiIiKtWDggIiIiIiIiIq34qALRR+bUZK8SvXKFiIiIiIgI4IgDIiIiIiIiIioCCwdEREREREREpBULB0RERERERESkFQsHRERERERERKQVCwdEREREREREpBXfqkD0kXGbuAM6inJlnQYREZWhlCifsk6BiIjeIxxxQERERERERERasXBARERERERERFqxcEBEREREREREWrFwQERERERERERasXBARERERERERFqxcEBEREREREREWrFwQERERESIjo5GrVq1YGpqClNTU3z22WfYvn17kdvMmTMHTk5OMDQ0hK2tLb755hs8efJEXL9v3z74+fnBysoKMpkMmzZtesNHQUREbwILB++hQ4cOQVdXFz4+0ncwp6SkQCaTQVdXFzdu3JCsS0tLg1wuh0wmQ0pKCiZNmgSZTFbkUhLp6ekYNmwYHBwcYGBggCpVqsDT0xPR0dHIzs6WxB48eBDt2rVD+fLlYWBgAHd3d8yaNQv5+flq/W7duhVNmzaFiYkJypUrhwYNGiAuLk7j8RYuJiYmcHV1xeDBg3Hx4kVJbH5+PqKiouDs7AxDQ0NUqFABjRo1wpIlS0p0nMHBweJ+9PX14eDggClTpiAvLw8AkJSUJMnFwsIC7dq1w8mTJ9X66dixo9o5DA0NRbVq1aBQKGBraws/Pz/8/vvvYoydnZ3Gn1FUVFSJ8iciIiqOjY0NoqKi8Ndff+HYsWNo0aIFOnTogNOnT2uMX7FiBcaMGYOJEyfi7NmzUKlUWL16NcaOHSvGPHr0CLVr18b8+fPf1mEQEdEbwMLBe0ilUiE0NBT79u3DzZs31dZbW1sjPj5e0rZ06VJYW1uLn0eMGIG0tDRxsbGxwZQpUyRtxbl8+TLq1q2LnTt3IiIiAn///TcOHTqEUaNGYevWrUhMTBRjN27ciKZNm8LGxgZ79uzBuXPnMGzYMEybNg3du3eHIAhi7Ny5c9GhQwd4enrizz//xD///IPu3btjwIABGDFihFoeiYmJSEtLw4kTJxAREYGzZ8+idu3akgvvyZMnY/bs2Zg6dSrOnDmDPXv2oH///sjIyCj2OAt5e3sjLS0NFy9exPDhwzFp0iR8//33kpjz588jLS0NO3bsQE5ODnx8fJCbm6u1z5SUFHh4eGD37t34/vvvcfLkSSQkJKB58+YYPHiwJPbFn09aWhpCQ0NLnD8REVFR/Pz80K5dO9SoUQOOjo747rvvYGxsjMOHD2uMP3jwIDw9PfHVV1/Bzs4Obdq0QUBAAI4cOSLGtG3bFtOmTUOnTp3e1mEQEdEbIC/rBKh0Hj58iNWrV+PYsWNIT09HXFycpLIPAEFBQYiNjUV4eLjYFhsbi6CgIEydOhUAYGxsDGNjY3G9rq4uTExMoFQqS5zLoEGDIJfLcezYMRgZGYnt1apVQ4cOHcRiwKNHj9CvXz+0b98eixcvFuP69u2LKlWqoH379lizZg26deuGa9euYfjw4QgLC0NERIQYO3z4cOjr62Po0KHo0qULGjVqJK6rWLGimHe1atXg5+eHli1bok+fPrh06RJ0dXWxZcsWDBo0CF26dBG3q127domPFQAUCoW4n4EDB2Ljxo3YsmWL5DxXrlwZ5ubmUCqVCAsLQ/v27XHu3DnUqlVL6zmUyWQ4cuSI5By6uroiJCREElvanw8REdHLys/Px9q1a/Ho0SN89tlnGmMaN26MZcuW4ciRI2jYsCEuX76Mbdu2oWfPnm85WyIietM44uA9s2bNGjg7O8PJyQmBgYGIiYmR3K0HgPbt2+P+/fvYv38/AGD//v24f/8+/Pz8Xlsed+/exc6dOzF48GDJBe/zCh932LlzJ+7evatxtICfnx8cHR2xcuVKAMC6devw9OlTjbFff/01jI2NxVhtdHR0MGzYMFy9ehV//fUXAECpVGL37t24c+dOqY6zKIaGhlpHE2RmZmLVqlUAAH19fY0x9+7dQ0JCgtZzaG5u/kr55eTkICsrS7IQEREV5eTJkzA2NoZCocCAAQOwceNGuLi4aIz96quvMGXKFHz++efQ09ND9erV0axZM7UbGkRE9P5j4eA9o1KpEBgYCODZ0PnMzEzs3btXEqOnpycWFQAgJiYGgYGB0NPTe215/PvvvxAEAU5OTpL2SpUqiaMZRo8eDQC4cOECAKBmzZoa+3J2dhZjLly4ADMzM1haWqrF6evro1q1amJsUZydnQE8exQAAGbNmoU7d+5AqVSiVq1aGDBgQLETPmkjCAISExOxY8cOtGjRQrLOxsYGxsbGMDc3x4oVK9C+fXsxlxcVnkNt6180evRo8dwWLn/88YfW+MjISJiZmYmLra1tyQ+SiIg+Sk5OTkhOTsaff/6JgQMHIigoCGfOnNEYm5SUhIiICCxYsADHjx/Hhg0b8Ntvv4mjG4mI6MPBRxXeI+fPn8eRI0ewceNGAIBcLke3bt2gUqnQrFkzSWxISAgaN26MiIgIrF27FocOHRIn8nuTjhw5goKCAvTo0QM5OTmSdS+OjHiTCvdVOOrBxcUFp06dwl9//YUDBw6IszwHBweXeILErVu3wtjYGE+fPkVBQQG++uorTJo0SRLzxx9/oFy5cjh8+DAiIiKwcOHCYnMsqZEjRyI4OFjS9vy8FS8KDw/H//73P/FzVlYWiwdERFSkwgmAAcDDwwNHjx7Fjz/+iEWLFqnFTpgwAT179kTfvn0BAO7u7nj06BH69++PcePGQUeH96eIiD4ULBy8R1QqFfLy8mBlZSW2CYIAhUKBefPmSWLd3d3h7OyMgIAA1KxZE25ubkhOTn5tuTg4OEAmk+H8+fOS9mrVqgF4Noy/kKOjIwDg7NmzaNy4sVpfZ8+eFYdBOjo6IjMzEzdv3pQcJwDk5ubi0qVLaN68ebH5nT17FgBgb28vtuno6KBBgwZo0KABwsLCsGzZMvTs2RPjxo2TxGnTvHlzREdHQ19fH1ZWVpDL1X997O3tYW5uDicnJ9y+fRvdunXDvn37NPZXo0YNyGQynDt3rth9A89GcxT+MVcSCoUCCoWixPFEREQvKigoULsRUCg7O1utOKCrqwvg7d4sICKiN4+l4PdEXl4e4uPjMXPmTCQnJ4vLiRMnYGVlpfG5/5CQECQlJalNsvc6VKxYEa1bt8a8efPw6NGjImPbtGmDChUqYObMmWrrtmzZgosXLyIgIAAA4O/vDz09PY2xCxcuxKNHj8RYbQoKCvDTTz/B3t4edevW1RpXWKwoLv9CRkZGcHBwwCeffKKxaPCiwYMH49SpU+IIkRdVqFABXl5emD9/vsYcSvPGByIiolcVHh6Offv2ISUlBSdPnkR4eDiSkpLQo0cPAECvXr0kEwL7+fkhOjoaq1atwpUrV7Br1y5MmDABfn5+YgHh4cOH4t8sAHDlyhUkJycjNTX1rR8fERG9PI44eE9s3boV9+/fR58+fWBmZiZZ5+/vD5VKBW9vb0l7v3790KVLl1eeZE+bBQsWwNPTE/Xr18ekSZNQq1Yt6Ojo4OjRozh37hw8PDwAPLvgXrRoEbp3747+/ftjyJAhMDU1xe+//46RI0eic+fO6Nq1KwDgk08+wYwZMzB8+HAYGBigZ8+e0NPTw+bNmzF27FgMHz5c8kYF4NlEjenp6cjOzsapU6cwZ84cHDlyBL/99pv4h0vnzp3h6emJxo0bQ6lU4sqVKwgPD4ejo2OJ5xgorXLlyqFfv36YOHEiOnbsKD428bz58+fD09MTDRs2xJQpU1CrVi3k5eVh165diI6OFkdOAMCDBw+Qnp6utg9TU9M3kj8REX1cbt++jV69eiEtLQ1mZmaoVasWduzYgdatWwMAUlNTJSMMxo8fD5lMhvHjx+PGjRuwsLCAn58fvvvuOzHm2LFjkpGChY/QBQUFIS4u7u0cGBERvTIWDt4T/4+9O4+rMf3/B/467cupyNYiayqKGLLOUKNkDUOSicgyyL5OxpKsY4sZ2xinGmNJlowxn8nase9kQgjTZChZi1Cpfn/0O/e32zmnTokYr+fjcT9G97Xc7+s+99D9Ptd93TKZDO7u7kpJA6AgcbBo0SKlVfN1dHRQuXLldxZT3bp1cfHiRcyfPx9BQUH4999/oa+vjwYNGmDSpEkYOXKkULd3796IjY3FvHnz8MUXX+DVq1eoV68evvvuO4wbN050Uz1u3DjUqVMHS5YswYoVK5CbmwtHR0esWbMGgwYNUorD3d0dQMFNdM2aNeHm5oZ169aJpvV7enpiy5YtWLBgAdLT02FhYYEvv/wSwcHBGs0eKK1Ro0Zh2bJl2LZtm5AcKaxOnTq4cOEC5s2bh4kTJyIlJQVVqlRB06ZNsWbNGlHdmTNnYubMmaJ933zzTZHrKBAREWlKJpMVWS6Xy0U/6+joYNasWZg1a5baNq6urnxsgYjoP0CSz7/NiT4JGRkZBW9XGBcFLX2j8g6HiIjKUdLCLuUdAhERlSPFvUF6erpGM5i5xgERERERERERqcXEAamUnJwMqVSqdvsvLWr0KY2ViIiIiIiopLjGAalkZWVV5Osb33xV4sfsUxorERERERFRSTFxQCrp6OiIFhf8L/uUxkpERERERFRSfFSBiIiIiIiIiNTijAOiT8zl2Z4arZxKREREREQEcMYBERERERERERWBiQMiIiIiIiIiUouJAyIiIiIiIiJSi4kDIiIiIiIiIlKLiQMiIiIiIiIiUouJAyIiIiIiIiJSi69jJPrEOM3aCy19o/IOg4joPytpYZfyDoGIiKhMccYBEREREREREanFxAERERERERERqcXEARERERERERGpxcQBEREREREREanFxAERERERERERqcXEAREREdE7tGbNGjRq1AimpqYwNTVFq1at8Oeff2rUNjIyEhKJBD169BDtz8/Px8yZM2FpaQlDQ0O4u7sjMTHxHURPRETExAERERHRO1W9enUsXLgQ58+fx7lz5/Dll1+ie/fuuHLlSpHtkpKSMGnSJHzxxRdKZYsWLcIPP/yAtWvX4vTp0zA2NoanpydevXr1roZBRESfsA82cTBw4ECl7DoAyOVySCQSPH36FACQm5uL0NBQNGzYEAYGBqhYsSI6deqE48ePi9oFBwejcePGSv0lJSVBIpEgLi5O1L9iq1KlCjp37oz4+PgSj+HkyZPQ1tZGly7K73NWHFexVapUCR06dMDFixeFOq6urkK5gYEBGjRogNWrVwvlERERQrmWlhYsLS3h4+OD5ORkjWMsy2NcuXIFffr0QZUqVaCvrw87OzvMnDkTL168UKp78eJFeHt7o1q1ajAwMEC9evUwdOhQ3LhxQ+X5KbydOnUKQMFnv3DhQjg4OMDQ0BDm5uZo0aIF1q9fLxznwYMHGDFiBGrUqAF9fX1YWFjA09NT6fpQ59KlS/Dy8kLVqlVhYGCAWrVqwcfHB2lpaQgODlYbo2JT2LJlC7S1tREYGKh0jOKuueKOERwcrNFYiIiofHTr1g2dO3dGvXr1YGdnh3nz5kEqlQr/nqmSm5uLr7/+GrNnz0adOnVEZfn5+Vi+fDmmT5+O7t27o1GjRtiwYQPu3buHXbt2vePREBHRp+iDTRxoIj8/H3379kVISAjGjh2LhIQEyOVy2NjYwNXV9a3+8bx+/TpSUlKwd+9eZGVloUuXLsjOzi5RHzKZDKNHj8aRI0dw7949lXUOHDggHOf58+fo1KmTkBQBgKFDhyIlJQVXr15Fnz59EBgYiC1btgjlpqamSElJwd27d7Fjxw5cv34d3t7eJYqzLI5x6tQptGjRAtnZ2fjjjz9w48YNzJs3DxEREfDw8BCduz179qBly5bIysrCpk2bkJCQgI0bN8LMzAwzZsxQeX4Kb02bNgUAzJ49G6GhoZgzZw6uXr2K2NhYDBs2THT+evXqhYsXL+KXX37BjRs3sHv3bri6uuLRo0fFnpcHDx6gffv2MDc3x969e5GQkIDw8HBYWVkhMzMTkyZNEsVVvXp1hISEiPYpyGQyTJkyBVu2bFH7bZC6a65wf8uXLxc+D8U2adKkYsdCREQfhtzcXERGRiIzMxOtWrVSWy8kJARVq1bF4MGDlcr+/vtvpKamwt3dXdhnZmaGFi1a4OTJk+8kbiIi+rTplHcAbyMqKgrbt2/H7t270a1bN2H/unXr8OjRIwwZMgQeHh4wNjYucd9Vq1ZFhQoVYGFhgXHjxsHLywvXrl1Do0aNNGr//PlzbN26FefOnUNqaioiIiIwbdo0pXqVKlWChYUFLCwssGTJErRp0wanT5+Gp6cnAMDIyAgWFhYACmZNbN68Gbt374avry+Agm+jFeWWlpYYPHgwxowZg4yMDJiammoU69seIz8/H4MHD0b9+vWxc+dOaGkV5KNq1qwJOzs7NGnSBKGhoZg6dSpevHiBQYMGoXPnzoiOjhZiqF27Nlq0aCG66S98flTZvXs3Ro4cKUpiODs7C39++vQpjh49Crlcjnbt2gkxNW/eXKPzcvz4caSnp2P9+vXQ0dER4nRzcxPqSKVS4c/a2towMTFRivfvv//GiRMnsGPHDsTGxmLnzp3o16+f0vE0uebMzMxEnwcREX0c4uPj0apVK7x69QpSqRTR0dFo0KCByrrHjh2DTCYTZkO+KTU1FQBQrVo10f5q1aoJZURERGXpo55xsHnzZtjZ2YmSBgoTJ07Eo0ePsH///rc6Rnp6OiIjIwEAenp6GreLioqCg4MD7O3t4efnh7CwMOTn5xfZxtDQEACKnNlgaGiotjwtLQ3R0dHQ1taGtra2xrG+7THi4uJw9epVTJgwQUgaKDg7O8Pd3V2YwbB37148fPgQU6ZMUdl/hQoVNI7TwsIChw4dwoMHD1SWS6VSSKVS7Nq1C1lZWRr3W7j/169fIzo6utjPrijh4eHo0qULzMzM4OfnB5lMVmT90l5zb8rKykJGRoZoIyKi8mFvb4+4uDicPn0aI0aMgL+/P65evapU79mzZ+jfvz9+/vlnVK5cuRwiJSIiUvZBzzjYs2eP6BtdoGCKn8KNGzdQv359lW0V+xXPzJdU9erVAQCZmZkAAC8vLzg4OGjcXiaTwc/PDwDQsWNHpKen4/Dhw3B1dVVZ/+nTp5gzZw6kUqnKb8Rzc3OxZcsW/PXXXxg2bJiwPz09HVKpFPn5+cJaAmPGjCnVLIvSHkNxjov6LI4dOwYAworPmp7L1q1bKyUjnj9/DgBYtmwZevfuDQsLCzg6OqJ169bo3r07OnXqBADQ0dFBREQEhg4dirVr1+Kzzz5Du3bt0LdvX41mjrRs2RLTpk1Dv379MHz4cDRv3hxffvklBgwYoPQtjzp5eXmIiIjAjz/+CADo27cvJk6ciL///hu1a9cW1X3ba+5NCxYswOzZs0vdnoiIyo6enh5sbW0BAE2bNsXZs2exYsUK/PTTT6J6t27dQlJSkuhLkby8PAAF/65dv35dmHV2//59WFpaCvXu37+vcj0nIiKit/VBzzhwc3NDXFycaCu88B2At/omuChHjx7F+fPnERERATs7O6xdu1bjttevX8eZM2eEqf46Ojrw8fFR+U1z69atIZVKUbFiRVy6dAlbt24V3ZSuXr0aUqkUhoaGGDp0KMaPH48RI0YI5SYmJoiLi8O5c+ewdOlSfPbZZ5g3b16JxlpWx9Dksyjp57V161ala0ChQYMGuHz5Mk6dOoWAgACkpaWhW7duGDJkiFCnV69euHfvHnbv3o2OHTtCLpfjs88+Q0REhEbHnzdvHlJTU7F27Vo4Ojpi7dq1cHBw0HixzP379yMzMxOdO3cGAFSuXBkeHh4ICwtTqvs215wqQUFBSE9PF7Y7d+68VX9ERFR28vLyVM6GU/wbU/jfPS8vL+F3IhsbG9SuXRsWFhY4ePCg0C4jIwOnT58uct0EIiKi0vqgZxwYGxsL2XmFf//9V/iznZ0dEhISVLZV7LezswNQsMBfenq6Uj3FM/VmZmai/bVr10aFChVgb2+PtLQ0+Pj44MiRIxrFLZPJ8Pr1a1hZWQn78vPzoa+vj5UrV4qOtXXrVjRo0ACVKlVSOU3/66+/xnfffQdDQ0NYWloqffuupaUlnKP69evj1q1bGDFiBH799VeNYi2LYyjOcUJCApo0aaLUf0JCglBH8d9r165p9MuNjY2N0jXwZmwuLi5wcXHBuHHjsHHjRvTv3x/fffed8I2+gYEBPDw84OHhgRkzZmDIkCGYNWsWBg4cWOzxgYJ1Fry9veHt7Y358+ejSZMmWLJkCX755Zdi28pkMjx+/Fh4DAUo+GXxr7/+wuzZs0Xn+m2uOVX09fWhr69f6vZERFQ2goKC0KlTJ9SoUQPPnj3D5s2bIZfLsXfvXgDAgAEDYG1tjQULFsDAwABOTk6i9orfDwrvHzduHObOnYt69eqhdu3amDFjBqysrFS+kYqIiOhtfdAzDorTt29fJCYm4vfff1cqW7p0KSpVqgQPDw8ABc8W/vvvv7h//76o3oULF2BgYIAaNWqoPU5gYCAuX74sWsxPndevX2PDhg1YunSp6NuCS5cuwcrKSvS2AqDgxrhu3bpqn+03MzODra0trK2tlW7oVfn222+xdetWXLhwodi6ZXWMxo0bw8HBAaGhocJ0SoVLly7hwIEDwuyLDh06oHLlyli0aJHKvt9cHLGkFAtNKab7q6tTVHlR9PT0ULduXY3aP3r0CL/99hsiIyNF18LFixfx5MkT7Nu3T23bklxzRET0YUtLS8OAAQNgb2+P9u3b4+zZs9i7d6/wO0pycrLoTTyamDJlCkaPHo1hw4bBxcUFz58/R0xMDAwMDN7FEIiI6BP3Qc84KE7fvn2xbds2+Pv7Y/HixWjfvj0yMjKwatUq7N69G9u2bROew/f09IS9vT18fX0xd+5cWFhY4MKFC5g+fTrGjh1b5GKCRkZGGDp0KGbNmoUePXpAIpGorbtnzx48efIEgwcPVprF0KtXL8hkMgwfPrxsToAKNjY26NmzJ2bOnIk9e/a8l2NIJBLIZDJ4eHigV69eCAoKgoWFBU6fPo2JEyeiVatWGDduHICCWSTr16+Ht7c3vLy8MGbMGNja2uLhw4eIiopCcnKysDAgUHDz/eYK0RUqVICBgQF69+6NNm3aoHXr1rCwsMDff/+NoKAg2NnZwcHBAY8ePYK3tzcCAgLQqFEjmJiY4Ny5c1i0aBG6d+9e7Dj37NmDyMhI9O3bF3Z2dsjPz8fvv/+O//3vfwgPDy+2/a+//opKlSqhT58+StdM586dIZPJ0LFjR5VtS3LNERHRh624RXHlcnmR5aoer5NIJAgJCUFISMhbREZERKSZj3rGgUQiQVRUFKZNm4bQ0FDY29vjiy++wD///AO5XC6arqejo4N9+/ahRo0a8PX1hZOTE2bNmoWxY8dizpw5xR5r1KhRSEhIwLZt24qsJ5PJ4O7urpQ0AAoSB+fOncNff/1V4rGWxPjx4/HHH3/gzJkz7+0YrVu3xqlTp6CtrY1OnTrB1tYWQUFB8Pf3x/79+0VT5rt3744TJ05AV1cX/fr1g4ODA3x9fZGeno65c+eKjuPu7g5LS0vRtmvXLgAFyaDff/8d3bp1g52dHfz9/eHg4IB9+/ZBR0cHUqkULVq0QGhoKNq2bQsnJyfMmDEDQ4cOxcqVK4sdY4MGDWBkZISJEyeicePGaNmyJaKiorB+/Xr079+/2PZhYWHo2bOnypv+Xr16Yffu3Xj48KHa9ppec0RERERERO+SJP9drS5IRB+UjIwMmJmZwWZcFLT0jco7HCKi/6ykhV3KOwQiIqIiKe4N0tPTYWpqWmz9j3rGARERERERERG9W0wclFBycjKkUqnaLTk5ubxDFBw9erTIWD91mzZtUntuHB0dyzs8IiIiIiKiD8JHvThiebCyskJcXFyR5R+KZs2aFRnrp87LywstWrRQWaarq/ueoyEiIiIiIvowMXFQQjo6OrC1tS3vMDRiaGj40cRaHkxMTGBiYlLeYRAREREREX3Q+KgCEREREREREanFGQdEn5jLsz01WjmViIiIiIgI4IwDIiIiIiIiIioCEwdEREREREREpBYTB0RERERERESkFhMHRERERERERKQWEwdEREREREREpBbfqkD0iXGatRda+kblHQYR0QctaWGX8g6BiIjog8EZB0RERERERESkFhMHRERERERERKQWEwdEREREREREpBYTB0RERERERESkFhMHRERERERERKQWEwdEREREREREpBYTB0REREQaWLNmDRo1agRTU1OYmpqiVatW+PPPP9XW//nnn/HFF1+gYsWKqFixItzd3XHmzBmlegkJCfDy8oKZmRmMjY3h4uKC5OTkdzkUIiKiEmHigIiIiEgD1atXx8KFC3H+/HmcO3cOX375Jbp3744rV66orC+Xy+Hr64vY2FicPHkSNjY26NChA+7evSvUuXXrFj7//HM4ODhALpfjr7/+wowZM2BgYPC+hkVERFSsDypxMHDgQEgkEkgkEujp6cHW1hYhISF4/fo15HK5UPbmlpqaCgAIDg4W9mlra8PGxgbDhg3D48ePRcdJTU3F6NGjUadOHejr68PGxgbdunXDwYMHhTq1atVSeayFCxcCAJKSkiCRSFC1alU8e/ZM1H/jxo0RHBws1Clqi4iIKPa85OfnY926dWjRogWkUikqVKiAZs2aYfny5Xjx4kWJxq7puBSbiYkJHB0dERgYiMTERFFfERERqFChAgDA1dW1yHG6uroWO06FBQsWQFtbG4sXL1Yqi4iIgEQiQf369ZXKtm3bBolEglq1apVpTBcvXoSPjw8sLS2hr6+PmjVromvXrvj999+Rn58vqvvLL7/AxcUFRkZGMDExQbt27bBnzx6lPnNzcxEaGoqGDRvCwMAAFStWRKdOnXD8+HGV41V8rhUrVkSLFi0QEhKC9PR0jeInIqKy0a1bN3Tu3Bn16tWDnZ0d5s2bB6lUilOnTqmsv2nTJowcORKNGzeGg4MD1q9fj7y8PNHvG9999x06d+6MRYsWoUmTJqhbty68vLxQtWrV9zUsIiKiYn1QiQMA6NixI1JSUpCYmIiJEyciODhYdAN5/fp1pKSkiLbC/7g6OjoiJSUFycnJCA8PR0xMDEaMGCGUJyUloWnTpjh06BAWL16M+Ph4xMTEwM3NDYGBgaJYQkJClI41evRoUZ1nz55hyZIlKsdiY2Mjajtx4kQhPsXm4+NT7Dnp378/xo0bh+7duyM2NhZxcXGYMWMGfvvtN+zbt0/jsZdkXAcOHEBKSgouXbqE+fPnIyEhAc7OzqJfdgrbuXOn0JdiGqaij5SUFOzcubPYcSqEhYVhypQpCAsLU1lubGyMtLQ0nDx5UrRfJpOhRo0aZRrTb7/9hpYtW+L58+f45ZdfkJCQgJiYGPTs2RPTp08X3bxPmjQJ33zzDXx8fPDXX3/hzJkz+Pzzz9G9e3esXLlSqJefn4++ffsiJCQEY8eORUJCAuRyOWxsbODq6opdu3aJYjA1NUVKSgr+/fdfnDhxAsOGDcOGDRvQuHFj3Lt3r9gxEBFR2cvNzUVkZCQyMzPRqlUrjdq8ePECOTk5MDc3BwDk5eXhjz/+gJ2dHTw9PVG1alW0aNFC6d8BIiKi8qZT3gG8SV9fHxYWFgCAESNGIDo6Grt37xb+Ua5atarwLbcqOjo6Qntra2t4e3sjPDxcKB85ciQkEgnOnDkDY2NjYb+joyMCAgJEfZmYmAh9qTN69GgsW7YMgYGBSt8OaGtri9pLpVJRfJqIiorCpk2bsGvXLnTv3l3YX6tWLXh5eSEjI0PjsZdkXJUqVRLq1KlTB926dUP79u0xePBg3Lp1C9ra2qL6il+CAODVq1dKfWjq8OHDePnyJUJCQrBhwwacOHECrVu3FtXR0dFBv379EBYWJlwX//77L+RyOcaPH48tW7aUSUyZmZkYPHgwunTpopRkqF+/PgYPHizMODh16hSWLl2KH374QZSEmTdvHl69eoUJEyage/fusLGxQVRUFLZv347du3ejW7duQt1169bh0aNHGDJkCDw8PITrUyKRCDFbWlqifv366NatGxwdHTFlyhRs3LhRo/EQEdHbi4+PR6tWrfDq1StIpVJER0ejQYMGGrWdOnUqrKys4O7uDgBIS0vD8+fPsXDhQsydOxfff/89YmJi8NVXXyE2Nhbt2rV7l0MhIiLS2Ac34+BNhoaGyM7OLlXbpKQk7N27F3p6egCAx48fIyYmBoGBgaKkgUJRCQl1fH19hUcq3oVNmzbB3t5elDRQkEgkMDMzU9nuzbG/LS0tLYwdOxb//PMPzp8/XyZ9qiKTyeDr6wtdXV34+vpCJpOprBcQEICoqCjhUY2IiAh07NgR1apVK7NY9u3bh0ePHmHKlClq60gkEgDAli1bIJVK8c033yjVmThxInJycrBjxw4AwObNm2FnZydKGhSu++jRI+zfv7/I2KpWrYqvv/4au3fvRm5urso6WVlZyMjIEG1ERPR27O3tERcXh9OnT2PEiBHw9/fH1atXi223cOFCREZGIjo6Wli/IC8vDwDQvXt3jB8/Ho0bN8a3336Lrl27Yu3ate90HERERCXxwSYO8vPzceDAAezduxdffvmlsL969eqQSqXC5ujoKGoXHx8PqVQKQ0ND1K5dG1euXMHUqVMBADdv3kR+fj4cHBw0imHq1KmiY0mlUhw9elRUR7E+wLp163Dr1q23HLWyxMRE2Nvba1S3qLEXpsm4VFGct6SkpBKNQVMZGRnYvn07/Pz8AAB+fn6IiorC8+fPleo2adIEderUwfbt25Gfn4+IiAilGSNv68aNGwAgOv9nz54VnTfF+gU3btxA3bp1VSZqrKysYGpqKvR348YNlWs0ABD2K+oWxcHBAc+ePcOjR49Uli9YsABmZmbCZmNjU2yfRERUNMUaTE2bNsWCBQvg7OyMFStWFNlmyZIlWLhwIfbt24dGjRoJ+ytXrgwdHR2lGQv169fnWxWIiOiD8sE9qrBnzx5IpVLk5OQgLy8P/fr1Q3BwMM6ePQsAOHr0KExMTIT6urq6ovb29vbYvXs3Xr16hY0bNyIuLk6YOv7mQnbFmTx5MgYOHCjaZ21trVTP09MTn3/+OWbMmIHNmzeX6BjFKUnMRY29ME3HpS4WxbfsZW3Lli2oW7cunJ2dARQsMlmzZk1s3boVgwcPVqofEBCA8PBw1KhRA5mZmejcubNoLYF3oVGjRoiLiwMA1KtXD69fvxbKSvJZlfRaLKoPdZ9HUFAQJkyYIPyckZHB5AERURnLy8tDVlaW2vJFixZh3rx52Lt3L5o1ayYq09PTg4uLC65fvy7af+PGDdSsWfOdxEtERFQaH1ziwM3NDWvWrIGenh6srKygoyMOsXbt2kU+UqD4JgAomBbYpUsXzJ49G3PmzEG9evUgkUhw7do1jWKpXLmy0FdxFi5ciFatWmHy5Mka1deUnZ2dxvEWNfbCSjKuwhISEgAUfAbvgkwmw5UrV0SfeV5eHsLCwlQmDr7++mtMmTIFwcHB6N+/v9K18rbq1asHoGBBzpYtWwIoWIND1bmzs7PDsWPHkJ2drTTr4N69e8jIyICdnZ1QV3Eu36TYr6hblISEBJiamqJSpUoqy/X19aGvr19sP0REpJmgoCB06tQJNWrUwLNnz7B582bI5XLs3bsXADBgwABYW1tjwYIFAIDvv/8eM2fOxObNm1GrVi3hLVCKWWtAQTLfx8cHbdu2hZubG2JiYvD7779DLpeXyxiJiIhU+eAeVTA2NoatrS1q1KhRJjeC06dPx5IlS3Dv3j2Ym5vD09MTq1atQmZmplLdp0+flvo4zZs3x1dffYVvv/32LaJV1q9fP9y4cQO//fabUll+fn6Rr+QrPPa3lZeXhx9++AG1a9dGkyZN3rq/N8XHx+PcuXOQy+WIi4sTNrlcjpMnT6pMnpibm8PLywuHDx8u88cUAKBDhw4wNzfH999/X2zdvn374vnz5/jpp5+UypYsWQJdXV306tVLqJuYmIjff/9dqe7SpUtRqVIleHh4FHm8tLQ0bN68GT169ICW1gf3vzER0X9SWloaBgwYAHt7e7Rv3x5nz57F3r17hb+zk5OTkZKSItRfs2YNsrOz0bt3b1haWgpb4bcx9ezZE2vXrsWiRYvQsGFDrF+/Hjt27MDnn3/+3sdHRESkzgc346A4aWlpwgr5CpUqVVJ6ZEGhVatWaNSoEebPn4+VK1di1apVaNOmDZo3b46QkBA0atQIr1+/xv79+7FmzRrRN8HPnj0Tvh1QMDIygqmpqcpjzZs3D46OjmX6zXefPn0QHR0NX19fTJ8+HR06dECVKlUQHx+P0NBQjB49Gj169NBo7CUZ16NHj5CamooXL17g8uXLWL58Oc6cOYM//vhD6Y0KZUEmk6F58+Zo27atUpmLiwtkMpnotZwKERERWL16tdpv3d+GVCrF+vXr4ePjgy5dumDMmDGoV68enj9/jpiYGAAQzkWrVq0wduxYTJ48GdnZ2ejRowdycnKwceNGrFixAsuXLxceE+jbty+2bdsGf39/LF68GO3bt0dGRgZWrVqF3bt3Y9u2baLFO/Pz85Gamor8/Hw8ffoUJ0+exPz582FmZoaFCxeW+biJiEg1dQv2Krw5S0DTNYECAgLeSQKciIiorHx0X1Xa29uLsvaWlpbFrvI/fvx4rF+/Hnfu3EGdOnVw4cIFuLm5YeLEiXBycoKHhwcOHjyINWvWiNrNnDlT6VhFrbBvZ2eHgIAApcTG25BIJNi8eTOWLVuGXbt2oV27dmjUqBGCg4PRvXt3eHp6Ftm+8NhLMi53d3dYWlqiYcOG+Pbbb1G/fn389ddfcHNzK7OxKWRnZ2Pjxo3CN/Jv6tWrFzZs2ICcnBylMkNDw3eSNFDo2bMnTpw4ASMjI+Fbpi+//BKHDh1CZGQkunbtKtRdvnw5Vq9ejS1btsDJyQnNmjXDkSNHsGvXLtFaExKJBFFRUZg2bRpCQ0Nhb2+PL774Av/88w/kcrlSIigjIwOWlpawtrZGq1at8NNPP8Hf3x8XL16EpaXlOxs7ERERERERAEjyy2KVNiL64GVkZBS8XWFcFLT0jco7HCKiD1rSwi7lHQIREdE7o7g3SE9PVzujvrCPbsYBEREREREREb0/TByUs06dOgmrK7+5zZ8/v7zDKzObNm1SO05HR0fGRERERERE9IH66BZH/K9Zv349Xr58qbLM3Nz8PUfz7nh5eaFFixYqy9QtbPmufYgxERERERERfWiYOChn1tbW5R3Ce2FiYgITE5PyDkPkQ4yJiIiIiIjoQ8NHFYiIiIiIiIhILc44IPrEXJ7tqdHKqURERERERABnHBARERERERFREZg4ICIiIiIiIiK1mDggIiIiIiIiIrWYOCAiIiIiIiIitZg4ICIiIiIiIiK1+FYFok+M06y90NI3Ku8wiIhUSlrYpbxDICIiojdwxgERERERERERqcXEARERERERERGpxcQBEREREREREanFxAERERERERERqcXEARERERERERGpxcQBEREREREREanFxAERERF9sBYsWAAXFxeYmJigatWq6NGjB65fv15km5ycHISEhKBu3bowMDCAs7MzYmJi3rpfIiKiTxUTB0RERPTBOnz4MAIDA3Hq1Cns378fOTk56NChAzIzM9W2mT59On766Sf8+OOPuHr1KoYPH46ePXvi4sWLb9UvERHRp+qDTxwMHDgQEokEEokEenp6sLW1RUhICF6/fg25XC6UvbmlpqYCAIKDg4V92trasLGxwbBhw/D48WPRcVJTUzF69GjUqVMH+vr6sLGxQbdu3XDw4EGhTq1atVQea+HChQCApKQkSCQSVK1aFc+ePRP137hxYwQHBwt1itoiIiIAAC9fvoS5uTkqV66MrKwslednx44d+PLLL1GxYkUYGhrC3t4eAQEBol+OIiIiVB7HwMCgxJ+Brq4uqlWrBg8PD4SFhSEvL09lG09PT2hra+Ps2bMAgKysLDg6OmLYsGFKdadMmYLatWvj2bNnyM3NxcKFC+Hg4ABDQ0OYm5ujRYsWWL9+fbFxrl27FiYmJnj9+rWw7/nz59DV1YWrq6uoruLauXXrlrDvxIkT6Ny5MypWrAgDAwM0bNgQy5YtQ25urqht4XNoamoKFxcX/Pbbb6I6ERERqFChgmhfQkICbGxs4O3tjezsbI0+l9KceyKi/5KYmBgMHDgQjo6OcHZ2RkREBJKTk3H+/Hm1bX799VdMmzYNnTt3Rp06dTBixAh07twZS5cufat+iYiIPlUffOIAADp27IiUlBQkJiZi4sSJCA4OxuLFi4Xy69evIyUlRbRVrVpVKHd0dERKSgqSk5MRHh6OmJgYjBgxQihPSkpC06ZNcejQISxevBjx8fGIiYmBm5sbAgMDRbGEhIQoHWv06NGiOs+ePcOSJUtUjsXGxkbUduLEiUJ8is3HxwdAQVLA0dERDg4O2LVrl1JfU6dOhY+PDxo3bozdu3fj+vXr2Lx5M+rUqYOgoCBRXVNTU6W4//nnH80+APzfZ5CUlIQ///wTbm5uGDt2LLp27Sq6UQeA5ORknDhxAqNGjUJYWBgAQF9fHxs2bEBERAT27t0r1D116hRCQ0MREREBExMTzJ49G6GhoZgzZw6uXr2K2NhYDBs2DE+fPi02Rjc3Nzx//hznzp0T9h09ehQWFhY4ffo0Xr16JeyPjY1FjRo1ULduXQBAdHQ02rVrh+rVqyM2NhbXrl3D2LFjMXfuXPTt2xf5+fmiY4WHhyMlJQXnzp1DmzZt0Lt3b8THx6uN7ezZs/jiiy/QsWNHbN26FXp6egA0+1xKcu6JiP7r0tPTAQDm5uZq62RlZSklxw0NDXHs2LG36peIiOhTpVPeAWhCX18fFhYWAIARI0YgOjoau3fvRqtWrQAAVatWVfp2tzAdHR2hvbW1Nby9vREeHi6Ujxw5EhKJBGfOnIGxsbGw39HREQEBAaK+TExMhL7UGT16NJYtW4bAwEBRAgMAtLW1Re2lUqkovsJkMhn8/PyQn58PmUwmJBSAghvuRYsWYcWKFRgzZoywv0aNGmjatKnSja5EIik27qIU/gysra3x2WefoWXLlmjfvj0iIiIwZMgQoW54eDi6du2KESNGoGXLlli2bBkMDQ3RtGlTfPfddxg8eDAuX74MAwMDDBo0CKNHj0a7du0AALt378bIkSPh7e0t9Ofs7KxRjPb29rC0tIRcLkfLli0BFMws6N69Ow4dOoRTp04JMw/kcjnc3NwAAJmZmRg6dCi8vLywbt06ob8hQ4agWrVq8PLyQlRUlOj8V6hQARYWFrCwsMCcOXOwYsUKxMbGomHDhkpxHTp0CN27d8fIkSPx/fffi8o0+VxKcu4Ly8rKEs1UycjIKPI4REQfury8PIwbNw5t2rSBk5OT2nqenp5YtmwZ2rZti7p16+LgwYPYuXOn0gyykvZLRET0qfooZhy8ydDQENnZ2aVqm5SUhL179wrf+D5+/BgxMTEIDAwUJQ0UikpIqOPr6ys8UlFat27dwsmTJ9GnTx/06dMHR48eFX0TvWXLFkilUowcOVJle4lEUupja+rLL7+Es7Mzdu7cKezLz89HeHg4/Pz84ODgAFtbW2zfvl0o/+6772BhYYExY8Zg+vTpkEgkmD9/vlBuYWGBQ4cO4cGDB6WKyc3NDbGxscLPsbGxcHV1Rbt27YT9L1++xOnTp4XEwb59+/Do0SNMmjRJqb9u3brBzs4OW7ZsUXm8169fQyaTAYBwTRUWHR2NLl26YPr06UpJg7eh6ty/acGCBTAzMxM2GxubMjs+EVF5CAwMxOXLlxEZGVlkvRUrVqBevXpwcHCAnp4eRo0ahUGDBkFLS/WvPZr2S0RE9Kn6qBIH+fn5OHDgAPbu3Ysvv/xS2F+9enVIpVJhc3R0FLWLj4+HVCqFoaEhateujStXrmDq1KkAgJs3byI/Px8ODg4axTB16lTRsaRSKY4ePSqqo1j3YN26daJn6EsiLCwMnTp1QsWKFWFubg5PT0/RLIkbN26gTp060NH5v0kjy5YtE8WlmHYJFEzBfDPuTp06lSq2whwcHJCUlCT8fODAAbx48QKenp4AAD8/P+HGGiiY/bFhwwZs27YNP/74IzZs2CCaTrps2TI8ePAAFhYWaNSoEYYPH44///xT43jc3Nxw/PhxvH79Gs+ePcPFixfRrl07tG3bFnK5HABw8uRJZGVlCYmDGzduAADq16+vdoyKOgq+vr6QSqXQ19fH+PHjUatWLfTp00dU5/nz5/D29sbkyZOF6+1Nb/O5vHnu3xQUFIT09HRhu3Pnjkb9EhF9iEaNGoU9e/YgNjYW1atXL7JulSpVsGvXLmRmZuKff/7BtWvXIJVKUadOnbfql4iI6FP1UTyqsGfPHkilUuTk5CAvLw/9+vVDcHCwsPDe0aNHYWJiItTX1dUVtbe3t8fu3bvx6tUrbNy4EXFxccK6BG9O6S/O5MmTMXDgQNE+a2trpXqenp74/PPPMWPGDGzevLlEx8jNzcUvv/yCFStWCPv8/PwwadIkzJw5U+03JgEBAfDy8sLp06eFRxwUTExMcOHCBVF9Q0PDEsWlSn5+vmh2Q1hYGHx8fISEhq+vLyZPnoxbt24J6wk0aNAAvXr1wtOnT9GsWTNRfw0aNMDly5dx/vx5HD9+HEeOHEG3bt0wcOBAjRZIdHV1RWZmJs6ePYsnT57Azs4OVapUQbt27TBo0CC8evUKcrkcderUQY0aNZTGoqnQ0FC4u7vj9u3bGD9+PH744Qel52INDQ3x+eef4+eff4avr6/KxMTbfC5vnvs36evrQ19fX6O+iIg+VPn5+Rg9ejSio6Mhl8tRu3ZtjdsaGBjA2toaOTk52LFjhyjB+zb9EhERfWo+isSBm5sb1qxZAz09PVhZWYm+ZQeA2rVrF/lIgeJtDACwcOFCdOnSBbNnz8acOXNQr149SCQSXLt2TaNYKleuLPRVnIULF6JVq1aYPHmyRvUV9u7di7t374qeqQcKEgoHDx6Eh4cH6tWrh2PHjiEnJ0dIlFSoUAEVKlTAv//+q9SnlpaWxnGXREJCgvDL1uPHjxEdHY2cnBysWbNGFHdYWBjmzZsn7NPR0VH6HAvH6uLiAhcXF4wbNw4bN25E//798d133xX7i52tra2wwOGTJ0+EtROsrKxgY2ODEydOIDY2VjRjxc7OThhL69atVY6xQYMGon0WFhawtbWFra0twsPD0blzZ1y9elW0poW2tjZ27dqFr776SniE4s3kwdt8LoXPPRHRf1VgYCA2b96M3377DSYmJsJbk8zMzIRE64ABA2BtbY0FCxYAAE6fPo27d++icePGuHv3LoKDg5GXl4cpU6aUqF8iIiIq8FE8qmBsbAxbW1vUqFFD7c1mSUyfPh1LlizBvXv3hMcAVq1apfLdzZqs5q9O8+bN8dVXX+Hbb78tUTuZTIa+ffsiLi5OtPXt21eY9u/r64vnz59j9erVpY7vbR06dAjx8fHo1asXAGDTpk2oXr06Ll26JIp76dKliIiIULsoVXEUN+2avlvbzc0Ncrkccrlc9BrGtm3b4s8//8SZM2eExxQAoEOHDjA3Nxe9pkth9+7dSExMhK+vr9rjNW/eHE2bNhUlRhT09fWxc+dOuLi4wM3NDVevXtVoDMV589wTEf1XrVmzBunp6XB1dYWlpaWwbd26VaiTnJyMlJQU4edXr15h+vTpaNCgAXr27Alra2scO3ZM9CWDJv0SERFRgY9ixkFx0tLSRK/aA4BKlSopPbKg0KpVKzRq1Ajz58/HypUrsWrVKrRp0wbNmzdHSEgIGjVqhNevX2P//v1Ys2YNEhIShLbPnj0TvpVQMDIygqmpqcpjzZs3D46OjhonPB48eIDff/8du3fvVlrZecCAAejZsyceP36MVq1aYeLEiZg4cSL++ecffPXVV8KrHmUyGSQSieiRhvz8fKW4gYI3Uqh79KGwrKwspKamIjc3F/fv30dMTAwWLFiArl27YsCAAQAKEh69e/dWitvGxgZBQUGIiYlBly5dijxO79690aZNG7Ru3RoWFhb4+++/ERQUBDs7O43XoVC8RjMnJ0eYcQAA7dq1w6hRo5CdnS1KHBgbG+Onn35C3759MWzYMIwaNQqmpqY4ePAgJk+ejN69eyutX/CmcePGoWfPnpgyZYrSoyv6+vrYsWMHvL294ebmhkOHDgnrcGjyuWhy7omI/qs0eYxMsYaNQrt27YpN1Jb0UUUiIqJP2Ucx46A4itfwFd7Onz9fZJvx48dj/fr1uHPnDurUqYMLFy7Azc0NEydOhJOTEzw8PHDw4EHRlHsAmDlzptKxCk99fJOdnR0CAgKUEhvqbNiwAcbGxmjfvr1SWfv27WFoaIiNGzcCAJYsWYLNmzfj4sWL6Nq1K+rVqwdvb2/k5eXh5MmTomRGRkaGUtyWlpZIS0vTKK6YmBhYWlqiVq1a6NixI2JjY/HDDz/gt99+g7a2Ns6fP49Lly6p/AbczMwM7du3Fy2SqI6npyd+//134W0G/v7+cHBwwL59+zROvri5ueHly5ewtbVFtWrVhP3t2rXDs2fPhOulsN69eyM2NhbJycn44osvYG9vj9DQUHz33XeIjIws9i0VHTt2RO3atVXOOgAKHpfZvn07WrduDTc3N1y+fBmAZp9LceeeiIiIiIjoXZLkM+VO9EnIyMgoeC3juCho6RuVdzhERColLSx6ZhoRERG9PcW9QXp6utrZ84X9J2YcEBEREREREdG7wcTBJy45ORlSqVTtlpycXN4hCj6mWImIiIiIiP4r/hOLI1LpWVlZIS4ursjyD8XHFCsREREREdF/BRMHnzgdHR3Y2tqWdxga+ZhiJSIiIiIi+q/gowpEREREREREpBZnHBB9Yi7P9tRo5VQiIiIiIiKAMw6IiIiIiIiIqAhMHBARERERERGRWkwcEBEREREREZFaTBwQERERERERkVpMHBARERERERGRWnyrAtEnxmnWXmjpG5V3GET0DiUt7FLeIRAREdF/CGccEBEREREREZFaTBwQERERERERkVpMHBARERERERGRWkwcEBEREREREZFaTBwQERERERERkVpMHBARERERERGRWkwcEBER/cctWLAALi4uMDExQdWqVdGjRw9cv35d4/aRkZGQSCTo0aOHUllCQgK8vLxgZmYGY2NjuLi4IDk5uQyjJyIiovLGxAFp7M6dOwgICICVlRX09PRQs2ZNjB07Fo8ePQIAfPvtt3BwcBC1uXbtGiQSCQYOHCjaHxERAX19fbx8+RIAIJFIYGBggH/++UdUr0ePHkpti5KamorRo0ejTp060NfXh42NDbp164aDBw8q1V2wYAG0tbWxePFipbKIiAhIJBJIJBJoaWnB0tISPj4+pfpleMuWLdDW1kZgYKDK8oyMDMyYMQOOjo4wNDREpUqV4OLigkWLFuHJkydCPVdXVyGmwtvw4cNLHBMRfVoOHz6MwMBAnDp1Cvv370dOTg46dOiAzMzMYtsmJSVh0qRJ+OKLL5TKbt26hc8//xwODg6Qy+X466+/MGPGDBgYGLyLYRAREVE5YeKANHL79m00a9YMiYmJ2LJlC27evIm1a9fi4MGDaNWqFR4/fgw3Nzdcv34dqampQrvY2FjY2NhALpeL+ouNjUXLli1haGgo7JNIJJg5c2apY0xKSkLTpk1x6NAhLF68GPHx8YiJiYGbm5vKm/awsDBMmTIFYWFhKvszNTVFSkoK7t69ix07duD69evw9vYucVwymQxTpkzBli1b8OrVK1HZ48eP0bJlS4SHh2PSpEk4ffo0Lly4gHnz5uHixYvYvHmzqP7QoUORkpIi2hYtWlTimIjo0xITE4OBAwfC0dERzs7OiIiIQHJyMs6fP19ku9zcXHz99deYPXs26tSpo1T+3XffoXPnzli0aBGaNGmCunXrwsvLC1WrVn1XQyEiIqJywMQBaSQwMBB6enrYt28f2rVrhxo1aqBTp044cOAA7t69i++++w6ff/45dHV1RUkCuVyOwMBAPH78GElJSaL9bm5uomOMGjUKGzduxOXLl0sV48iRIyGRSHDmzBn06tULdnZ2cHR0xIQJE3Dq1ClR3cOHD+Ply5cICQlBRkYGTpw4odSfRCKBhYUFLC0t0bp1awwePBhnzpxBRkaGxjH9/fffOHHiBL799lvY2dlh586dovJp06YhOTkZZ86cwaBBg9CoUSPUrFkTHTp0wJYtWzBy5EhRfSMjI1hYWIg2U1PTEpwlIiIgPT0dAGBubl5kvZCQEFStWhWDBw9WKsvLy8Mff/wBOzs7eHp6omrVqmjRogV27dr1LkImIiKicsTEARXr8ePH2Lt3L0aOHCmaIQAAFhYW+Prrr7F161YYGRnBxcUFsbGxQrlcLkf79u3Rpk0bYf/t27eRnJyslDho06YNunbtim+//bZUMcbExCAwMBDGxsZK5RUqVBD9LJPJ4OvrC11dXfj6+kImkxXZf1paGqKjo6GtrQ1tbW2N4woPD0eXLl1gZmYGPz8/0XHy8vKwdetW+Pn5wcrKSmV7iUSi8bHelJWVhYyMDNFGRJSXl4dx48ahTZs2cHJyUlvv2LFjkMlk+Pnnn1WWp6Wl4fnz51i4cCE6duyIffv2oWfPnvjqq69w+PDhdxU+ERERlQMmDqhYiYmJyM/PR/369VWW169fH0+ePMGDBw/g5uYmzDi4evUqXr16hSZNmqBt27bCfrlcDgMDA7Rs2VKprwULFiAmJgZHjx4tUYw3b95Efn6+0hoLqmRkZGD79u3w8/MDAPj5+SEqKgrPnz8X1UtPT4dUKoWxsTGqVauG2NhYtYkJVfLy8hARESEcp2/fvjh27Bj+/vtvAMCDBw/w9OlT2Nvbi9o1bdoUUqkUUqkUvr6+orLVq1cLZYpt06ZNKo+/YMECmJmZCZuNjY1GcRPRf1tgYCAuX76MyMhItXWePXuG/v374+eff0blypVV1snLywMAdO/eHePHj0fjxo3x7bffomvXrli7du07iZ2IiIjKBxMHpLH8/Pxi67i6uuLGjRtISUmBXC7H559/Dm1tbbRr106UOGjdujX09fWV2jdo0AADBgwo8awDTWJT2LJlC+rWrQtnZ2cAQOPGjVGzZk1s3bpVVM/ExARxcXE4d+4cli5dis8++wzz5s3T+Dj79+9HZmYmOnfuDACoXLkyPDw81K6poBAdHY24uDh4enoKi0cqfP3114iLixNtXl5eKvsJCgpCenq6sN25c0fj2Inov2nUqFHYs2cPYmNjUb16dbX1bt26haSkJHTr1g06OjrQ0dHBhg0bsHv3bujo6ODWrVuoXLkydHR00KBBA1Hb+vXr860KRERE/zE65R0AffhsbW0hkUiQkJCAnj17KpUnJCSgYsWKqFKlCtq0aQM9PT3ExsYiNjYW7dq1AwC4uLjg4cOHuH37NuRyOb755hu1x5s9ezbs7OxK9JxsvXr1IJFIcO3atWLrymQyXLlyBTo6/3f55+XlISwsTPQcr5aWFmxtbQEU/CJ869YtjBgxAr/++qtGMclkMjx+/Fj0eEdeXh7++usvzJ49G1WqVEGFChWUXolWo0YNAAWJi6dPn4rKzMzMhJiKo6+vrzI5Q0Sfnvz8fIwePRrR0dGQy+WoXbt2kfUdHBwQHx8v2jd9+nQ8e/YMK1asgI2NDfT09ODi4qL0d9iNGzdQs2bNMh8DERERlR/OOKBiVapUCR4eHli9erXSN+CpqanYtGkTfHx8IJFIYGhoiBYtWkAul+Pw4cNwdXUFAOjq6qJly5aQyWS4c+eO0voGhdnY2GDUqFGYNm0acnNzNYrR3Nwcnp6eWLVqlcrXiyluwOPj43Hu3DnI5XLRt/ZyuRwnT54sMvHw7bffYuvWrbhw4UKx8Tx69Ai//fYbIiMjRce5ePEinjx5gn379kFLSwt9+vTBxo0bce/ePY3GSURUGoGBgdi4cSM2b94MExMTpKamIjU1VfR3+oABAxAUFAQAMDAwgJOTk2irUKECTExM4OTkBD09PQDA5MmTsXXrVvz888+4efMmVq5cid9//11pYVciIiL6uDFxQBpZuXIlsrKy4OnpiSNHjuDOnTuIiYmBh4cHrK2tRVP43dzcEBkZiVevXuGzzz4T9rdr1w4//vgjjI2N4eLiUuTxgoKCcO/ePRw4cEDjGFetWoXc3Fw0b94cO3bsQGJiIhISEvDDDz+gVatWAApmATRv3hxt27YV/ULctm1buLi4FLlIoo2NDXr27KnRKyN//fVXVKpUCX369BEdx9nZGZ07dxaOM3/+fFhbW6N58+YICwvDX3/9hVu3biE6OhonT55UWojxxYsXwi/8iu3JkycanyMi+jStWbMG6enpcHV1haWlpbAVfkQrOTkZKSkpJeq3Z8+eWLt2LRYtWoSGDRti/fr12LFjBz7//POyHgIRERGVIyYOSCP16tXDuXPnUKdOHfTp0wd169bFsGHD4ObmhpMnT4pe6eXm5oZnz56hTZs2oscB2rVrh2fPngmvbSyKubk5pk6dilevXmkcY506dXDhwgW4ublh4sSJcHJygoeHBw4ePIg1a9YgOzsbGzduRK9evVS279WrFzZs2ICcnBy1xxg/fjz++OMPnDlzpshYwsLC0LNnT5VvRejVqxd2796Nhw8folKlSjhz5gwGDBiAxYsXo3nz5mjYsCGCg4Ph4+OjtJr5zz//LPql39LSUmkBRSKiN+Xn56vcBg4cKNSRy+WIiIhQ20dERITKR8gCAgKQmJiIly9fIi4uDt27dy/7ARAREVG5kuSXZFU5IvpoZWRkFLxdYVwUtPSNyjscInqHkhZ2Ke8QiIiI6AOmuDdIT0+HqalpsfU544CIiIiIiIiI1GLigD4KycnJkEqlarf3/eqvo0ePFhkPERERERHRfwVfx0gfBSsrK8TFxRVZ/j41a9asyHiIiIiIiIj+K5g4oI+Cjo4ObG1tyzsMgaGh4QcVDxERERER0bvCRxWIiIiIiIiISC0mDoiIiIiIiIhILT6qQPSJuTzbU6NXrhAREREREQGccUBERERERERERWDigIiIiIiIiIjUYuKAiIiIiIiIiNRi4oCIiIiIiIiI1GLigIiIiIiIiIjU4lsViD4xTrP2QkvfqLzDIPpkJS3sUt4hEBEREZUIZxwQERERERERkVpMHBARERERERGRWkwcEBEREREREZFaTBwQERERERERkVpMHBARERERERGRWkwcEBEREREREZFaTBwQERGVkwULFsDFxQUmJiaoWrUqevTogevXrxfZ5ueff8YXX3yBihUromLFinB3d8eZM2eE8pycHEydOhUNGzaEsbExrKysMGDAANy7d+9dD4eIiIj+o0qVODh58iS0tbXRpYv4XdRJSUmQSCTQ1tbG3bt3RWUpKSnQ0dGBRCJBUlISgoODIZFIityKM3DgQKGurq4uateujSlTpuDVq1eieur6j4yMFOrk5uYiNDQUDRs2hIGBASpWrIhOnTrh+PHjJTo32dnZWLRoEZydnWFkZITKlSujTZs2CA8PR05OTpnGLZfLhX1aWlowMzNDkyZNMGXKFKSkpIj6Cg4ORuPGjQEAtWrVKvK8Dxw4sNhxFq6vo6ODGjVqYMKECcjKylJ5zMIU10lcXJzSOCQSCapUqYLOnTsjPj5e1K7weSu8dezYUekYCxYsgLa2NhYvXqxUpmlc6vz6668wNjbGzZs3Rfvv3buHihUrYuXKlQDUn+eFCxcq9enp6QltbW2cPXtWqUzT64WIPj6HDx9GYGAgTp06hf379yMnJwcdOnRAZmam2jZyuRy+vr6IjY3FyZMnYWNjgw4dOgj/7r548QIXLlzAjBkzcOHCBezcuRPXr1+Hl5fX+xoWERER/cfolKaRTCbD6NGjIZPJcO/ePVhZWYnKra2tsWHDBgQFBQn7fvnlF1hbWyM5ORkAMGnSJAwfPlwod3FxwbBhwzB06NASxdKxY0fhpvz8+fPw9/eHRCLB999/L6oXHh6udINZoUIFAEB+fj769u2LAwcOYPHixWjfvj0yMjKwatUquLq6Ytu2bejRo0exsWRnZ8PT0xOXLl3CnDlz0KZNG5iamuLUqVNYsmQJmjRpItywlkXcCtevX4epqSkyMjJw4cIFLFq0CDKZDHK5HA0bNlSK8+zZs8jNzQUAnDhxAr169RL6AABDQ8Nix1o4tpycHFy6dAmDBg2CsbEx5syZo1H7NyliuHfvHiZPnowuXbrg5s2b0NPTE+oozlth+vr6Sn2FhYVhypQpCAsLw+TJk0sVjzr9+/dHdHQ0Bg4ciCNHjkBLqyD/NnToUDRt2hSBgYFC3ZCQEKVr2sTERPRzcnIyTpw4gVGjRiEsLAwuLi5Kx9T0eiGij0tMTIzo54iICFStWhXnz59H27ZtVbbZtGmT6Of169djx44dOHjwIAYMGAAzMzPs379fVGflypVo3rw5kpOTUaNGjbIdBBEREf3nlThx8Pz5c2zduhXnzp1DamoqIiIiMG3aNFEdf39/hIeHixIH4eHh8Pf3F24qpVIppFKpUK6trQ0TExNYWFiUKB59fX2hjY2NDdzd3bF//36lG6oKFSqo7TsqKgrbt2/H7t270a1bN2H/unXr8OjRIwwZMgQeHh4wNjYuMpbly5fjyJEjOHfuHJo0aSLsr1OnDry9vZGdnV2mcStUrVpVqGdnZ4fu3bujSZMmGDFiBI4dO6ZUv0qVKsKfzc3NRX2UROHYbGxs0L17d1y4cKFEfagbx7hx4+Dl5YVr166hUaNGQp3C502dw4cP4+XLlwgJCcGGDRtw4sQJtG7dutRxqfLTTz/B0dERy5Ytw6RJkxAREYHjx48jPj5eNFtGk2s6PDwcXbt2xYgRI9CyZUssW7ZMKXmj6fVCRB+39PR0AP/3d7MmXrx4gZycnCLbpKenQyKRlPjveSIiIiKgFI8qREVFwcHBAfb29vDz80NYWBjy8/NFdby8vPDkyRPhpvXYsWN48uSJ6Kb8Xbh8+TJOnDgh+oZaE5s3b4adnZ3K+CZOnIhHjx4pfXujyqZNm+Du7i5KGijo6uqqTTyUNm51DA0NMXz4cBw/fhxpaWll0mdxbty4gUOHDqFFixZv3Vd6errwOEZpzolMJoOvry90dXXh6+sLmUz21jG9qUqVKli3bh1mzJiB/fv3Y/z48VixYgVsbGxK1E9+fj7Cw8Ph5+cHBwcH2NraYvv27UW20fR6ycrKQkZGhmgjog9XXl4exo0bhzZt2sDJyUnjdlOnToWVlRXc3d1Vlr969QpTp06Fr6+vMLOMiIiIqCRKnDiQyWTw8/MDUDB9Oj09HYcPHxbV0dXVFZIKQMG0cT8/P+jq6pZByGJ79uyBVCqFgYEBGjZsiLS0NJVT0319fYVZDopN8djEjRs3UL9+fZX9K/bfuHGj2FgSExPh4ODw3uIuiiKOpKQkjeIpDUVsBgYGsLe3h6Ojo2iWSUlVr14dUqkUFSpUwObNm+Hl5aV0PhXnrfA2f/58oTwjIwPbt28XrlE/Pz9ERUXh+fPnpY5LnR49eqBPnz7o2LEj2rVrB39/f6U6U6dOVYr36NGjQvmBAwfw4sULeHp6CvGqSnRoer0UtmDBApiZmQlbSZMaRPR+BQYG4vLly6L1d4qzcOFCREZGIjo6GgYGBkrlOTk56NOnD/Lz87FmzZqyDJeIiIg+ISV6VOH69es4c+YMoqOjCxrr6MDHxwcymQyurq6iugEBAWjdujXmz5+Pbdu24eTJk3j9+nWZBa7g5uaGNWvWIDMzE6GhodDR0UGvXr2U6oWGhip9G1N4bYY3Z028SZNvvovro7Cyiru4WDRZZLK0FLHl5ubi5s2bmDBhAvr371+iX3oLO3r0KIyMjHDq1CnMnz8fa9euVaqjOG+FFZ6eu2XLFtStWxfOzs4AgMaNG6NmzZrYunUrBg8eXKq4ijJjxgxs2LAB06dPV1k+efJkpcUmra2thT+HhYXBx8cHOjoF/yv6+vpi8uTJuHXrFurWrSvU0/R6KSwoKAgTJkwQfs7IyGDygOgDNWrUKOzZswdHjhxB9erVNWqzZMkSLFy4EAcOHBA90qWgSBr8888/OHToEGcbEBERUamVKHEgk8nw+vVrpRtufX19YSV5hYYNG8LBwQG+vr6oX78+nJycil2tvjSMjY1ha2sLoOAmzNnZGTKZTOkm0cLCQqj3pnr16iEhIUFlmWK/nZ1dsbHY2dnh2rVr7y3uoijirlWrVonbaqpwbPb29nj27Bl8fX0xd+5c2NrawtTUVHhet7CnT58CAMzMzET7a9eujQoVKsDe3h5paWnw8fHBkSNHRHUKnzdVZDIZrly5ItyIAwXTf8PCwoRzW9K4iqI4TuHjFVa5cmW18T5+/BjR0dHIyckRJUNyc3MRFhaGefPmCfs0vV4K09fXV7lwJBF9OPLz8zF69GhER0dDLpejdu3aGrVbtGgR5s2bh71796JZs2ZK5YqkQWJiImJjY1GpUqWyDp2IiIg+IRo/qvD69Wts2LABS5cuRVxcnLBdunQJVlZW2LJli1KbgIAAyOVyBAQElGnQ6mhpaWHatGmYPn06Xr58qXE7X19fJCYm4vfff1cqW7p0KaysrODh4VFsP/369cOBAwdw8eJFpbKcnBy1r9cqbdzqvHz5EuvWrUPbtm1FCyG+a9ra2sLxgYJkwr///ov79++L6l24cAEGBgZFruytmLKrmN2iifj4eJw7dw5yuVx0jcrlcpw8eVJI6rxNXGVp06ZNqF69Oi5duiSKd+nSpYiIiBDefPGmsr5eiKj8BAYGYuPGjdi8eTNMTEyQmpqK1NRU0f/bAwYMED0G9v3332PGjBkICwtDrVq1hDaKR7JycnLQu3dvnDt3Dps2bUJubq5Qp/AivURERESa0jhxsGfPHjx58gSDBw+Gk5OTaOvVq5fK57KHDh2KBw8eYMiQIWUadFG8vb2hra2NVatWifY/ffpU+MVJsSlu5Pv27YsePXrA398fMpkMSUlJ+Ouvv/DNN99gz5492Lhxo0brMygWtWrfvj1WrVqFS5cu4fbt24iKikLLli2RmJhYpnErpKWlITU1FYmJiYiMjESbNm3w8OHDd/48qyK2e/fu4fDhwwgJCYGdnZ2wLoSnpyfs7e3h6+uLEydO4Pbt29i+fTumT5+OsWPHCokGVYyMjDB06FDMmjVL9AhIVlaW0vl4+PAhgILZBs2bN0fbtm1F12fbtm3h4uIiXKNvE1dJPXv2TClexSKFMpkMvXv3Vvr/afDgwXj48KHSa9oKU3e9ENHHZc2aNUhPT4erqyssLS2FbevWrUKd5ORkpKSkiNpkZ2ejd+/eojZLliwBANy9exe7d+/Gv//+i8aNG4vqnDhx4r2PkYiIiD5+GicOZDIZ3N3dVU7j7tWrF86dO6e0aruOjg4qV66sdhr3u6Cjo4NRo0Zh0aJFohvsQYMGiX55srS0xI8//gigYB2Abdu2Ydq0aQgNDYW9vT2cnZ2xfft2XLx4EW5ubhodW19fH/v378eUKVPw008/oWXLlnBxccEPP/yAMWPGFLlKdmniVrC3t4eVlRWaNm2KhQsXwt3dHZcvX0aDBg1KcupKTBFb9erV4evrC0dHR/z555+i6fv79u1DjRo14OvrCycnJ8yaNQtjx44VXstZlFGjRiEhIQHbtm0T9sXExCidj88//xzZ2dnYuHGj2uf+e/XqhQ0bNiAnJ+et4yqJmTNnKsU7ZcoUnD9/HpcuXVIZr5mZGdq3b1/k2yDUXS9E9HHJz89XuRVeG0UulyMiIkL4OSkpSWWb4OBgAAWPqKnr9831iIiIiIg0IckvyYp+n5ALFy7A3d0dgwcPxuLFi8s7HKK3lpGRUfB2hXFR0NI3Ku9wiD5ZSQu7lHcIRERE9IlT3Bukp6drtIByiV/H+Kn47LPPcPDgQRgbG+PWrVvlHQ4RERERERFRufhgEwfJycnCe+9VbcnJye88hiZNmiA4OFh4LZ6jo6PaeDZt2vTO43lf5s+fr3acnTp1Ku/w3otOnTqpPQfz588v7/CIiIiIiIjem/e3+EAJWVlZFfn6xsKvhHxf/ve//yEnJ0dlWbVq1d5zNO/O8OHD0adPH5VlhoaG7zma8rF+/Xq1bywwNzd/z9EQERERERGVnw82caCjoyO8t/5DUbNmzfIO4b0wNzf/5G+Ora2tyzsEIiIiIiKiD8IH+6gCEREREREREZW/D3bGARG9G5dne2q0cioRERERERHAGQdEREREREREVAQmDoiIiIiIiIhILSYOiIiIiIiIiEgtJg6IiIiIiIiISC0mDoiIiIiIiIhILSYOiIiIiIiIiEgtvo6R6BPjNGsvtPSNyjsM+sQkLexS3iEQERERUSlxxgERERERERERqcXEARERERERERGpxcQBEREREREREanFxAERERERERERqcXEARERERERERGpxcQBERG9V0eOHEG3bt1gZWUFiUSCXbt2FVlfLpdDIpEobampqaJ6d+/ehZ+fHypVqgRDQ0M0bNgQ586de4cjISIiIvo08HWMRET0XmVmZsLZ2RkBAQH46quvNG53/fp1mJqaCj9XrVpV+POTJ0/Qpk0buLm54c8//0SVKlWQmJiIihUrlmnsRERERJ+ij2rGwZ07dxAQEAArKyvo6emhZs2aGDt2LB49eiTUcXV1FX0jVa1aNXh7e+Off/4R6uTm5mLhwoVwcHCAoaEhzM3N0aJFC6xfv16jONasWYNGjRrB1NQUpqamaNWqFf78809RnVevXiEwMBCVKlWCVCpFr169cP/+/RKNd8eOHXB1dYWZmRmkUikaNWqEkJAQPH78GAAQEREBiUSCjh07ito9ffoUEokEcrlcqFPUlpSUVGQcL168QFBQEOrWrQsDAwNUqVIF7dq1w2+//Saqd+XKFfTp0wdVqlSBvr4+7OzsMHPmTLx48UKpz4sXL8Lb2xvVqlWDgYEB6tWrh6FDh+LGjRsAgKSkJEgkEsTFxSm19fT0hLa2Ns6ePatUNnDgQPTo0aPI8RSlpHEpflZs5ubmaNeuHY4ePSrqNz8/H+vWrUOLFi0glUpRoUIFNGvWDMuXLxfOT3BwMBo3bqw2tjevbcU2fPjwUo+XqDx06tQJc+fORc+ePUvUrmrVqrCwsBA2La3/+yfs+++/h42NDcLDw9G8eXPUrl0bHTp0QN26dcs6fCIiIqJPzkeTOLh9+zaaNWuGxMREbNmyBTdv3sTatWtx8OBBtGrVSriZBoChQ4ciJSUF9+7dw2+//YY7d+7Az89PKJ89ezZCQ0MxZ84cXL16FbGxsRg2bBiePn2qUSzVq1fHwoULcf78eZw7dw5ffvklunfvjitXrgh1xo8fj99//x3btm3D4cOHce/evRJ9s/bdd9/Bx8cHLi4u+PPPP3H58mUsXboUly5dwq+//irU09HRwYEDBxAbG6uyHx8fH6SkpAhbq1athPOj2GxsbIqMZfjw4di5cyd+/PFHXLt2DTExMejdu7coYXPq1Cm0aNEC2dnZ+OOPP3Djxg3MmzcPERER8PDwQHZ2tlB3z549aNmyJbKysrBp0yYkJCRg48aNMDMzw4wZM4qMJTk5GSdOnMCoUaMQFhamyanU2NvEdeDAAaSkpODIkSOwsrJC165dRYmi/v37Y9y4cejevTtiY2MRFxeHGTNm4LfffsO+ffs0jvHNzy4lJQWLFi0q9ZiJPiaNGzeGpaUlPDw8cPz4cVHZ7t270axZM3h7e6Nq1apo0qQJfv7553KKlIiIiOi/5aN5VCEwMBB6enrYt28fDA0NAQA1atRAkyZNULduXXz33XdYs2YNAMDIyAgWFhYAAEtLS4waNQrffPON0Nfu3bsxcuRIeHt7C/ucnZ01jqVbt26in+fNm4c1a9bg1KlTcHR0RHp6OmQyGTZv3owvv/wSABAeHo769evj1KlTaNmyZZH9nzlzBvPnz8fy5csxduxYYX+tWrXg4eEhSnAYGxujT58++Pbbb3H69GmlvgwNDYXzBQB6enqi86OJ3bt3Y8WKFejcubMQR9OmTYXy/Px8DB48GPXr18fOnTuFbwFr1qwJOzs7NGnSBKGhoZg6dSpevHiBQYMGoXPnzoiOjhb6qF27Nlq0aFFs8iY8PBxdu3bFiBEj0LJlSyxbtkw0vtJ627gqVaokfAs6bdo0REZG4vTp0/Dy8kJUVBQ2bdqEXbt2oXv37kKbWrVqwcvLCxkZGRrHWdLPjui/wNLSEmvXrkWzZs2QlZWF9evXw9XVFadPn8Znn30GoCC5vGbNGkyYMAHTpk3D2bNnMWbMGOjp6cHf37+cR0BERET0cfsoZhw8fvwYe/fuxciRI5VuEi0sLPD1119j69atyM/PV9k2KioKLVq0ELU5dOgQHjx48Nax5ebmIjIyEpmZmWjVqhUA4Pz588jJyYG7u7tQz8HBATVq1MDJkyeL7XPTpk2QSqUYOXKkyvIKFSqIfg4ODkZ8fDy2b99e+oEUwcLCAv/73//w7NkzleVxcXG4evUqJkyYIJo6DBQkZNzd3bFlyxYAwN69e/Hw4UNMmTJFZV9vjq2w/Px8hIeHw8/PDw4ODrC1tS2zMb9NXIW9fPkSGzZsAFCQpAEKPk97e3tR0kBBIpHAzMysdEEXIysrCxkZGaKN6GNkb2+Pb775Bk2bNkXr1q0RFhaG1q1bIzQ0VKiTl5eHzz77DPPnz0eTJk0wbNgwDB06FGvXri3HyImIiIj+Gz6KxEFiYiLy8/NRv359leX169fHkydPhETA6tWrIZVKYWxsjEqVKuH69euiae3Lli3DgwcPYGFhgUaNGmH48OFKaxQUJz4+HlKpFPr6+hg+fDiio6PRoEEDAEBqair09PSUbjarVaumtAq4uvHWqVMHurq6GsViZWWFsWPH4rvvvsPr169LNA5NrFu3DidOnEClSpXg4uKC8ePHi6YJK57/L+rzUdRJTEwEUJBIKakDBw7gxYsX8PT0BAD4+flBJpOVuB9V3iYuAGjdurVwzS1ZsgRNmzZF+/bthb7t7e3LJE7FtV1427Rpk8q6CxYsgJmZmbAV90gK0cekefPmuHnzpvCzpaWl8HewQv369ZGcnPy+QyMiIiL6z/koEgcKqmYUqPL1118jLi4Oly5dwrFjx2Bra4sOHToI35g3aNAAly9fxqlTpxAQEIC0tDR069YNQ4YM0TgWe3t7xMXF4fTp0xgxYgT8/f1x9erVUo3rTZqOs7CpU6fiwYMHZf7cPwC0bdsWt2/fxsGDB9G7d29cuXIFX3zxBebMmSOqp0ncpRmbQlhYGHx8fKCjU/CEja+vL44fP45bt26Vus+yiAsAtm7diosXL2LHjh2wtbVFRESEkPh5274LU1zbhTcvLy+VdYOCgpCeni5sd+7cKbM4iMpbXFwcLC0thZ/btGmD69evi+rcuHEDNWvWfN+hEREREf3nfBSJA1tbW0gkEiQkJKgsT0hIQMWKFVGlShUAgJmZGWxtbWFra4s2bdpAJpMhMTERW7duFdpoaWnBxcUF48aNw86dOxEREQGZTIa///5bo5j09PRga2uLpk2bYsGCBXB2dsaKFSsAFEztz87OVnou/v79+xo9n25nZ4fbt28jJydHo1iAgqn0QUFBmD17tsq3GLwtXV1dfPHFF5g6dSr27duHkJAQzJkzB9nZ2bCzswOAIj8fRR3Ff69du1ai4z9+/BjR0dFYvXo1dHR0oKOjA2tra7x+/bpMkiWljUvBxsYG9erVQ8+ePTF//nz07NkTWVlZQt+l7fdNha9txWZiYqKyrr6+vvDmD8VG9CF4/vy5kPgCgL///htxcXHC7ICgoCAMGDBAqL98+XL89ttvuHnzJi5fvoxx48bh0KFDCAwMFOqMHz8ep06dwvz583Hz5k1s3rwZ69atE9UhIiIiotL5KBIHlSpVgoeHB1avXo2XL1+KylJTU7Fp0yb4+PhAIpGobK+trQ0ASm0LU0xxzczMLFWMeXl5wo1i06ZNoauri4MHDwrl169fR3JysrAOQlH69euH58+fY/Xq1SrL1S3UN3r0aGhpaQkJjHepQYMGeP36NV69eoXGjRvDwcEBoaGhyMvLE9W7dOkSDhw4AF9fXwBAhw4dULlyZbVvAlA3tk2bNqF69eq4dOmS6Nv2pUuXIiIiArm5uW81ntLGpUrv3r2ho6MjfH79+vXDjRs3lF5fCRTMRkhPTy9VzEQfq3PnzqFJkyZo0qQJAGDChAlo0qQJZs6cCQBISUkRPWKQnZ2NiRMnomHDhmjXrp3w94ricSAAcHFxQXR0NLZs2QInJyfMmTMHy5cvx9dff/1+B0dERET0H/TRvFVh5cqVaN26NTw9PTF37lzUrl0bV65cweTJk2FtbY158+YJdV+8eCGsJXD//n3MmTMHBgYG6NChA4CCG7s2bdqgdevWsLCwwN9//42goCDY2dlp9Ix7UFAQOnXqhBo1auDZs2fYvHkz5HI59u7dC6DgW+HBgwdjwoQJMDc3h6mpKUaPHo1WrVoV+0YFAGjRogWmTJmCiRMn4u7du+jZsyesrKyEV1B+/vnnorctKBgYGGD27Nll/g2bq6srfH190axZM1SqVAlXr17FtGnT4ObmJnyLLZPJ4OHhgV69eiEoKAgWFhY4ffo0Jk6ciFatWmHcuHEACt4CsX79enh7e8PLywtjxoyBra0tHj58iKioKCQnJyMyMlIpBplMht69e8PJyUm038bGBkFBQYiJiUGXLl0AAOnp6cI3mQqVKlUq8hn/0salikQiwZgxYxAcHIxvvvkGffr0QXR0NHx9fTF9+nR06NABVapUQXx8PEJDQzF69Gj06NEDQEFy683YTUxMhHfRF762FfT19VGxYkWNYiP6ELi6uhb5CE9ERITo5ylTpqhduLSwrl27omvXrm8bHhERERG94aOYcQAA9erVw7lz51CnTh306dMHdevWxbBhw+Dm5oaTJ0/C3NxcqPvzzz/D0tISlpaWcHNzw8OHD/G///1PWKDO09MTv//+O7p16wY7Ozv4+/vDwcEB+/btE56fL0paWhoGDBgAe3t7tG/fHmfPnsXevXvh4eEh1AkNDUXXrl3Rq1cvtG3bFhYWFti5c6fG4/3++++xefNmnD59Gp6ennB0dMSECRPQqFGjIl8t5u/vjzp16mh8HE14enril19+QYcOHVC/fn2MHj0anp6eiIqKEuq0bt0ap06dgra2Njp16gRbW1sEBQXB398f+/fvh76+vlC3e/fuOHHiBHR1ddGvXz84ODjA19cX6enpmDt3rtLxz58/j0uXLqFXr15KZWZmZmjfvr1okUS5XC58m6nYZs+eXew4SxpXUfz9/ZGTk4OVK1dCIpFg8+bNWLZsGXbt2oV27dqhUaNGCA4ORvfu3YXFHoGCZ7LfjL3wq0QLX9uKTTGbg4iIiIiI6F2Q5Jflym1E9MHKyMgoeLvCuCho6RuVdzj0iUla2KW8QyAiIiKi/09xb5Cenq7RWmgfzYwDIiIiIiIiInr/mDh4Q3JyMqRSqdqtLN4JPnz4cLX9Dx8+vAxGUTJFjffo0aPvPZ53YdOmTWrH6OjoWN7hERERERERfbD4qMIbXr9+jaSkJLXltWrV0mgdhKKkpaUhIyNDZZmpqSmqVq36Vv2X1M2bN9WWWVtbw9DQ8D1G8248e/YM9+/fV1mmq6v7SbzrnY8qUHniowpEREREH46SPqrw0bxV4X3R0dGBra3tOz1G1apV33tyoCjverwfAhMTE5iYmJR3GERERERERB8dPqpARERERERERGpxxgHRJ+bybE+NpiMREREREREBnHFAREREREREREVg4oCIiIiIiIiI1GLigIiIiIiIiIjUYuKAiIiIiIiIiNRi4oCIiIiIiIiI1OJbFYg+MU6z9kJL36i8w/joJS3sUt4hEBERERG9F5xxQERERERERERqMXFARERERERERGoxcUBEREREREREajFxQERERERERERqMXFARERERERERGoxcUBEREREREREajFxQERUBlatWoVatWrBwMAALVq0wJkzZ4qsv3z5ctjb28PQ0BA2NjYYP348Xr16JZQvWLAALi4uMDExQdWqVdGjRw9cv379XQ+DiIiIiEgJEwdERG9p69atmDBhAmbNmoULFy7A2dkZnp6eSEtLU1l/8+bN+PbbbzFr1iwkJCRAJpNh69atmDZtmlDn8OHDCAwMxKlTp7B//37k5OSgQ4cOyMzMfF/DIiIiIiICwMTBRyE1NRWjR49GnTp1oK+vDxsbG3Tr1g0HDx4EANSqVQsSiQSnTp0StRs3bhxcXV1FddRtAwcOLDYOVe0+//xzUR0HBwfo6+sjNTVVqb2rqyskEgkWLlyoVNalSxdIJBIEBwdrdE5K2pei/pvb8OHDlcb35nnMyspCpUqVIJFIIJfLVZ4PHR0d1KhRAxMmTEBWVpZQJyIiQuVxDQwMhDoDBw4U9uvq6qJ27dqYMmWK6NtnoOBG8ssvv4S5uTmMjIxQr149+Pv7Izs7W6NzRu/OsmXLMHToUAwaNAgNGjTA2rVrYWRkhLCwMJX1T5w4gTZt2qBfv36oVasWOnToAF9fX9EshZiYGAwcOBCOjo5wdnZGREQEkpOTcf78+fc1LCIiIiIiAEwcfPCSkpLQtGlTHDp0CIsXL0Z8fDxiYmLg5uaGwMBAoZ6BgQGmTp2qtp+zZ88iJSUFKSkp2LFjBwDg+vXrwr4VK1ZoFE94eLjQJiUlBbt37xbKjh07hpcvX6J379745ZdfVLa3sbFBRESEaN/du3dx8OBBWFpaahRDafsaOnSoKPaUlBQsWrRIqc/w8HDRvujoaEilUpUxKM7H33//jdWrV+PXX3/F3LlzRXVMTU2VjvvPP/+I6nTs2BEpKSm4ffs2QkND8dNPP2HWrFlC+dWrV9GxY0c0a9YMR44cQXx8PH788Ufo6ekhNze32HNF7052djbOnz8Pd3d3YZ+Wlhbc3d1x8uRJlW1at26N8+fPC4mC27dv43//+x86d+6s9jjp6ekAAHNz8zKMnoiIiIioeDrlHQAVbeTIkZBIJDhz5gyMjY2F/Y6OjggICBB+HjZsGNauXav25qNKlSrCnxU3HlWrVkWFChVKFE+FChVgYWGhskwmk6Ffv35o164dxo4dqzKR0bVrV0RFReH48eNo06YNAOCXX35Bhw4dkJycXKJYStqXkZGR2tgV/P398cMPP2D58uUwNDQEAISFhcHf3x9z5sxRql/4fNjY2KB79+64cOGCqI5EIin2uPr6+qJ+3N3dsX//fnz//fcAgH379sHCwkKU6Khbty46duxYZL/07j18+BC5ubmoVq2aaH+1atVw7do1lW369euHhw8f4vPPP0d+fj5ev36N4cOHix5VKCwvLw/jxo1DmzZt4OTkVOZjICIiIiIqCmccfMAeP36MmJgYBAYGipIGCoVv+mvXro3hw4cjKCgIeXl57zHKAs+ePcO2bdvg5+cHDw8PpKen4+jRo0r19PT08PXXX4u+1Y+IiBAlQTRVln0pNG3aFLVq1RJmZSQnJ+PIkSPo379/sW1v3LiBQ4cOoUWLFqU+PgBcvnwZJ06cgJ6enrDPwsICKSkpOHLkiMb9ZGVlISMjQ7TRh0Eul2P+/PlYvXo1Lly4gJ07d+KPP/5QmZwCgMDAQFy+fBmRkZHvOVIiIiIiIiYOPmg3b95Efn4+HBwcNKo/ffp0/P3339i0adM7i8nX1xdSqVTYdu3aBQCIjIxEvXr14OjoCG1tbfTt2xcymUxlHwEBAYiKikJmZiaOHDmC9PR0dO3atVTxlKSv1atXi2KXSqUqz1VAQIDwbHpERAQ6d+4smrFRmOJ8GBgYwN7eHo6OjggKChLVSU9PVzpup06dRHX27Nkj9NOwYUOkpaVh8uTJQrm3tzd8fX3Rrl07WFpaomfPnli5cmWRyYAFCxbAzMxM2GxsbNTWpdKrXLkytLW1cf/+fdH++/fvq51pMmPGDPTv3x9DhgxBw4YN0bNnT8yfPx8LFixQSvyNGjUKe/bsQWxsLKpXr/7OxkFEREREpA4TBx+w/Pz8EtWvUqUKJk2ahJkzZ76zBfNCQ0MRFxcnbB4eHgAKpvP7+fkJ9fz8/LBt2zY8e/ZMqQ9nZ2fUq1cP27dvR1hYGPr37w8dndI9NVOSvr7++mtR7HFxcfDy8lKq5+fnh5MnT+L27dvFzmBQnI9Lly5hz549uHHjhtLsBBMTE6Xjrl+/XlTHzc0NcXFxOH36NPz9/TFo0CD06tVLKNfW1kZ4eDj+/fdfLFq0CNbW1pg/fz4cHR2RkpKiMragoCCkp6cL2507d9SOg0pPT08PTZs2FRYrBQoeLTh48CBatWqlss2LFy+gpSX+61dbWxvA//1/n5+fj1GjRiE6OhqHDh1C7dq139EIiIiIiIiKxjUOPmD16tWDRCJR+5y0KhMmTMDq1auxevXqdxKThYUFbG1tRfuuXr2KU6dO4cyZM6J1DXJzcxEZGYmhQ4cq9RMQEIBVq1bh6tWrxb7vvjia9mVmZqYUuyqVKlVC165dMXjwYLx69QqdOnVSmQABxOfD3t4ez549g6+vL+bOnSvs19LSKva4xsbGQp2wsDA4OztDJpNh8ODBonrW1tbo378/+vfvjzlz5sDOzg5r167F7NmzlfrU19eHvr5+seOltzdhwgT4+/ujWbNmaN68OZYvX47MzEwMGjQIADBgwABYW1tjwYIFAIBu3bph2bJlaNKkCVq0aIGbN29ixowZ6Natm5BACAwMxObNm/Hbb7/BxMREeFOJmZmZsP4GEREREdH7wBkHHzBzc3N4enpi1apVKt/d/vTpU6V9UqkUM2bMwLx589Te7JY1mUyGtm3b4tKlS6Jv1SdMmKD2cYV+/fohPj4eTk5OaNCgwVsdvyz7UggICIBcLseAAQOEGzlNKOq+fPmy1MfW0tLCtGnTMH369CL7qVixIiwtLVVeG/R++fj4YMmSJZg5cyYaN26MuLg4xMTECAsmJicni2aGTJ8+HRMnTsT06dPRoEEDDB48GJ6envjpp5+EOmvWrEF6ejpcXV1haWkpbFu3bn3v4yMiIiKiTxtnHHzgVq1ahTZt2qB58+YICQlBo0aN8Pr1a+zfvx9r1qxBQkKCUpthw4YhNDQUmzdvfuuF+oqTk5ODX3/9FSEhIUqrvQ8ZMgTLli3DlStX4OjoKCqrWLEiUlJSoKur+9YxaNrXixcvhG9tFfT19VGxYkWluh07dsSDBw9gampaZJ9Pnz5Famoq8vLykJiYiJCQENjZ2aF+/fpCnfz8fKXjAgVvtXhzurqCt7c3Jk+ejFWrVmHSpEn46aefEBcXh549e6Ju3bp49eoVNmzYgCtXruDHH38sMkZ6P0aNGoVRo0apLJPL5aKfdXR0MGvWLNErN99U0keViIiIiIjeFc44+MDVqVMHFy5cgJubGyZOnAgnJyd4eHjg4MGDWLNmjco2urq6mDNnDl69evXO49u9ezcePXqEnj17KpXVr18f9evXVzvroEKFCirfFlEamvT1888/i765tbS0hK+vr8q6EokElStXFr3ZQJVBgwbB0tIS1atXh6+vLxwdHfHnn3+K1lnIyMhQOq6lpSXS0tLU9qujo4NRo0Zh0aJFyMzMRPPmzfH8+XMMHz4cjo6OaNeuHU6dOoVdu3ahXbt2RcZIRERERET0NiT5/FqL6JOQkZFR8HaFcVHQ0jcq73A+ekkLu5R3CEREREREpaK4N0hPTy92ljXAGQdEREREREREVAQmDggAMH/+fEilUpVbp06d3mssR48eVRuLVCp9r7EQERERERF96rg4IgEAhg8fjj59+qgse9+vfmvWrBni4uLe6zGJiIiIiIhINSYOCEDBqx/Nzc3LOwwABYkKW1vb8g6DiIiIiIiIwEcViIiIiIiIiKgInHFA9Im5PNtTo5VTiYiIiIiIAM44ICIiIiIiIqIiMHFARERERERERGoxcUBEREREREREajFxQERERERERERqMXFARERERERERGrxrQpEnxinWXuhpW9U3mF8tJIWdinvEIiIiIiI3ivOOCAiIiIiIiIitZg4ICIiIiIiIiK1mDggIiIiIiIiIrWYOCAiIiIiIiIitZg4ICIiIiIiIiK1mDggIiIiIiIiIrWYOCAiegurVq1CrVq1YGBggBYtWuDMmTNq67q6ukIikShtXbr83yse79+/j4EDB8LKygpGRkbo2LEjEhMT38dQiIiIiIhUYuKAiKiUtm7digkTJmDWrFm4cOECnJ2d4enpibS0NJX1d+7ciZSUFGG7fPkytLW14e3tDQDIz89Hjx49cPv2bfz222+4ePEiatasCXd3d2RmZr7PoRERERERCd5J4uDkyZPQ1tYWfYsGAElJSZBIJNDW1sbdu3dFZSkpKdDR0YFEIkFSUhKCg4NVfjNXeCvOwIEDhbq6urqoXbs2pkyZglevXonqqes/MjJSqJObm4vQ0FA0bNgQBgYGqFixIjp16oTjx4+X6NxkZ2dj0aJFcHZ2hpGRESpXrow2bdogPDwcOTk5ZRq3XC4X9mlpacHMzAxNmjTBlClTkJKSIuorODgYjRs3BgDUqlWryPM+cODAYsdZuL6Ojg5q1KiBCRMmICsrS+UxC1NcJ3FxcUrjkEgkqFKlCjp37oz4+HhRu8LnrfDWsWNHpWMsWLAA2traWLx4sVKZpnEVR9NrJiIiQvQ5WVpawsfHB8nJycIxi9oiIiI0iofK3rJlyzB06FAMGjQIDRo0wNq1a2FkZISwsDCV9c3NzWFhYSFs+/fvh5GRkZA4SExMxKlTp7BmzRq4uLjA3t4ea9aswcuXL7Fly5b3OTQiIiIiIsE7SRzIZDKMHj0aR44cwb1795TKra2tsWHDBtG+X375BdbW1sLPkyZNEn0zV716dYSEhIj2aaJjx45ISUnB7du3ERoaip9++gmzZs1SqhceHi7qOyUlBT169ABQ8C1g3759ERISgrFjxyIhIQFyuRw2NjZwdXXFrl27NIolOzsbnp6eWLhwIYYNG4YTJ07gzJkzCAwMxI8//ogrV66UadwK169fx71793D27FlMnToVBw4cgJOTk9KNt8LZs2eFvnbs2CH0odi3YsUKjcariO3vv//G6tWr8euvv2Lu3LkatVVFEcPevXuRlZWFLl26IDs7W1RHcd4Kb6puuMLCwjBlyhS1N3hvq6TXjKmpKVJSUnD37l3s2LED169fh7e3N2xsbERjmThxIhwdHUX7fHx83skYqGjZ2dk4f/483N3dhX1aWlpwd3fHyZMnNepDJpOhb9++MDY2BgAhsWZgYCDqU19fH8eOHSvD6ImIiIiINKdT1h0+f/4cW7duxblz55CamoqIiAhMmzZNVMff3x/h4eEICgoS9oWHh8Pf3x9z5swBAEilUkilUqFcW1sbJiYmsLCwKFE8+vr6QhsbGxu4u7tj//79+P7770X1KlSooLbvqKgobN++Hbt370a3bt2E/evWrcOjR48wZMgQeHh4CL/8q7N8+XIcOXIE586dQ5MmTYT9derUgbe3t+gmuCziVqhatapQz87ODt27d0eTJk0wYsQIlTcjVapUEf5sbm4u6qMkCsdmY2OD7t2748KFCyXqQ904xo0bBy8vL1y7dg2NGjUS6hQ+b+ocPnwYL1++REhICDZs2IATJ06gdevWpY5LlZJeMxKJRIjb0tISgwcPxpgxY5CZmSkaj1QqhY6Ojkb/H2RlZYlmeGRkZJTV8AjAw4cPkZubi2rVqon2V6tWDdeuXSu2/ZkzZ3D58mXIZDJhn4ODA2rUqIGgoCD89NNPMDY2RmhoKP7991+Nk6VERERERGWtzGccREVFwcHBAfb29vDz80NYWBjy8/NFdby8vPDkyRPhpvXYsWN48uSJ6AbrXbh8+TJOnDgBPT29ErXbvHkz7OzsVMY3ceJEPHr0CPv37y+2n02bNsHd3V2UNFDQ1dVVm3gobdzqGBoaYvjw4Th+/LjaZ7HL2o0bN3Do0CG0aNHirftKT08XHscozTmRyWTw9fWFrq4ufH19RTduZeVtrpm0tDRER0dDW1sb2trapY5hwYIFMDMzEzYbG5tS90VlTyaToWHDhmjevLmwT1dXFzt37sSNGzdgbm4OIyMjxMbGolOnTtDS4pI0RERERFQ+yvw3UZlMBj8/PwAF08bT09Nx+PBhUR1dXV0hqQAUTBv38/ODrq5uWYeDPXv2QCqVwsDAAA0bNkRaWhomT56sVM/X11eY5aDYkpOTARTc9NavX19l/4r9N27cKDaWxMREODg4vLe4i6KIIykpSaN4SkMRm4GBAezt7eHo6CiaZVJS1atXh1QqRYUKFbB582Z4eXkpnU/FeSu8zZ8/XyjPyMjA9u3bhWvUz88PUVFReP78eanjUqWk10x6ejqkUimMjY1RrVo1xMbGIjAwsNhZLEUJCgpCenq6sN25c6fUfZGyypUrQ1tbG/fv3xftv3//frEzQjIzMxEZGYnBgwcrlTVt2hRxcXF4+vQpUlJSEBMTg0ePHqFOnTplGj8RERERkabK9FGF69ev48yZM4iOji7oXEcHPj4+kMlkcHV1FdUNCAhA69atMX/+fGzbtg0nT57E69evyzIcAICbmxvWrFmDzMxMhIaGQkdHB7169VKqFxoaKnpWGQCsrKyEP785a+JNmnzzXVwfhZVV3MXFoskik6WliC03Nxc3b97EhAkT0L9/f9GikyVx9OhRGBkZ4dSpU5g/fz7Wrl2rVEdx3gpTPG4BAFu2bEHdunXh7OwMAGjcuDFq1qyJrVu3qryJexsluWZMTExw4cIF5OTk4M8//8SmTZswb968tzq+vr4+9PX136oPUk9PTw9NmzbFwYMHhXVF8vLycPDgQYwaNarIttu2bUNWVpaQwFLFzMwMQEHC8dy5c8JjXERERERE71uZJg5kMhlev36tdMOtr6+PlStXiuo2bNgQDg4O8PX1Rf369eHk5KTxavUlYWxsDFtbWwAFMxucnZ0hk8mUbhItLCyEem+qV68eEhISVJYp9tvZ2RUbi52dnUbPPpdV3EVRxF2rVq0St9VU4djs7e3x7Nkz+Pr6Yu7cubC1tYWpqSnS09OV2j19+hTA/904KdSuXRsVKlSAvb090tLS4OPjgyNHjojqFD5vqshkMly5cgU6Ov936efl5SEsLEw4tyWNS5WSXjNaWlpC3PXr18etW7cwYsQI/Prrr8Uei8rPhAkT4O/vj2bNmqF58+ZYvnw5MjMzMWjQIADAgAEDYG1tjQULFojayWQy9OjRA5UqVVLqc9u2bahSpQpq1KiB+Ph4jB07Fj169ECHDh3ey5iIiIiIiN5UZo8qvH79Ghs2bMDSpUsRFxcnbJcuXYKVlZXKle0DAgIgl8sREBBQVmEUSUtLC9OmTcP06dPx8uVLjdv5+voiMTERv//+u1LZ0qVLYWVlBQ8Pj2L76devHw4cOICLFy8qleXk5Kh9T3tp41bn5cuXWLduHdq2bStaCPFdUzyvrxiDvb09/v33X6Wp3hcuXICBgQFq1Kihtq/AwEBcvnxZmN2iifj4eJw7dw5yuVx0jcrlcpw8eVJI6rxNXApve818++232Lp161stJknvno+PD5YsWYKZM2eicePGiIuLQ0xMjLBgYnJystKihtevX8exY8fUznBJSUlB//794eDggDFjxqB///58FSMRERERlasySxzs2bMHT548weDBg+Hk5CTaevXqpXIBuqFDh+LBgwcYMmRIWYVRLG9vb2hra2PVqlWi/U+fPkVqaqpoU9zI9+3bFz169IC/vz9kMhmSkpLw119/4ZtvvsGePXuwceNGjdZnGDduHNq0aYP27dtj1apVuHTpEm7fvo2oqCi0bNkSiYmJZRq3QlpaGlJTU5GYmIjIyEi0adMGDx8+VJrSX9YUsd27dw+HDx9GSEgI7OzshGf8PT09YW9vD19fX5w4cQK3b9/G9u3bMX36dIwdO7bIhQGNjIwwdOhQzJo1S/RIQFZWltL5ePjwIYCCb3mbN2+Otm3biq7Ptm3bwsXFRbhG3yYuhbe9ZmxsbNCzZ0/MnDlTo3NN5WfUqFH4559/kJWVhdOnT4sWAJXL5YiIiBDVt7e3R35+vtrE0ZgxY3Dnzh1kZ2fjn3/+wZw5c8psYVQiIiIiotIos8SBTCaDu7u7ymncvXr1wrlz55ReB6ejo4PKlSuLpo2/azo6Ohg1ahQWLVokusEeNGgQLC0tRduPP/4IoGAdgG3btmHatGkIDQ2Fvb09nJ2dsX37dly8eBFubm4aHVtfXx/79+/HlClT8NNPP6Fly5ZwcXHBDz/8gDFjxsDJyalM41awt7eHlZUVmjZtioULF8Ld3R2XL19GgwYNSnLqSkwRW/Xq1eHr6wtHR0f8+eefwueto6ODffv2oUaNGvD19YWTkxNmzZqFsWPHavQ896hRo5CQkIBt27YJ+2JiYpTOx+eff47s7Gxs3LhR5ToRQME1umHDBuTk5Lx1XEDZXDPjx4/HH3/8gTNnzmh0TCIiIiIiondBkl+SFftIcOHCBbi7u2Pw4MFYvHhxeYdDH4HyvmYyMjIKXss4Lgpa+kbv/fj/FUkLu5R3CEREREREb0Vxb5Ceng5TU9Ni6/PF4KX02Wef4eDBgzA2NsatW7fKOxz6CPCaISIiIiKij9FHmzhITk6GVCpVuyUnJ7/zGJo0aYLg4GDUrVsXAODo6Kg2nk2bNr3zeN6X+fPnqx1np06dyju896JTp05qz8H8+fPVtnvzmiEiIiIiIvrQvb/FBcqYlZVVka9vLPxKyPflf//7H3JyclSWKVZZ/y8YPnw4+vTpo7LM0NDwPUdTPtavX6/2DRfm5ubvORoiIiIiIqJ356NNHOjo6Ajvvf9Q1KxZs7xDeC/Mzc0/+Ztja2vr8g6BiIiIiIjovfhoH1UgIiIiIiIionfvo51xQESlc3m2p0YrpxIREREREQGccUBERERERERERWDigIiIiIiIiIjUYuKAiIiIiIiIiNRi4oCIiIiIiIiI1GLigIiIiIiIiIjU4lsViD4xTrP2QkvfqLzD+KAlLexS3iEQEREREX0wOOOAiIiIiIiIiNRi4oCIiIiIiIiI1GLigIiIiIiIiIjUYuKAiIiIiIiIiNRi4oCIiIiIiIiI1GLigIiIiIiIiIjUYuKAiKgYq1atQq1atWBgYIAWLVrgzJkzauu6urpCIpEobV26/N8rHoODg+Hg4ABjY2NUrFgR7u7uOH369PsYChERERFRib114uDkyZPQ1tYW/VIMAElJSZBIJNDW1sbdu3dFZSkpKdDR0YFEIkFSUhKCg4NV/qJdeCvOwIEDhbq6urqoXbs2pkyZglevXonqqes/MjJSqJObm4vQ0FA0bNgQBgYGqFixIjp16oTjx4+X6NxkZ2dj0aJFcHZ2hpGRESpXrow2bdogPDwcOTk5ZRq3XC4X9mlpacHMzAxNmjTBlClTkJKSIuorODgYjRs3BgDUqlWryPM+cODAYsdZuL6Ojg5q1KiBCRMmICsrS+UxC1NcJ3FxcUrjkEgkqFKlCjp37oz4+HhRu8LnrfDWsWNHpWMsWLAA2traWLx4sVKZpnEVRRFzxYoVlT63s2fPqryGNb3GIiIiVI7r6dOnkEgkkMvlxcZHb2fr1q2YMGECZs2ahQsXLsDZ2Rmenp5IS0tTWX/nzp1ISUkRtsuXL0NbWxve3t5CHTs7O6xcuRLx8fE4duwYatWqhQ4dOuDBgwfva1hERERERBp768SBTCbD6NGjceTIEdy7d0+p3NraGhs2bBDt++WXX2BtbS38PGnSJNEv2tWrV0dISIhonyY6duyIlJQU3L59G6Ghofjpp58wa9YspXrh4eGivlNSUtCjRw8AQH5+Pvr27YuQkBCMHTsWCQkJkMvlsLGxgaurK3bt2qVRLNnZ2fD09MTChQsxbNgwnDhxAmfOnEFgYCB+/PFHXLlypUzjVrh+/Tru3buHs2fPYurUqThw4ACcnJyUbrwVzp49K/S1Y8cOoQ/FvhUrVmg0XkVsf//9N1avXo1ff/0Vc+fO1aitKooY9u7di6ysLHTp0gXZ2dmiOorzVnjbsmWLUl9hYWGYMmUKwsLCSh2PJkxMTBAdHS3aJ5PJUKNGDdG+kl5jOjo6OHDgAGJjY99p/KTasmXLMHToUAwaNAgNGjTA2rVrYWRkpPZ6Mjc3h4WFhbDt378fRkZGosRBv3794O7ujjp16sDR0RHLli1DRkYG/vrrr/c1LCIiIiIijem8TePnz59j69atOHfuHFJTUxEREYFp06aJ6vj7+yM8PBxBQUHCvvDwcPj7+2POnDkAAKlUCqlUKpRra2vDxMQEFhYWJYpHX19faGNjYwN3d3fs378f33//vahehQoV1PYdFRWF7du3Y/fu3ejWrZuwf926dXj06BGGDBkCDw8PGBsbFxnL8uXLceTIEZw7dw5NmjQR9tepUwfe3t6im+CyiFuhatWqQj07Ozt0794dTZo0wYgRI3Ds2DGl+lWqVBH+bG5uLuqjJArHZmNjg+7du+PChQsl6kPdOMaNGwcvLy9cu3YNjRo1EuoUPm/qHD58GC9fvkRISAg2bNiAEydOoHXr1qWOqyj+/v4ICwuDr68vAODly5eIjIzEmDFjhGsdKPk1ZmxsjD59+uDbb7/ldPb3LDs7G+fPnxf9/aWlpQV3d3ecPHlSoz5kMhn69u2r9u+M7OxsrFu3DmZmZnB2di6TuImIiIiIytJbzTiIioqCg4MD7O3t4efnh7CwMOTn54vqeHl54cmTJ8JN67Fjx/DkyRPRDdO7cPnyZZw4cQJ6enolard582bY2dmpjG/ixIl49OgR9u/fX2w/mzZtgru7uyhpoKCrq6v2JqK0catjaGiI4cOH4/jx42qnVpe1Gzdu4NChQ2jRosVb95Weni48jlGacyKTyeDr6wtdXV34+vpCJpO9dUzq9O/fH0ePHkVycjIAYMeOHahVqxY+++wzUb3SXGPBwcGIj4/Hmpko/QAAOntJREFU9u3bNY4nKysLGRkZoo1K5uHDh8jNzUW1atVE+6tVq4bU1NRi2585cwaXL1/GkCFDlMr27NkDqVQKAwMDhIaGYv/+/ahcuXKZxU5EREREVFbeKnEgk8ng5+cHoGDaeHp6Og4fPiyqo6urKyQVgIJp435+ftDV1X2bQ6tU+Bfxhg0bIi0tDZMnT1aq5+vrK8xyUGyKm70bN26gfv36KvtX7L9x40axsSQmJsLBweG9xV0URRxJSUkaxVMaitgMDAxgb28PR0dH0be0JVW9enVIpVJUqFABmzdvhpeXl9L5VJy3wtv8+fOF8oyMDGzfvl24Rv38/BAVFYXnz5+XOq6iVK1aFZ06dUJERASAgms9ICBAqV5prjErKyuMHTsW3333HV6/fq1RPAsWLICZmZmw2djYlGA0VBZkMhkaNmyI5s2bK5W5ubkhLi4OJ06cQMeOHdGnT5/3ltwjIiIiIiqJUicOrl+/jjNnzgjTsnV0dODj46PyG92AgABs27YNqamp2LZtm8qbqbKg+EX89OnT8Pf3x6BBg9CrVy+leqGhoYiLixNtVlZWQvmbsybepMk338X18S7iLi4WTRaZLC1FbJcuXcKePXtw48YN9O/fv9T9HT16FOfPn0dERATs7Oywdu1apTqK81Z4Gz58uFC+ZcsW1K1bV5j+3bhxY9SsWRNbt24tdVzFCQgIQEREBG7fvo2TJ0/i66+/VlmvJNeHwtSpU/HgwQON12oICgpCenq6sN25c6fEx/zUVa5cGdra2rh//75o//3794t9TCYzMxORkZEYPHiwynJjY2PY2tqiZcuWkMlk0NHReaczYoiIiIiISqvUaxzIZDK8fv1a6YZbX18fK1euFNVt2LAhHBwc4Ovri/r168PJyUmj1epLSvGLOFDwba+zszNkMpnSL+4WFhZCvTfVq1cPCQkJKssU++3s7IqNxc7ODteuXXtvcRdFEXetWrVK3FZThWOzt7fHs2fP4Ovri7lz58LW1hampqZIT09Xavf06VMAgJmZmWh/7dq1UaFCBdjb2yMtLQ0+Pj44cuSIqE7h86aKTCbDlStXoKPzf5d5Xl4ewsLChHNb0riK06lTJwwbNgyDBw9Gt27dUKlSJaU6dnZ2pbrGKlSogKCgIMyePRtdu3YtNhZ9fX3o6+uXKH4S09PTQ9OmTXHw4EFhIdK8vDwcPHgQo0aNKrLttm3bkJWVJcx4KU5eXp7oTSRERERERB+KUs04eP36NTZs2IClS5eKvu29dOkSrKysVK5sHxAQALlc/s5mG7xJS0sL06ZNw/Tp0/Hy5UuN2/n6+iIxMRG///67UtnSpUthZWUFDw+PYvvp168fDhw4gIsXLyqV5eTkIDMzs0zjVufly5dYt24d2rZtK1oI8V3T1tYWjg8UJBP+/fdfpW9uL1y4AAMDA6U3DxQWGBiIy5cvK72xoCjx8fE4d+4c5HK56BqVy+U4efKkkNR5m7hU0dHRwYABA4q81vv27VvkNVapUiW119jo0aOhpaWl8dsu6O1NmDABP//8M3755RckJCRgxIgRyMzMxKBBgwAAAwYMUPlYjkwmQ48ePZSSR5mZmZg2bRpOnTqFf/75B+fPn0dAQADu3r0revMCEREREdGHolSJgz179uDJkycYPHgwnJycRFuvXr1UTrcdOnQoHjx4oHKRsHfF29sb2traWLVqlWj/06dPkZqaKtoUN/J9+/ZFjx494O/vD5lMhqSkJPz111/45ptvsGfPHmzcuFGj9RnGjRuHNm3aoH379li1ahUuXbqE27dvIyoqCi1btkRiYmKZxq2QlpaG1NRUJCYmIjIyEm3atMHDhw+xZs0aTU9bqShiu3fvHg4fPoyQ/9fevcf1eP5/AH99lM6nRaVIoQNSDiHEklPOh+8oh60Q+6LQHIephlVzns35ULax2EbMISrCHJM12lKRyqycD9Wo1P37w7f75/bp04HyyXo9H4/78fC57+u+rvd9sLnf93Vd96JFsLGxEcfsu7m5wdbWFqNGjcKZM2eQlpaGn376CZ999hmmT58uJhpKo6WlhYkTJyIgIEDSxT8/P1/ufNy7dw/Ai4e2jh074v3335fcn++//z46dOgg3qNvEpciixcvxt27d+Hm5lbq9pEjR2LYsGGl3mP79+/Hli1bFE6eqaGhgc8//xxr1qypdFz0ejw8PLB8+XL4+/ujTZs2SEhIQGRkpDhhYmZmptwnY5OTk/Hrr7+WOkxBRUUFV69exQcffCBOknn//n2cOnUKdnZ2b+WYiIiIiIgq47USB1u3bkWvXr1K7cb9wQcf4OLFi3IzuKuqqqJ+/fqSbuPVTVVVFb6+vli6dKnkAXvcuHEwNTWVLF9//TWAF/MA/Pjjj5g/fz5WrVoFW1tbtG7dGj/99BN+++03uLq6VqhtdXV1REVFYc6cOdi4cSM6deqEDh06YM2aNZg2bRpatWpVpXGXsLW1hZmZGRwdHRESEoJevXohMTERLVu2rMypq7SS2Bo1aoRRo0bBzs4Ohw8fFq+3qqoqjh49isaNG2PUqFFo1aoVAgICMH36dMmnChXx9fVFUlISfvzxR3FdZGSk3Pno2rUrCgoK8P3335c6TwTw4h799ttvUVhY+MZxlUZNTQ3169dXOKeETCbD7t27JfdYt27dkJGRgdjYWLFLvCJeXl5o2rTpa8VGr8fX1xcZGRnIz8/H+fPnJV8MiY2NFSfELGFrawtBEErtOaKhoYE9e/bg1q1byM/Px99//419+/ahQ4cO1X0YRERERESvRSa8zixttcylS5fQq1cveHt7Y9myZcoOh+i1PHny5MXXFfx2o466lrLDqdHSQwYoOwQiIiIiompT8mzw+PFj6OnplVv+jT7HWFu0a9cOMTEx0NbWxvXr15UdDhEREREREdFb804kDjIzM6Gjo6NwyczMrPYY2rZti8DAQDRr1gwAYGdnpzCeHTt2VHs8b0tQUJDC4+zXr5+yw3sr+vXrp/AcBAUFKTs8IiIiIiKiavX2Jhx4A2ZmZmV+vvHlT0K+LYcOHUJhYWGp20omTfs3mDRpEtzd3Uvdpqmp+ZajUY4tW7Yo/MKFoaHhW46GiIiIiIjo7XonEgeqqqqwsrJSdhgSFhYWyg7hrTA0NKz1D8cNGzZUdghERERERERK804MVSAiIiIiIiIi5WDigIiIiIiIiIgUeieGKhBR1Un83K1Cn1whIiIiIiIC2OOAiIiIiIiIiMrAxAERERERERERKcTEAREREREREREpxMQBERERERERESnExAERERERERERKcSvKhDVMq0CjqCOupayw6jR0kMGKDsEIiIiIqIagz0OiIiIiIiIiEghJg6IiIiIiIiISCEmDoiIiIiIiIhIISYOiIiIiIiIiEghJg6IiIiIiIiISCEmDoiIiIiIiIhIISYOiIjKsHbtWlhaWkJDQwNOTk64cOGCwrLdu3eHTCaTWwYMePF5x8LCQsydOxf29vbQ1taGmZkZPD098ffff7+twyEiIiIiqrQqSRyMHTtW/Ady3bp10aRJE8yZMwfPnj0Ty5T2j2mZTIbw8HCxjCAI2Lx5Mzp37gw9PT3o6OjAzs4O06dPx7Vr18RygYGBaNOmjSSGBw8ewM/PDxYWFlBTU4OZmRnGjx+PzMzMUmMNCQmRrI+IiIBMJqvQ8cbGxkqOwcjICP3798eVK1dKLe/m5gYVFRXExcUBANLT0xWej5IlLCxMbOfRo0eSdu3s7FBUVCRpw8DAAGFhYZJ1v/32Gzw8PGBqagp1dXVYWFhg4MCB+OWXXyAIQoWOFQB+/vlndO/eHfr6+tDR0YGDgwMWLVqEBw8eAADCwsJgYGCgcP+xY8di6NChkt+lHXPfvn3FMpaWlpDJZDh37pykLj8/P3Tv3l1SRtEyduzYMo+rU6dOmDRpkmTdhg0bxPP/6jF069atzPpKCIKATZs2wcnJCTo6OjAwMED79u2xevVq/PPPPwCk93B5xzF69GhoaWlh586dknaKi4vRpUsXDB8+vEJxUeXt2rULM2bMQEBAAC5duoTWrVvDzc0Nd+7cKbX8nj17kJWVJS6JiYlQUVHBiBEjAAD//PMPLl26hIULF+LSpUvYs2cPkpOTMXjw4Ld5WERERERElVJlPQ769u2LrKwspKWlYdWqVdi4cSMCAgIkZUJDQyX/qM7KyhIfKAVBwOjRozFt2jT0798fR48exZ9//omtW7dCQ0MDS5YsUdj2gwcP0KlTJ0RHR2PDhg24du0awsPDce3aNXTo0AFpaWmS8hoaGvjyyy/x8OHDNzrm5ORkZGVl4ciRI8jPz8eAAQNQUFAgKZOZmYkzZ87A19cX27ZtAwCYm5tLzsHMmTNhZ2cnWefh4aGw3bS0NHz77bdlxrZv3z506tQJubm52L59O5KSkhAZGYlhw4bhs88+w+PHjyt0jAsWLICHhwc6dOiAw4cPIzExEStWrMDvv/+O7777rkJ1lKbkfnl5+eGHHyRlNDQ0MHfuXIV1xMXFifv+/PPPAP7/mmRlZeGrr74qMwZXV1fExsZK1h0/fhzm5uZy62NjY9GjR48KHdtHH30EPz8/DBkyBMePH0dCQgIWLlyIffv24ejRo5U+jvXr1yMkJARTp05FVlaWuN+KFSuQlpaGDRs2VCguqryVK1di4sSJGDduHFq2bIkNGzZAS0tL/Lv8KkNDQzRo0EBcoqKioKWlJSYO9PX1ERUVBXd3d9ja2qJTp0745ptvEB8fL5fkJCIiIiKqKVSrqiJ1dXU0aNAAwIsH4169eiEqKgpffvmlWMbAwEAs86pdu3YhPDwc+/btk7x9a9y4MTp16lTmG/IFCxbg77//xrVr18T6GzdujCNHjsDa2ho+Pj44fPiwWL5Xr164du0agoODsXTp0tc+ZmNjY/GY/Pz8MHjwYFy9ehUODg5imdDQUAwcOBCTJ09Gp06dsHLlSmhqakrOg46ODlRVVRWem1dNnToVAQEBGD16NNTV1eW25+XlwdvbGwMGDMCePXsk21q0aAFvb+8K9Ti4cOECgoKCsHr1akyfPl1cb2lpid69e4s9IV7Hy/eLIh9//DE2bNiAQ4cOoX///nLbjYyMxD8bGhoC+P9rUhGurq4ICQlBdna2GMuJEyfg7+8vuS9u3LiBjIwMuLq6llvn7t27sWPHDkRERGDIkCHiektLSwwePBhPnjx5reOYOnUqIiIiMHHiRBw4cABXr16Fv78/du3ahfr161foeKlyCgoKEB8fj3nz5onr6tSpg169euHs2bMVqmPr1q0YOXIktLW1FZZ5/PgxZDJZhe9bIiIiIqK3rVrmOEhMTMSZM2egpqZW4X1++OEH2NraKuyyq2gYQXFxMcLDwzFmzBi5B1FNTU1MmTIFR44cEbvVA4CKigqCgoLw9ddf46+//qpwjIo8fvxYHHLx8jELgoDQ0FB8+OGHaN68OaysrPDTTz+9cXt+fn54/vw5vv7661K3Hz16FPfv38ecOXMU1lGRYRk7duyAjo4OpkyZUur26n7QadKkCSZNmoR58+ahuLi4yut3dnZG3bp1cfz4cQDAn3/+iadPn8Lb2xv379/HjRs3ALzohaChoYHOnTuXW+eOHTtga2srSRqUkMlk0NfXf61YZTIZQkNDcerUKWzevBljx47FyJEjy+zinp+fjydPnkgWqrh79+6hqKgIJiYmkvUmJibIzs4ud/8LFy4gMTEREyZMUFjm2bNnmDt3LkaNGgU9Pb03jpmIiIiIqDpUWeLgwIED0NHRgYaGBuzt7XHnzh3Mnj1bUmbUqFHQ0dGRLCXdc1NSUmBraysp7+fnJ5Zr1KhRqe3evXsXjx49QosWLUrd3qJFCwiCIJkjAQCGDRuGNm3ayA2nqIxGjRqJY9h37tyJwYMHo3nz5uL26Oho/PPPP3BzcwMAfPjhh9i6detrt1dCS0sLAQEBCA4OLnXIQUpKCgBIzmdcXJzkvB84cKDcdlJTU9G0aVPUrVv3jWN+Vcn98vISFBQkV+6zzz7DjRs3sGPHjiqPQVtbGx07dhSHJcTGxqJr165QV1dHly5dJOs7d+5cau+OV6Wmpsrdx1XFwsICq1evxqRJkyo0FCM4OBj6+vriYm5uXi1xUem2bt0Ke3t7dOzYsdTthYWFcHd3hyAIWL9+/VuOjoiIiIio4qosceDq6oqEhAScP38eXl5eGDduHD744ANJmVWrViEhIUGymJmZKaxzwYIFSEhIgL+/P3Jzc8tsvzKT/ZX48ssvxfH/r+PUqVOIj49HWFgYbGxs5Maab9u2DR4eHlBVfTEiZNSoUTh9+jSuX7/+Wu29zNvbG/Xq1ZMMBSmLg4ODeM7z8vLw/Pnzcvd5nXNaUSX3y8vLqxMVAi+68c+aNQv+/v5y80dUhe7du0sSBCUTL7q4uEjWV2SYAlC95wwAxo0bB1NTU0ydOrXcN9Tz5s3D48ePxeXmzZvVGtu/Tf369aGiooLbt29L1t++fbvcYTZ5eXkIDw+Ht7d3qdtLkgYZGRmIiopibwMiIiIiqtGqLHGgra0NKysrtG7dGtu2bcP58+fl3q43aNAAVlZWkqXkodra2hrJycmS8kZGRrCysoKxsbHCdo2MjGBgYKDw4T8pKQkymQxWVlZy295//324ublJxjBXRpMmTWBrawsvLy9MmDBBMqHhgwcPsHfvXqxbtw6qqqpQVVVFw4YN8fz5c4UTq1WGqqoqvvjiC3z11Vdyn3KztrYGAMn5VFdXF895RdnY2CAtLQ2FhYVvHO+rSu6Xl5eS8f2vmjFjBp4+fYp169ZVeRyurq5ISUnBrVu3EBsbCxcXFwD/nzi4fv06bt68WeGJEW1sbHD16tUqj/NlJfdTedTV1aGnpydZqOLU1NTg6OiImJgYcV1xcTFiYmLKHbby448/Ij8/Hx9++KHctpKkQWpqKqKjo1GvXr0qj52IiIiIqCpVyxwHderUwfz58/HZZ5/h6dOnFdpn1KhRSE5Oxr59+yrdlru7O3bu3Ck37rjkYdPNzU3hQ2lISAh++eWXCk92poiPjw8SExOxd+9eAC/Gujdq1Ai///675K36ihUrEBYWJvc5xdcxYsQI2NnZ4fPPP5es79OnDwwNDSvcG0GR0aNHIzc3V+ED+5tMjlgZOjo6WLhwIb744gvk5ORUad1dunSBmpoa1q1bh2fPnsHR0REA0KFDB9y9exfbtm0ThzRUxOjRo5GSklLqfSwIQoW/ZkE1w4wZM7B582axZ9LkyZORl5eHcePGAQA8PT1LTTxu3boVQ4cOlUsKFBYWYvjw4bh48SJ27NiBoqIiZGdnIzs7u1p61BARERERVYVqSRwALx5qVVRUsHbtWnHdo0ePxH8klyx5eXkAgJEjR2L48OEYOXIkFi1ahPPnzyM9PR0nTpzArl27oKKiorCtoKAgNGjQAL1798bhw4dx8+ZNnDx5Em5ubigsLJTE8Cp7e3uMGTMGa9aseaPj1dLSwsSJExEQEABBELB161YMHz4crVq1kize3t64d+8eIiMj36i9EiEhIdi2bZt4HoEXD9pbtmzBwYMHMWDAABw5cgRpaWm4fPmy+LWAss5nCScnJ8yZMwczZ87EnDlzcPbsWWRkZCAmJgYjRozA9u3bxbJFRUVyQw/KGgKSn58vdy/cu3dPYfmPP/4Y+vr62LlzZ0VOS4VpamqiU6dO+Prrr+Hs7CyeFzU1Ncn6is7z4O7uDg8PD4waNQpBQUG4ePEiMjIycODAAfTq1UuciJHeDR4eHli+fDn8/f3Rpk0bJCQkIDIyUpwwMTMzU/KJTOBFT59ff/211GEKt27dwv79+/HXX3+hTZs2MDU1FZczZ868lWMiIiIiIqqsakscqKqqwtfXF0uXLhUfakvGZ7+8lHwZQCaTYdeuXVi9ejUOHTqEnj17wtbWFuPHj4e5uTl+/fVXhW3Vq1cP586dg6urK/773/+iWbNmcHd3R7NmzRAXF4emTZuWGeuiRYuqZNZ+X19fJCUlYenSpfj999/l5ngAXnzHvWfPnlUySSIA9OjRAz169JCbs2DYsGE4c+YMtLS04OnpCVtbW/To0QPHjh1DeHg4Bg4cWKH6v/zyS+zcuRPnz5+Hm5sb7OzsMGPGDDg4OMDLy0ssl5ubi7Zt20qWQYMGKaw3MjJS7l7o2rWrwvJ169bF4sWL8ezZswrFXRmurq7IyckR5zco4eLigpycnArPbwC8uI937tyJlStXIiIiAi4uLnBwcEBgYCCGDBkiTpRJ7w5fX19kZGQgPz8f58+fh5OTk7gtNjYWYWFhkvK2trYQBAG9e/eWq8vS0hKCIJS6vHr/ERERERHVFDKhumdzI6Ia4cmTJy++ruC3G3XUtZQdTo2WHjJA2SEQEREREVWbkmeDx48fV2gutGrrcUBERERERERE7z4mDkrRr18/6OjolLoEBQUpO7wqM2nSJIXHWdqnEd81QUFBCo+vX79+r1Vnbbk3iIiIiIiISnCoQilu3bql8GsQhoaGCr/Q8K65c+cOnjx5Uuo2PT29Mj+D+S548OABHjx4UOo2TU1NNGzYsNJ1vsv3BocqVByHKhARERHRv1llhyqU/zH4Wuh1HijfRcbGxu98cqAs1fEgX1vuDSIiIiIiohIcqkBERERERERECrHHAVEtk/i5W4W6IxEREREREQHscUBEREREREREZWDigIiIiIiIiIgUYuKAiIiIiIiIiBRi4oCIiIiIiIiIFGLigIiIiIiIiIgUYuKAiIiIiIiIiBTi5xiJaplWAUdQR11L2WHUGOkhA5QdAhERERFRjcYeB0RERERERESkEBMHRERERERERKQQEwdEREREREREpBATB0RERERERESkEBMHRERERERERKQQEwdERP+zdu1aWFpaQkNDA05OTrhw4UKZ5R89egQfHx+YmppCXV0dNjY2OHTokLjd0tISMplMbvHx8anuQyEiIiIiqjL8HCMREYBdu3ZhxowZ2LBhA5ycnLB69Wq4ubkhOTkZxsbGcuULCgrQu3dvGBsb46effkLDhg2RkZEBAwMDsUxcXByKiorE34mJiejduzdGjBjxNg6JiIiIiKhKsMdBLZOdnY2pU6eiadOmUFdXh7m5OQYNGoSYmBixzJkzZ9C/f3+899570NDQgL29PVauXCl5AAIgeYOqra0Na2trjB07FvHx8ZJysbGxpb51lclkyM7OrlDcT548wcKFC2FnZwdNTU3Uq1cPHTp0wNKlS/Hw4UOxXPfu3UttZ9KkSZK4NTQ0kJGRIWlj6NChGDt2rPh77Nix4v5169aFiYkJevfujW3btqG4uFiyr6I3yyEhIQCA9PR0yXpDQ0O4uLjg1KlTFTp+AAgMDCy1jejo6ArXQYqtXLkSEydOxLhx49CyZUts2LABWlpa2LZtW6nlt23bhgcPHiAiIgLOzs6wtLSEi4sLWrduLZYxMjJCgwYNxOXAgQNo1qwZXFxc3tZhERERERG9MSYOapH09HQ4Ojri2LFjWLZsGa5cuYLIyEi4urqKXaf37t0LFxcXNGrUCMePH8fVq1cxffp0LFmyBCNHjoQgCJI6Q0NDkZWVhT/++ANr165Fbm4unJyc8O2338q1n5ycjKysLMlS2pvcVz148ACdOnVCaGgoZs2ahfPnz+PSpUv44osv8Ntvv2Hnzp2S8hMnTpRrZ+nSpZIyMpkM/v7+5bbdt29fZGVlIT09HYcPH4arqyumT5+OgQMH4vnz55KyixYtkmt36tSpkjLR0dHIysrCyZMnYWZmhoEDB+L27dvlxlHCzs5Oro3333+/wvtT6QoKChAfH49evXqJ6+rUqYNevXrh7Nmzpe6zf/9+dO7cGT4+PjAxMUGrVq0QFBQkl2B7uY3vv/8e48ePh0wmq5bjICIiIiKqDhyqUItMmTIFMpkMFy5cgLa2trjezs4O48ePR15eHiZOnIjBgwdj06ZN4vYJEybAxMQEgwcPxu7du+Hh4SFuMzAwQIMGDQC8eOvep08feHl5wdfXF4MGDcJ7770nljU2NpZ0466o+fPnIzMzEykpKTAzMxPXW1hYoE+fPnLJDC0tLTEmRXx9fbFy5UrMnj0brVq1UlhOXV1drKthw4Zo164dOnXqhJ49eyIsLAwTJkwQy+rq6pbbbr169cS3z/Pnz0d4eDjOnz+PwYMHl7lfCVVV1XLboMq7d+8eioqKYGJiIllvYmKCq1evlrpPWloajh07hjFjxuDQoUO4du0apkyZgsLCQgQEBMiVj4iIwKNHjyS9WoiIiIiI3gXscVBLPHjwAJGRkfDx8ZEkDUoYGBjg6NGjuH//PmbNmiW3fdCgQbCxscEPP/xQbluffPIJcnJyEBUV9cZxFxcXY9euXfjwww8lSYOXvc7bW2dnZwwcOBCffvpppfft0aMHWrdujT179lR63xJPnz4Ve2Woqam9dj1lyc/Px5MnTyQLVZ3i4mIYGxtj06ZNcHR0hIeHBxYsWIANGzaUWn7r1q3o16+fwvuYiIiIiKimYuKglrh27RoEQUDz5s0VlklJSQEAtGjRotTtzZs3F8uUpaSN9PR0yfpGjRpBR0dHXOzs7Mqt6+7du3j06BFsbW0l6x0dHcV6Ro0aJdm2bt06STs6OjrYsWOHXN3BwcGIjIys1DwDJZo3by53fHPnzpVr99W6u3TpAh0dHWhra2P58uVwdHREz549K9zulStXJPV37NhRYdng4GDo6+uLi7m5eaWOsTapX78+VFRU5IaN3L59W2EPD1NTU9jY2EBFRUVc16JFC2RnZ6OgoEBSNiMjA9HR0ZIeKkRERERE7woOVaglXu3OX1Vly9r/1Z4Ap06dgq6urvi7bt26r93G3r17UVBQgLlz5+Lp06eSbWPGjMGCBQsk617tgg4ALVu2hKenJz799FOcPn26Uu0LgiB3fLNnz5brht6wYUPJ7127dqF58+ZITEzEnDlzEBYWVqnzYGtri/3794u/1dXVFZadN28eZsyYIf5+8uQJkwcKqKmpwdHRETExMRg6dCiAFz0KYmJi4OvrW+o+zs7O2LlzJ4qLi1GnzoscbEpKCkxNTeV6kYSGhsLY2BgDBgyo1uMgIiIiIqoOTBzUEtbW1pDJZArHawOAjY0NACApKQldunSR256UlISWLVuW21ZSUhIAoEmTJpL1TZo0qfQcB0ZGRjAwMEBycrJkfePGjQG8mFfg0aNHkm36+vqwsrKqUP2ff/45bGxsEBERUam4kpKS5I6vfv365bZrbm4Oa2trWFtb4/nz5xg2bBgSExPLTAC8TE1NrcLHpq6uXuF6CZgxYwa8vLzQvn17dOzYEatXr0ZeXh7GjRsHAPD09ETDhg0RHBwMAJg8eTK++eYbTJ8+HVOnTkVqaiqCgoIwbdo0Sb3FxcUIDQ2Fl5cXVFX5n1wiIiIievdwqEItYWhoCDc3N6xduxZ5eXly2x89eoQ+ffrA0NAQK1askNu+f/9+pKamyg0LKM3q1auhp6cnmaH+ddWpUwfu7u74/vvv8ffff79xfa8yNzeHr68v5s+fr3A2/FcdO3YMV65cwQcffPBGbQ8fPhyqqqpYt27dG9VDVcPDwwPLly+Hv78/2rRpg4SEBERGRoq9VTIzM5GVlSWWNzc3x5EjRxAXFwcHBwdMmzYN06dPl5s3Izo6GpmZmRg/fvxbPR4iIiIioqrC11+1yNq1a+Hs7IyOHTti0aJFcHBwwPPnzxEVFYX169cjKSkJGzduxMiRI/Hxxx/D19cXenp6iImJwezZszF8+HC4u7tL6nz06BGys7ORn5+PlJQUbNy4EREREfj222/lehfcuXMHz549k6yrV69euV31g4KCEBsbK8bdvn17aGtr4/Llyzh79qzcVxH++ecfZGdnS9apq6tLvvDwsnnz5mHz5s24ceOG5IsRwIsJBrOzs1FUVITbt28jMjISwcHBGDhwIDw9PSVlc3Jy5NrV0tKCnp5eqe3KZDJMmzYNgYGB+O9//wstLa0yzwNVP19fX4VDE2JjY+XWde7cGefOnSuzztK+/EFERERE9C5hj4NapGnTprh06RJcXV0xc+ZMtGrVCr1790ZMTAzWr18P4MVb8OPHjyMzMxPdunWDra0tVq1ahQULFiA8PFxuXP+4ceNgamqK5s2bY/LkydDR0cGFCxcwevRoufZtbW1hamoqWeLj48uNu169erhw4QI8PT2xbNkydOzYEfb29ggMDISHhwc2b94sKb9582a5dsrqKWFoaIi5c+fKJTUAIDIyEqamprC0tETfvn1x/PhxrFmzBvv27ZNMigcA/v7+cu3OmTOnzGPz8vJCYWEhvvnmm3LPAxERERERkTLIBL4KI6oVnjx58uLrCn67UUedvRtKpIdwwkIiIiIiql1Kng0eP36ssIf0y9jjgIiIiIiIiIgUYuKAlE5HR0fhcurUKWWH99bwPBARERERUU3EyRFJ6RISEhRua9iw4dsLRMl4HoiIiIiIqCZi4oCUzsrKStkh1Ag8D0REREREVBNxqAIRERERERERKcQeB0S1TOLnbhWaOZWIiIiIiAhgjwMiIiIiIiIiKgN7HBDVEoIgAHjxzVYiIiIiIqq9Sp4JSp4RysPEAVEtcf/+fQCAubm5kiMhIiIiIqKaICcnB/r6+uWWY+KAqJYwNDQEAGRmZlboPw70djx58gTm5ua4efMm556oQXhdaiZel5qJ16Xm4TWpmXhdaqbael0EQUBOTg7MzMwqVJ6JA6Jaok6dF1Oa6Ovr16r/KL4r9PT0eF1qIF6XmonXpWbidal5eE1qJl6Xmqk2XpfKvEzk5IhEREREREREpBATB0RERERERESkEBMHRLWEuro6AgICoK6uruxQ6CW8LjUTr0vNxOtSM/G61Dy8JjUTr0vNxOtSMTKhot9fICIiIiIiIqJahz0OiIiIiIiIiEghJg6IiIiIiIiISCEmDoiIiIiIiIhIISYOiIiIiIiIiEghJg6IiIiIiIiISCEmDohqibVr18LS0hIaGhpwcnLChQsXlB1SrXby5EkMGjQIZmZmkMlkiIiIUHZIBCA4OBgdOnSArq4ujI2NMXToUCQnJys7rFpt/fr1cHBwgJ6eHvT09NC5c2ccPnxY2WHRK0JCQiCTyeDn56fsUGq1wMBAyGQyydK8eXNlh0UAbt26hQ8//BD16tWDpqYm7O3tcfHiRWWHVatZWlrK/X2RyWTw8fFRdmg1EhMHRLXArl27MGPGDAQEBODSpUto3bo13NzccOfOHWWHVmvl5eWhdevWWLt2rbJDoZecOHECPj4+OHfuHKKiolBYWIg+ffogLy9P2aHVWo0aNUJISAji4+Nx8eJF9OjRA0OGDMEff/yh7NDof+Li4rBx40Y4ODgoOxQCYGdnh6ysLHH59ddflR1Srffw4UM4Ozujbt26OHz4MP7880+sWLEC7733nrJDq9Xi4uIkf1eioqIAACNGjFByZDWTTBAEQdlBEFH1cnJyQocOHfDNN98AAIqLi2Fubo6pU6fi008/VXJ0JJPJsHfvXgwdOlTZodAr7t69C2NjY5w4cQLvv/++ssOh/zE0NMSyZcvg7e2t7FBqvdzcXLRr1w7r1q3DkiVL0KZNG6xevVrZYdVagYGBiIiIQEJCgrJDoZd8+umnOH36NE6dOqXsUKgMfn5+OHDgAFJTUyGTyZQdTo3DHgdE/3IFBQWIj49Hr169xHV16tRBr169cPbsWSVGRlTzPX78GMCLB1VSvqKiIoSHhyMvLw+dO3dWdjgEwMfHBwMGDJD8P4aUKzU1FWZmZmjatCnGjBmDzMxMZYdU6+3fvx/t27fHiBEjYGxsjLZt22Lz5s3KDoteUlBQgO+//x7jx49n0kABJg6I/uXu3buHoqIimJiYSNabmJggOztbSVER1XzFxcXw8/ODs7MzWrVqpexwarUrV65AR0cH6urqmDRpEvbu3YuWLVsqO6xaLzw8HJcuXUJwcLCyQ6H/cXJyQlhYGCIjI7F+/XrcuHED3bp1Q05OjrJDq9XS0tKwfv16WFtb48iRI5g8eTKmTZuG7du3Kzs0+p+IiAg8evQIY8eOVXYoNZaqsgMgIiKqiXx8fJCYmMjxwTWAra0tEhIS8PjxY/z000/w8vLCiRMnmDxQops3b2L69OmIioqChoaGssOh/+nXr5/4ZwcHBzg5OcHCwgK7d+/m0B4lKi4uRvv27REUFAQAaNu2LRITE7FhwwZ4eXkpOToCgK1bt6Jfv34wMzNTdig1FnscEP3L1a9fHyoqKrh9+7Zk/e3bt9GgQQMlRUVUs/n6+uLAgQM4fvw4GjVqpOxwaj01NTVYWVnB0dERwcHBaN26Nb766itlh1WrxcfH486dO2jXrh1UVVWhqqqKEydOYM2aNVBVVUVRUZGyQyQABgYGsLGxwbVr15QdSq1mamoql+hs0aIFh5HUEBkZGYiOjsaECROUHUqNxsQB0b+cmpoaHB0dERMTI64rLi5GTEwMxwgTvUIQBPj6+mLv3r04duwYmjRpouyQqBTFxcXIz89Xdhi1Ws+ePXHlyhUkJCSIS/v27TFmzBgkJCRARUVF2SESXkxeef36dZiamio7lFrN2dlZ7tO+KSkpsLCwUFJE9LLQ0FAYGxtjwIAByg6lRuNQBaJaYMaMGfDy8kL79u3RsWNHrF69Gnl5eRg3bpyyQ6u1cnNzJW+Abty4gYSEBBgaGqJx48ZKjKx28/Hxwc6dO7Fv3z7o6uqK84Do6+tDU1NTydHVTvPmzUO/fv3QuHFj5OTkYOfOnYiNjcWRI0eUHVqtpqurKzf3h7a2NurVq8c5QZRo1qxZGDRoECwsLPD3338jICAAKioqGDVqlLJDq9U++eQTdOnSBUFBQXB3d8eFCxewadMmbNq0Sdmh1XrFxcUIDQ2Fl5cXVFX5aFwWnh2iWsDDwwN3796Fv78/srOz0aZNG0RGRspNmEhvz8WLF+Hq6ir+njFjBgDAy8sLYWFhSoqK1q9fDwDo3r27ZH1oaCgnTFKSO3fuwNPTE1lZWdDX14eDgwOOHDmC3r17Kzs0ohrnr7/+wqhRo3D//n0YGRmha9euOHfuHIyMjJQdWq3WoUMH7N27F/PmzcOiRYvQpEkTrF69GmPGjFF2aLVedHQ0MjMzMX78eGWHUuPJBEEQlB0EEREREREREdVMnOOAiIiIiIiIiBRi4oCIiIiIiIiIFGLigIiIiIiIiIgUYuKAiIiIiIiIiBRi4oCIiIiIiIiIFGLigIiIiIiIiIgUYuKAiIiIiIiIiBRi4oCIiIiI3jnp6elYsmQJcnNzlR0KEdG/HhMHRERERDVU9+7d4efnp+wwapz8/HyMGDEC9evXh46OTrnlLS0tsXr16tduLywsDAYGBq+9PxHRu46JAyIiInotY8eOxdChQ5UdhkLp6emQyWRISEhQdihUSeXdW5988gn69OmDSZMmVai+uLg4fPzxxxUqW1qSwcPDAykpKRXan4jo30hV2QEQERERVbWCggJlh1ArFRQUQE1NrdrbWbduXYXKlcRjZGT0Ru1pampCU1PzjeogInqXsccBERERVYnu3btj6tSp8PPzw3vvvQcTExNs3rwZeXl5GDduHHR1dWFlZYXDhw+L+8TGxkImk+HgwYNwcHCAhoYGOnXqhMTEREndP//8M+zs7KCurg5LS0usWLFCst3S0hKLFy+Gp6cn9PT08PHHH6NJkyYAgLZt20Imk6F79+4AXrx97t27N+rXrw99fX24uLjg0qVLkvpkMhm2bNmCYcOGQUtLC9bW1ti/f7+kzB9//IGBAwdCT08Purq66NatG65fvy5u37JlC1q0aAENDQ00b9683IfdvLw8eHp6QkdHB6ampnLHCLzooj9r1iw0bNgQ2tracHJyQmxsrLg9IyMDgwYNwnvvvQdtbW3Y2dnh0KFDCtvMz8/H3LlzYW5uDnV1dVhZWWHr1q0AgKKiInh7e6NJkybQ1NSEra0tvvrqK8n+JT0DvvjiC5iZmcHW1hYA8N1336F9+/bQ1dVFgwYNMHr0aNy5c6dC5y8wMBDbt2/Hvn37IJPJIJPJxGO8efMm3N3dYWBgAENDQwwZMgTp6enlxvNyLwJBEBAYGIjGjRtDXV0dZmZmmDZtGoAX93BGRgY++eQTsW2g9KEKISEhMDExga6uLry9vfHpp5+iTZs24vbShpkMHToUY8eOrbbrSURUXZg4ICIioiqzfft21K9fHxcuXMDUqVMxefJkjBgxAl26dMGlS5fQp08ffPTRR/jnn38k+82ePRsrVqxAXFwcjIyMMGjQIBQWFgIA4uPj4e7ujpEjR+LKlSsIDAzEwoULERYWJqlj+fLlaN26NX777TcsXLgQFy5cAABER0cjKysLe/bsAQDk5OTAy8sLv/76K86dOwdra2v0798fOTk5kvo+//xzuLu74/Lly+jfvz/GjBmDBw8eAABu3bqF999/H+rq6jh27Bji4+Mxfvx4PH/+HACwY8cO+Pv744svvkBSUhKCgoKwcOFCbN++XeG5mz17Nk6cOIF9+/bh6NGjiI2NlUto+Pr64uzZswgPD8fly5cxYsQI9O3bF6mpqQAAHx8f5Ofn4+TJk7hy5Qq+/PLLMucA8PT0xA8//IA1a9YgKSkJGzduFMsXFxejUaNG+PHHH/Hnn3/C398f8+fPx+7duyV1xMTEIDk5GVFRUThw4AAAoLCwEIsXL8bvv/+OiIgIpKenSx6Yyzp/s2bNgru7O/r27YusrCxkZWWhS5cuKCwshJubG3R1dXHq1CmcPn0aOjo66Nu3r6SHSWnxvOznn3/GqlWrsHHjRqSmpiIiIgL29vYAgD179qBRo0ZYtGiR2HZpdu/ejcDAQAQFBeHixYswNTWtcC+Il1X19SQiqjYCERER0Wvw8vIShgwZIv52cXERunbtKv5+/vy5oK2tLXz00UfiuqysLAGAcPbsWUEQBOH48eMCACE8PFwsc//+fUFTU1PYtWuXIAiCMHr0aKF3796StmfPni20bNlS/G1hYSEMHTpUUubGjRsCAOG3334r8ziKiooEXV1d4ZdffhHXARA+++wz8Xdubq4AQDh8+LAgCIIwb948oUmTJkJBQUGpdTZr1kzYuXOnZN3ixYuFzp07l1o+JydHUFNTE3bv3i2uKzkP06dPFwRBEDIyMgQVFRXh1q1bkn179uwpzJs3TxAEQbC3txcCAwPLPN4SycnJAgAhKiqqQuUFQRB8fHyEDz74QPzt5eUlmJiYCPn5+WXuFxcXJwAQcnJyBEEo//y9em8JgiB89913gq2trVBcXCyuy8/PFzQ1NYUjR46UGY+FhYWwatUqQRAEYcWKFYKNjY3Ctl8uWyI0NFTQ19cXf3fu3FmYMmWKpIyTk5PQunVr8beLi4t47UoMGTJE8PLyEgSh6q8nEVF1Yo8DIiIiqjIODg7in1VUVFCvXj3xbS4AmJiYAIBct/XOnTuLfzY0NIStrS2SkpIAAElJSXB2dpaUd3Z2RmpqKoqKisR17du3r1CMt2/fxsSJE2FtbQ19fX3o6ekhNzcXmZmZCo9FW1sbenp6YtwJCQno1q0b6tatK1d/Xl4erl+/Dm9vb+jo6IjLkiVLJEMZXnb9+nUUFBTAyclJ7jyUuHLlCoqKimBjYyOp98SJE2K906ZNw5IlS+Ds7IyAgABcvnxZ4XlISEiAiooKXFxcFJZZu3YtHB0dYWRkBB0dHWzatEnuPNnb28vNaxAfH49BgwahcePG0NXVFdso2bes86fI77//jmvXrkFXV1c8dkNDQzx79kxyXkuL52UjRozA06dP0bRpU0ycOBF79+4Ve4pUVFJSkuRaAdJ7uCKq+noSEVUnTo5IREREVebVB0GZTCZZVzJmvLi4uMrb1tbWrlA5Ly8v3L9/H1999RUsLCygrq6Ozp07y02oWNqxlMRd1kR5ubm5AIDNmzfLPVyqqKhUKEZF9aqoqCA+Pl6unpLu6xMmTICbmxsOHjyIo0ePIjg4GCtWrMDUqVPl6itvsr/w8HDMmjULK1asQOfOnaGrq4tly5bh/PnzknKvnve8vDy4ubnBzc0NO3bsgJGRETIzM+Hm5iae49eZaDA3NxeOjo7YsWOH3LaXJz8s7z4wNzdHcnIyoqOjERUVhSlTpmDZsmU4ceJEpRIZ5alTpw4EQZCsKxl+A1T99SQiqk7scUBERERKd+7cOfHPDx8+REpKClq0aAEAaNGiBU6fPi0pf/r0adjY2JT5IF7y1vnlXgkl+06bNg39+/cXJ1y8d+9epeJ1cHDAqVOnJA+CJUxMTGBmZoa0tDRYWVlJlpIJG1/VrFkz1K1bV/JQXnIeSrRt2xZFRUW4c+eOXL0NGjQQy5mbm2PSpEnYs2cPZs6cic2bN5fapr29PYqLi3HixIlSt58+fRpdunTBlClT0LZtW1hZWSnsMfGyq1ev4v79+wgJCUG3bt3QvHlzuR4mZZ0/4MW1e/W6tWvXDqmpqTA2NpY7fn19/XLjepmmpiYGDRqENWvWIDY2FmfPnsWVK1cUtv2qFi1ayCVQXr6HgRfJjJfnSCgqKpJM+lnV15OIqDoxcUBERERKt2jRIsTExCAxMRFjx45F/fr1MXToUADAzJkzERMTg8WLFyMlJQXbt2/HN998g1mzZpVZp7GxMTQ1NREZGYnbt2/j8ePHAABra2t89913SEpKwvnz5zFmzJhKvwH39fXFkydPMHLkSFy8eBGpqan47rvvkJycDODFxIrBwcFYs2YNUlJScOXKFYSGhmLlypWl1qejowNvb2/Mnj0bx44dE89DnTr//081GxsbjBkzBp6entizZw9u3LiBCxcuIDg4GAcPHgQA+Pn54ciRI7hx4wYuXbqE48ePiwmYV1laWsLLywvjx49HREQEbty4gdjYWHHyQ2tra1y8eBFHjhxBSkoKFi5ciLi4uHLPTePGjaGmpoavv/4aaWlp2L9/PxYvXlyp82dpaYnLly8jOTkZ9+7dQ2FhIcaMGYP69etjyJAhOHXqlBjvtGnT8Ndff5UbV4mwsDBs3boViYmJSEtLw/fffw9NTU1YWFiIbZ88eRK3bt1SmFCaPn06tm3bhtDQUKSkpCAgIAB//PGHpEyPHj1w8OBBHDx4EFevXsXkyZPx6NEjcXtVX08iourExAEREREpXUhICKZPnw5HR0dkZ2fjl19+EXsMtGvXDrt370Z4eDhatWoFf39/LFq0SDJLf2lUVVWxZs0abNy4EWZmZhgyZAgAYOvWrXj48CHatWuHjz76CNOmTYOxsXGl4q1Xrx6OHTuG3NxcuLi4wNHREZs3bxa7uk+YMAFbtmxBaGgo7O3t4eLigrCwMIU9DgBg2bJl6NatGwYNGoRevXqha9eucHR0lJQJDQ2Fp6cnZs6cCVtbWwwdOhRxcXFo3LgxgBdvtX18fNCiRQv07dsXNjY2Zc72v379egwfPhxTpkxB8+bNMXHiROTl5QEA/vvf/+I///kPPDw84OTkhPv372PKlCnlnhsjIyOEhYXhxx9/RMuWLRESEoLly5dX6vxNnDgRtra2aN++PYyMjHD69GloaWnh5MmTaNy4Mf7zn/+gRYsW8Pb2xrNnz6Cnp1duXCUMDAywefNmODs7w8HBAdHR0fjll19Qr149AC+SWOnp6WjWrJlkCMTLPDw8sHDhQsyZMweOjo7IyMjA5MmTJWXGjx8PLy8veHp6wsXFBU2bNoWrq6ukTFVfTyKi6iITXh18RURERPSWxMbGwtXVFQ8fPoSBgYGywyF6bYGBgYiIiEBCQoKyQyEiqnLscUBERERERERECjFxQEREREREREQKcagCERERERERESnEHgdEREREREREpBATB0RERERERESkEBMHRERERERERKQQEwdEREREREREpBATB0RERERERESkEBMHRERERERERKQQEwdEREREREREpBATB0RERERERESk0P8BFuLpBMlbieAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les colonnes 'feature' et 'importance' de votre dataframe\n",
    "features = df_feature_importances_lightGBM['feature'].to_numpy()\n",
    "importances = df_feature_importances_lightGBM['importance'].to_numpy()\n",
    "\n",
    "# Création d'un masque pour les importances supérieures à 0\n",
    "mask = importances > 0\n",
    "\n",
    "# Filtrage des features et des importances en utilisant le masque\n",
    "filtered_feature_names = features[mask]\n",
    "filtered_feature_importances = importances[mask]\n",
    "\n",
    "# Supposons que vous vouliez afficher les 10 principales caractéristiques\n",
    "n_features_to_display = 20\n",
    "\n",
    "# Tri des indices filtrés par importance\n",
    "sorted_idx = filtered_feature_importances.argsort()\n",
    "\n",
    "# Limiter aux n principales caractéristiques\n",
    "top_n_idx = sorted_idx[-n_features_to_display:]\n",
    "\n",
    "# Affichage de l'importance des caractéristiques filtrées pour les n principales\n",
    "plt.figure(figsize=(10, len(top_n_idx) * 0.4))\n",
    "bars = plt.barh(filtered_feature_names[top_n_idx], filtered_feature_importances[top_n_idx])\n",
    "\n",
    "# Ajuster les limites de l'axe des y pour réduire l'espace autour des barres\n",
    "plt.ylim(-0.5, len(top_n_idx)-0.5)\n",
    "\n",
    "plt.xlabel(\"Importance des caractéristiques\")\n",
    "plt.title(\"Importance des caractéristiques avec le lightGBM\")\n",
    "\n",
    "# Ajouter les valeurs à côté des barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    \n",
    "    # Déterminez le formatage en fonction de la magnitude\n",
    "    if width < 0.01:\n",
    "        fmt = '{:.4f}'\n",
    "    elif width < 0.1:\n",
    "        fmt = '{:.3f}'\n",
    "    else:\n",
    "        fmt = '{:.2f}'\n",
    "    \n",
    "    plt.text(width + 0.01 * width,  # Ajouter un padding proportionnel à la valeur\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             fmt.format(width),\n",
    "             va='center', ha='left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07509970-8aa4-489e-80e4-0512e1f37e97",
   "metadata": {},
   "source": [
    "# 3. Création du pipeline de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d7434-6068-4de1-bb3d-181f655c8381",
   "metadata": {
    "tags": []
   },
   "source": [
    "Le modèle le plus efficace semble être LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f826d3a-1b9f-4fe4-8b6c-201e879ea5f4",
   "metadata": {},
   "source": [
    "## Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8dec4c-cd41-41df-9895-c968d455f85b",
   "metadata": {},
   "source": [
    "### 3.1 Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0b95667-c5fd-4a45-a030-2cc6872fc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = application_train.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X_test = application_test.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "396793ef-106f-44ae-89b6-b2f4c389b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Séparation des jeux de données : \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99149115-469d-4bf3-ab20-4e2c9becfd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### On standardise nos données, c'est à dire qu'on centre et réduit nos données \n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bf3acd0-0d60-4b7e-b71d-88ca15f189ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lr': 0.021541725724886718, 'num_leaves': 70, 'n_estimators': 140, 'threshold': 0.48000000000000004}\n"
     ]
    }
   ],
   "source": [
    "# Obtenez les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "\n",
    "# Affichez les meilleurs paramètres pour vérification\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9fb5d65-4b40-44eb-af6b-dfd895a33e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline test : \n",
    "pipeline = pipeline.Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()), \n",
    "    ('regressor', LGBMClassifier(\n",
    "        learning_rate=best_params['lr'], \n",
    "        num_leaves=best_params['num_leaves'], \n",
    "        n_estimators=best_params['n_estimators'], \n",
    "        class_weight=class_weights,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79607412-b0dd-4bec-bcbf-7bc19537e53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.021541725724886718,\n",
       "                                n_estimators=140, num_leaves=70, verbose=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.021541725724886718,\n",
       "                                n_estimators=140, num_leaves=70, verbose=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "               learning_rate=0.021541725724886718, n_estimators=140,\n",
       "               num_leaves=70, verbose=-1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('regressor',\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.021541725724886718,\n",
       "                                n_estimators=140, num_leaves=70, verbose=-1))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Apprentissage sur les données train : \n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4917abf-dbf7-486f-8271-dca2b17c04e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.707124955245256"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calcul des performances : \n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd0661-1f9a-49bc-8ac9-2cbcc6405db3",
   "metadata": {},
   "source": [
    "#### Sérialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d30283b-5143-4af5-bb9c-70bb6b5014a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37f6e145-acbf-476e-ba7c-04eac8bd4db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_home_credit_complet.joblib']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, 'pipeline_home_credit_complet.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9715a3c-b68c-49f3-bf6f-87f389c1bf91",
   "metadata": {},
   "source": [
    "#### Signature MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f7f19f4-c85e-497f-968c-b3bbd22013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "918c1dbc-9621-48c0-ab10-259d4d6d252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ea12a-aac7-40ad-ad64-1653d617019c",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c1131a4-750a-4a42-9af8-58d63ef6e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b48a561f-4b31-4a75-a596-c589fc31d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sauvegarde du pipeline : \n",
    "mlflow.sklearn.save_model(pipeline, 'mlflow_model_complet', signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91889830-611e-4caa-969d-ab5ae98a8a99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2 Dataset petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d16dfdd9-8ca4-4599-94c0-2c138a0d77c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_petit = application_train_petit_clean.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "X_test_petit = application_test_petit.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b067d2b-f26e-45df-822c-dacfbdde04c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### Séparation des jeux de données : \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(\u001b[43mX\u001b[49m, y, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "### Séparation des jeux de données : \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd680dd3-f3ed-491a-b494-6a580cccd7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### On standardise nos données, c'est à dire qu'on centre et réduit nos données \u001b[39;00m\n\u001b[0;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mStandardScaler()\n\u001b[1;32m----> 3\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m      4\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "### On standardise nos données, c'est à dire qu'on centre et réduit nos données \n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0e294e50-7dcc-4568-8ea4-f03fe90876d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lr': 0.03202900440110068, 'num_leaves': 50, 'n_estimators': 500, 'threshold': 0.52}\n"
     ]
    }
   ],
   "source": [
    "# Obtenez les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "\n",
    "# Affichez les meilleurs paramètres pour vérification\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "019c71cf-b270-485a-b12f-d6fbbb7c38f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Pipeline test petit dataset : \n",
    "pipeline = pipeline.Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()), \n",
    "    ('regressor', LGBMClassifier(\n",
    "        learning_rate=best_params['lr'], \n",
    "        num_leaves=best_params['num_leaves'], \n",
    "        n_estimators=best_params['n_estimators'], \n",
    "        class_weight=class_weights,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ac063132-5843-444f-bafc-3a20f43f191e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.03202900440110068,\n",
       "                                n_estimators=500, num_leaves=50, verbose=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.03202900440110068,\n",
       "                                n_estimators=500, num_leaves=50, verbose=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "               learning_rate=0.03202900440110068, n_estimators=500,\n",
       "               num_leaves=50, verbose=-1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('regressor',\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.03202900440110068,\n",
       "                                n_estimators=500, num_leaves=50, verbose=-1))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Apprentissage sur les données train : \n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d98efe7e-f19f-4218-93eb-eb6731604206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7411385606874329"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calcul des performances : \n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c815f-7eb0-44a6-a708-030abd96195a",
   "metadata": {},
   "source": [
    "#### Sérialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2f649341-596a-4c43-9ed6-0d8f7c3f2256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9afbcdbe-9b47-4395-ade6-f1b85587e2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_home_credit.joblib']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline, 'pipeline_home_credit.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b9563-1baf-40c4-a475-2f2a1e4d5c9c",
   "metadata": {},
   "source": [
    "#### Signature MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1d734c6e-0e75-4759-8d91-c8dd059cf9d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0f7f1b45-2127-4277-8c91-387bfae40d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288130e-27f5-4300-8775-bf250a6dff8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sauvegarde du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6125e773-f562-4d56-b1ec-bf5efb15edf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79ea46-5ae9-4d14-b4ce-3c59d95f19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sauvegarde du pipeline : \n",
    "mlflow.sklearn.save_model(pipeline, 'mlflow_model', signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81c9f3-5fd9-4949-9472-d8c81865a4b7",
   "metadata": {},
   "source": [
    "### 3.2 Dataset petit encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a34189de-3490-44d6-bb66-deb9f26d7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Séparation des jeux de données : \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a99ba02f-ab54-460e-bad6-090c42e19afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### On standardise nos données, c'est à dire qu'on centre et réduit nos données \n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59d97acd-ea8b-4e61-8fc3-8a0da250d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lr': 0.02127915254605793, 'num_leaves': 58, 'n_estimators': 481, 'threshold': 0.52}\n"
     ]
    }
   ],
   "source": [
    "# Obtenez les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "\n",
    "# Affichez les meilleurs paramètres pour vérification\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d67e40e5-e98f-4a97-98a7-87056f72ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline test petit dataset : \n",
    "pipeline = pipeline.Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()), \n",
    "    ('regressor', LGBMClassifier(\n",
    "        learning_rate=best_params['lr'], \n",
    "        num_leaves=best_params['num_leaves'], \n",
    "        n_estimators=best_params['n_estimators'], \n",
    "        class_weight=class_weights,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "87f9aa2d-3801-4cba-a386-63020edc0802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.02127915254605793,\n",
       "                                n_estimators=481, num_leaves=58, verbose=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.02127915254605793,\n",
       "                                n_estimators=481, num_leaves=58, verbose=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "               learning_rate=0.02127915254605793, n_estimators=481,\n",
       "               num_leaves=58, verbose=-1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('regressor',\n",
       "                 LGBMClassifier(class_weight={0: 1, 1: 11.386727412006612},\n",
       "                                learning_rate=0.02127915254605793,\n",
       "                                n_estimators=481, num_leaves=58, verbose=-1))])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Apprentissage sur les données train : \n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f18acf6-49fc-422d-be1c-eb2d9adf90e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.704700061842919"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calcul des performances : \n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39c1ae-cb2b-4091-aa24-68f440a81bcd",
   "metadata": {},
   "source": [
    "#### Sérialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "839e7e93-3252-42f1-a16a-fc5e7068efa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_home_credit.joblib']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, 'pipeline_home_credit.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564daaa-72df-4a3d-b99e-e9e965972202",
   "metadata": {},
   "source": [
    "#### Signature MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "28d790b6-79da-4f62-ad2a-a246b464204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e8508-d609-4f3f-bec5-a9c1ef689daa",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d9805c8-d76c-455a-b4f6-f150dab678b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "### Sauvegarde du pipeline : \n",
    "mlflow.sklearn.save_model(pipeline, 'mlflow_model_dataset_petit_encoded', signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5999428-233e-4679-9297-1036be35dc72",
   "metadata": {},
   "source": [
    "#### Test de prédiction de refus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92666bec-dfa9-4b2e-90c0-5c801aca541d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anais\\anaconda3\\lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# Charger les datasets\n",
    "train_df = pd.read_csv('application_train_petit_encoded.csv')\n",
    "test_df = pd.read_csv('application_test_petit_encoded.csv')\n",
    "\n",
    "# Charger le modèle MLflow\n",
    "model = mlflow.pyfunc.load_model(\"mlflow_model_dataset_petit_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb2fd67b-eb47-4f08-976c-5a2a47698bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun client refusé trouvé.\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions sur un sous-ensemble aléatoire de données d'entraînement\n",
    "subset = train_df.sample(n=1000, random_state=42).drop(columns=['SK_ID_CURR', 'TARGET']).astype('float64')\n",
    "client_ids = train_df.sample(n=1000, random_state=42)['SK_ID_CURR']\n",
    "\n",
    "# Faire des prédictions\n",
    "predictions = model.predict(subset)\n",
    "\n",
    "# Ajouter les prédictions aux données originales\n",
    "results_df = pd.DataFrame({'SK_ID_CURR': client_ids, 'prediction': predictions})\n",
    "\n",
    "# Trouver un client refusé\n",
    "refused_client = results_df[results_df['prediction'] == 1]['SK_ID_CURR'].values\n",
    "\n",
    "if len(refused_client) > 0:\n",
    "    print(f\"Numéro de client refusé trouvé : {refused_client[0]}\")\n",
    "else:\n",
    "    print(\"Aucun client refusé trouvé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ee3c71b-42dc-4e38-a5b8-466c8d0cb1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun client refusé trouvé.\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions sur l'ensemble des données d'entraînement\n",
    "subset = train_df.drop(columns=['SK_ID_CURR', 'TARGET']).astype('float64')\n",
    "client_ids = train_df['SK_ID_CURR']\n",
    "\n",
    "# Faire des prédictions\n",
    "predictions = model.predict(subset)\n",
    "\n",
    "# Ajouter les prédictions aux données originales\n",
    "results_df = pd.DataFrame({'SK_ID_CURR': client_ids, 'prediction': predictions})\n",
    "\n",
    "# Trouver un client refusé\n",
    "refused_client = results_df[results_df['prediction'] == 1]['SK_ID_CURR'].values\n",
    "\n",
    "if len(refused_client) > 0:\n",
    "    print(f\"Numéro de client refusé trouvé : {refused_client[0]}\")\n",
    "else:\n",
    "    print(\"Aucun client refusé trouvé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdf6a8-5571-4a76-aede-4c0fb23caf49",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.4 Feature Importance using Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a95df2-72c9-4db5-bd35-edb04263e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "categorical_feats = [\n",
    "    f for f in application_train.columns if application_train[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "for col in categorical_feats:\n",
    "    lb = preprocessing.LabelEncoder()\n",
    "    lb.fit(list(application_train[col].values.astype('str')) + list(application_test[col].values.astype('str')))\n",
    "    application_train[col] = lb.transform(list(application_train[col].values.astype('str')))\n",
    "    application_test[col] = lb.transform(list(application_test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a64be-9878-4d3d-a7a8-078ce00803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.fillna(-999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524a899-b9dc-48b0-9427-1ee7ba21d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=8, min_samples_leaf=4, max_features=0.5, random_state=2018)\n",
    "rf.fit(application_train.drop(['SK_ID_CURR', 'TARGET'],axis=1), application_train.TARGET)\n",
    "features = application_train.drop(['SK_ID_CURR', 'TARGET'],axis=1).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02a25b-e780-40b0-8ebb-9abe87f23a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n",
    "                                                            reverse = False)))\n",
    "trace2 = go.Bar(\n",
    "    x=x ,\n",
    "    y=y,\n",
    "    marker=dict(\n",
    "        color=x,\n",
    "        colorscale = 'Viridis',\n",
    "        reversescale = True\n",
    "    ),\n",
    "    name='Random Forest Feature importance',\n",
    "    orientation='h',\n",
    ")\n",
    "\n",
    "layout = dict(\n",
    "    title='Barplot of Feature importances',\n",
    "     width = 900, height = 2000,\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "#         domain=[0, 0.85],\n",
    "    ),\n",
    "    margin=dict(\n",
    "    l=300,\n",
    "),\n",
    ")\n",
    "\n",
    "fig1 = go.Figure(data=[trace2])\n",
    "fig1['layout'].update(layout)\n",
    "py.iplot(fig1, filename='plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad2a3d-f4c2-4946-94fb-aa9a4fa1c48a",
   "metadata": {},
   "source": [
    "### Choix des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59fc2770-07f8-4cdf-8fac-d5728ca1ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anais\\anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080730 -> initscore=-2.432469\n",
      "[LightGBM] [Info] Start training from score -2.432469\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080734 -> initscore=-2.432418\n",
      "[LightGBM] [Info] Start training from score -2.432418\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3323\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3319\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 231\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000020\n",
      "[LightGBM] [Info] Start training from score -0.000020\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3324\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 19843, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3322\n",
      "[LightGBM] [Info] Number of data points in the train set: 245783, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500008 -> initscore=0.000030\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28696\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28490\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 237\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28541\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 236\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "[LightGBM] [Info] Number of positive: 225940, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28496\n",
      "[LightGBM] [Info] Number of data points in the train set: 451880, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.919268 -> initscore=2.432448\n",
      "[LightGBM] [Info] Start training from score 2.432448\n",
      "Essai 50/50 terminé!\n",
      "Temps écoulé: 135894.42 secondes\n",
      "                                                       Model      Strategy  \\\n",
      "Model              Strategy     Metric                                       \n",
      "DummyClassifier    class_weight business     DummyClassifier  class_weight   \n",
      "                                f-beta       DummyClassifier  class_weight   \n",
      "                   none         business     DummyClassifier          none   \n",
      "                                f-beta       DummyClassifier          none   \n",
      "                   smote        business     DummyClassifier         smote   \n",
      "                                f-beta       DummyClassifier         smote   \n",
      "LGBMClassifier     class_weight business      LGBMClassifier  class_weight   \n",
      "                                f-beta        LGBMClassifier  class_weight   \n",
      "                   none         business      LGBMClassifier          none   \n",
      "                                f-beta        LGBMClassifier          none   \n",
      "                   smote        business      LGBMClassifier         smote   \n",
      "                                f-beta        LGBMClassifier         smote   \n",
      "LogisticRegression class_weight business  LogisticRegression  class_weight   \n",
      "                                f-beta    LogisticRegression  class_weight   \n",
      "                   none         business  LogisticRegression          none   \n",
      "                                f-beta    LogisticRegression          none   \n",
      "                   smote        business  LogisticRegression         smote   \n",
      "                                f-beta    LogisticRegression         smote   \n",
      "\n",
      "                                            Metric  Learning Rate  Num Leaves  \\\n",
      "Model              Strategy     Metric                                          \n",
      "DummyClassifier    class_weight business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "                   none         business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "                   smote        business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "LGBMClassifier     class_weight business  business       0.045110        37.0   \n",
      "                                f-beta      f-beta       0.014809        67.0   \n",
      "                   none         business  business       0.098081        33.0   \n",
      "                                f-beta      f-beta       0.081405        70.0   \n",
      "                   smote        business  business       0.002907        50.0   \n",
      "                                f-beta      f-beta       0.003521        57.0   \n",
      "LogisticRegression class_weight business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "                   none         business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "                   smote        business  business            NaN         NaN   \n",
      "                                f-beta      f-beta            NaN         NaN   \n",
      "\n",
      "                                          Threshold       AUC  Accuracy  \\\n",
      "Model              Strategy     Metric                                    \n",
      "DummyClassifier    class_weight business       0.58  0.500000  0.919268   \n",
      "                                f-beta         0.50  0.500000  0.919268   \n",
      "                   none         business       0.56  0.500000  0.919268   \n",
      "                                f-beta         0.40  0.500000  0.919268   \n",
      "                   smote        business       0.48  0.500000  0.919268   \n",
      "                                f-beta         0.47  0.500000  0.919268   \n",
      "LGBMClassifier     class_weight business       0.53  0.702810  0.722916   \n",
      "                                f-beta         0.53  0.701872  0.746537   \n",
      "                   none         business       0.40  0.693595  0.918188   \n",
      "                                f-beta         0.40  0.690928  0.918048   \n",
      "                   smote        business       0.50  0.930644  0.500299   \n",
      "                                f-beta         0.44  0.918522  0.500000   \n",
      "LogisticRegression class_weight business       0.54  0.684323  0.705138   \n",
      "                                f-beta         0.54  0.684323  0.705138   \n",
      "                   none         business       0.40  0.684409  0.919083   \n",
      "                                f-beta         0.40  0.684409  0.919083   \n",
      "                   smote        business       0.54  0.717979  0.511640   \n",
      "                                f-beta         0.54  0.717979  0.511640   \n",
      "\n",
      "                                          Business Score    F-beta  \\\n",
      "Model              Strategy     Metric                               \n",
      "DummyClassifier    class_weight business          248030  0.000000   \n",
      "                                f-beta            248030  0.000000   \n",
      "                   none         business          248030  0.000000   \n",
      "                                f-beta            248030  0.000000   \n",
      "                   smote        business          248030  0.000000   \n",
      "                                f-beta            248030  0.000000   \n",
      "LGBMClassifier     class_weight business          185217  0.243253   \n",
      "                                f-beta            186051  0.247165   \n",
      "                   none         business          244321  0.034495   \n",
      "                                f-beta            244175  0.035991   \n",
      "                   smote        business          282418  0.666786   \n",
      "                                f-beta            282425  0.666667   \n",
      "LogisticRegression class_weight business          192218  0.229755   \n",
      "                                f-beta            192218  0.229755   \n",
      "                   none         business          247592  0.004405   \n",
      "                                f-beta            247592  0.004405   \n",
      "                   smote        business          279549  0.671559   \n",
      "                                f-beta            279549  0.671559   \n",
      "\n",
      "                                            Fit Time  Prediction Time  \n",
      "Model              Strategy     Metric                                 \n",
      "DummyClassifier    class_weight business    1.414213         0.000998  \n",
      "                                f-beta      1.450910         0.000000  \n",
      "                   none         business    1.500929         0.000000  \n",
      "                                f-beta      1.372604         0.000000  \n",
      "                   smote        business    1.469402         0.000000  \n",
      "                                f-beta      1.446526         0.000000  \n",
      "LGBMClassifier     class_weight business   23.331135         0.000000  \n",
      "                                f-beta     66.248429         0.000505  \n",
      "                   none         business   35.279004         0.000000  \n",
      "                                f-beta     38.124226         0.000000  \n",
      "                   smote        business   77.495906         0.000000  \n",
      "                                f-beta     37.399123         0.000000  \n",
      "LogisticRegression class_weight business  549.009265         0.000000  \n",
      "                                f-beta    545.758310         0.000000  \n",
      "                   none         business  365.198692         0.000000  \n",
      "                                f-beta    353.174884         0.000000  \n",
      "                   smote        business  239.145348         0.000000  \n",
      "                                f-beta    237.910887         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, fbeta_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "# Configurer le logging\n",
    "optuna_logger = logging.getLogger('optuna')\n",
    "optuna_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('Model_Comparison')\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('application_train_petit_encoded.csv')\n",
    "test_df = pd.read_csv('application_test_petit_encoded.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = df[\"TARGET\"]\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Définir les plis de validation croisée\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Calculer les poids de classe\n",
    "nb_0 = (y == 0).sum()\n",
    "nb_1 = (y == 1).sum()\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "# Résultats\n",
    "results = []\n",
    "\n",
    "total_combinations = 50  # Définissez cela avant d'appeler study.optimize()\n",
    "\n",
    "def objective(trial, model, X_train, y_train, strategy, metric):\n",
    "    lr = None\n",
    "    num_leaves = None\n",
    "    n_estimators = None\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    if isinstance(model, LGBMClassifier):\n",
    "        lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "        num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "\n",
    "        if strategy == 'none':\n",
    "            model.set_params(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators)\n",
    "        elif strategy == 'class_weight':\n",
    "            model.set_params(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators, class_weight=class_weights)\n",
    "        elif strategy == 'smote':\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            model.set_params(learning_rate=lr, num_leaves=num_leaves, n_estimators=n_estimators)\n",
    "\n",
    "    elif isinstance(model, LogisticRegression):\n",
    "        if strategy == 'class_weight':\n",
    "            model.set_params(class_weight=class_weights)\n",
    "        elif strategy == 'smote':\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X_train, y_train, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y_train, y_prob)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    fbeta = fbeta_score(y_train, y_pred, beta=1.0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "\n",
    "    if metric == 'business':\n",
    "        score = cost\n",
    "    else:\n",
    "        score = fbeta\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": type(model).__name__,\n",
    "        \"Strategy\": strategy,\n",
    "        \"Metric\": metric,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost,\n",
    "        \"F-beta\": fbeta,\n",
    "        \"Fit Time\": fit_duration,\n",
    "        \"Prediction Time\": pred_duration\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Model\", type(model).__name__)\n",
    "        mlflow.log_param(\"Strategy\", strategy)\n",
    "        mlflow.log_param(\"Metric\", metric)\n",
    "        if isinstance(model, LGBMClassifier):\n",
    "            mlflow.log_param(\"Learning Rate\", lr)\n",
    "            mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "            mlflow.log_param(\"N Estimators\", n_estimators)\n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        mlflow.log_metric(\"F-beta\", fbeta)\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_train, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return score\n",
    "\n",
    "def print_progress(study, trial, total_combinations):\n",
    "    print(f\"Essai {trial.number + 1}/{total_combinations} terminé!\", end='\\r', flush=True)\n",
    "\n",
    "# Modèles à tester\n",
    "models = [\n",
    "    DummyClassifier(strategy=\"most_frequent\"),\n",
    "    LogisticRegression(random_state=42, solver='liblinear'),\n",
    "    LGBMClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "# Stratégies à tester\n",
    "strategies = ['none', 'class_weight', 'smote']\n",
    "\n",
    "# Métriques à tester\n",
    "metrics = ['business', 'f-beta']\n",
    "\n",
    "for model in models:\n",
    "    for strategy in strategies:\n",
    "        for metric in metrics:\n",
    "            study = optuna.create_study(direction='minimize' if metric == 'business' else 'maximize')\n",
    "            study.optimize(lambda trial: objective(trial, model, X, y, strategy, metric), n_trials=total_combinations, callbacks=[lambda study, trial: print_progress(study, trial, total_combinations)])\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(results_df.groupby(['Model', 'Strategy', 'Metric']).apply(lambda x: x.loc[x['Business Score'].idxmin() if 'Business Score' in x.columns else x['F-beta'].idxmax()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b8423da-92e1-423e-8bd5-555633e24c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Num Leaves</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Business Score</th>\n",
       "      <th>F-beta</th>\n",
       "      <th>Fit Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">DummyClassifier</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">class_weight</th>\n",
       "      <th>business</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.414213</td>\n",
       "      <td>0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.450910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">none</th>\n",
       "      <th>business</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>none</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500929</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>none</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.372604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">smote</th>\n",
       "      <th>business</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>smote</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.469402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>smote</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.919268</td>\n",
       "      <td>248030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.446526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">LGBMClassifier</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">class_weight</th>\n",
       "      <th>business</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>business</td>\n",
       "      <td>0.045110</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.702810</td>\n",
       "      <td>0.722916</td>\n",
       "      <td>185217</td>\n",
       "      <td>0.243253</td>\n",
       "      <td>23.331135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.701872</td>\n",
       "      <td>0.746537</td>\n",
       "      <td>186051</td>\n",
       "      <td>0.247165</td>\n",
       "      <td>66.248429</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">none</th>\n",
       "      <th>business</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>none</td>\n",
       "      <td>business</td>\n",
       "      <td>0.098081</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.693595</td>\n",
       "      <td>0.918188</td>\n",
       "      <td>244321</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>35.279004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>none</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>0.081405</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.690928</td>\n",
       "      <td>0.918048</td>\n",
       "      <td>244175</td>\n",
       "      <td>0.035991</td>\n",
       "      <td>38.124226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">smote</th>\n",
       "      <th>business</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>smote</td>\n",
       "      <td>business</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.930644</td>\n",
       "      <td>0.500299</td>\n",
       "      <td>282418</td>\n",
       "      <td>0.666786</td>\n",
       "      <td>77.495906</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>smote</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.918522</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>282425</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>37.399123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">LogisticRegression</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">class_weight</th>\n",
       "      <th>business</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.684323</td>\n",
       "      <td>0.705138</td>\n",
       "      <td>192218</td>\n",
       "      <td>0.229755</td>\n",
       "      <td>549.009265</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>class_weight</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.684323</td>\n",
       "      <td>0.705138</td>\n",
       "      <td>192218</td>\n",
       "      <td>0.229755</td>\n",
       "      <td>545.758310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">none</th>\n",
       "      <th>business</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>none</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.684409</td>\n",
       "      <td>0.919083</td>\n",
       "      <td>247592</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>365.198692</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>none</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.684409</td>\n",
       "      <td>0.919083</td>\n",
       "      <td>247592</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>353.174884</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">smote</th>\n",
       "      <th>business</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>smote</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.717979</td>\n",
       "      <td>0.511640</td>\n",
       "      <td>279549</td>\n",
       "      <td>0.671559</td>\n",
       "      <td>239.145348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-beta</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>smote</td>\n",
       "      <td>f-beta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.717979</td>\n",
       "      <td>0.511640</td>\n",
       "      <td>279549</td>\n",
       "      <td>0.671559</td>\n",
       "      <td>237.910887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Model      Strategy  \\\n",
       "Model              Strategy     Metric                                       \n",
       "DummyClassifier    class_weight business     DummyClassifier  class_weight   \n",
       "                                f-beta       DummyClassifier  class_weight   \n",
       "                   none         business     DummyClassifier          none   \n",
       "                                f-beta       DummyClassifier          none   \n",
       "                   smote        business     DummyClassifier         smote   \n",
       "                                f-beta       DummyClassifier         smote   \n",
       "LGBMClassifier     class_weight business      LGBMClassifier  class_weight   \n",
       "                                f-beta        LGBMClassifier  class_weight   \n",
       "                   none         business      LGBMClassifier          none   \n",
       "                                f-beta        LGBMClassifier          none   \n",
       "                   smote        business      LGBMClassifier         smote   \n",
       "                                f-beta        LGBMClassifier         smote   \n",
       "LogisticRegression class_weight business  LogisticRegression  class_weight   \n",
       "                                f-beta    LogisticRegression  class_weight   \n",
       "                   none         business  LogisticRegression          none   \n",
       "                                f-beta    LogisticRegression          none   \n",
       "                   smote        business  LogisticRegression         smote   \n",
       "                                f-beta    LogisticRegression         smote   \n",
       "\n",
       "                                            Metric  Learning Rate  Num Leaves  \\\n",
       "Model              Strategy     Metric                                          \n",
       "DummyClassifier    class_weight business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "                   none         business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "                   smote        business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "LGBMClassifier     class_weight business  business       0.045110        37.0   \n",
       "                                f-beta      f-beta       0.014809        67.0   \n",
       "                   none         business  business       0.098081        33.0   \n",
       "                                f-beta      f-beta       0.081405        70.0   \n",
       "                   smote        business  business       0.002907        50.0   \n",
       "                                f-beta      f-beta       0.003521        57.0   \n",
       "LogisticRegression class_weight business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "                   none         business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "                   smote        business  business            NaN         NaN   \n",
       "                                f-beta      f-beta            NaN         NaN   \n",
       "\n",
       "                                          Threshold       AUC  Accuracy  \\\n",
       "Model              Strategy     Metric                                    \n",
       "DummyClassifier    class_weight business       0.58  0.500000  0.919268   \n",
       "                                f-beta         0.50  0.500000  0.919268   \n",
       "                   none         business       0.56  0.500000  0.919268   \n",
       "                                f-beta         0.40  0.500000  0.919268   \n",
       "                   smote        business       0.48  0.500000  0.919268   \n",
       "                                f-beta         0.47  0.500000  0.919268   \n",
       "LGBMClassifier     class_weight business       0.53  0.702810  0.722916   \n",
       "                                f-beta         0.53  0.701872  0.746537   \n",
       "                   none         business       0.40  0.693595  0.918188   \n",
       "                                f-beta         0.40  0.690928  0.918048   \n",
       "                   smote        business       0.50  0.930644  0.500299   \n",
       "                                f-beta         0.44  0.918522  0.500000   \n",
       "LogisticRegression class_weight business       0.54  0.684323  0.705138   \n",
       "                                f-beta         0.54  0.684323  0.705138   \n",
       "                   none         business       0.40  0.684409  0.919083   \n",
       "                                f-beta         0.40  0.684409  0.919083   \n",
       "                   smote        business       0.54  0.717979  0.511640   \n",
       "                                f-beta         0.54  0.717979  0.511640   \n",
       "\n",
       "                                          Business Score    F-beta  \\\n",
       "Model              Strategy     Metric                               \n",
       "DummyClassifier    class_weight business          248030  0.000000   \n",
       "                                f-beta            248030  0.000000   \n",
       "                   none         business          248030  0.000000   \n",
       "                                f-beta            248030  0.000000   \n",
       "                   smote        business          248030  0.000000   \n",
       "                                f-beta            248030  0.000000   \n",
       "LGBMClassifier     class_weight business          185217  0.243253   \n",
       "                                f-beta            186051  0.247165   \n",
       "                   none         business          244321  0.034495   \n",
       "                                f-beta            244175  0.035991   \n",
       "                   smote        business          282418  0.666786   \n",
       "                                f-beta            282425  0.666667   \n",
       "LogisticRegression class_weight business          192218  0.229755   \n",
       "                                f-beta            192218  0.229755   \n",
       "                   none         business          247592  0.004405   \n",
       "                                f-beta            247592  0.004405   \n",
       "                   smote        business          279549  0.671559   \n",
       "                                f-beta            279549  0.671559   \n",
       "\n",
       "                                            Fit Time  Prediction Time  \n",
       "Model              Strategy     Metric                                 \n",
       "DummyClassifier    class_weight business    1.414213         0.000998  \n",
       "                                f-beta      1.450910         0.000000  \n",
       "                   none         business    1.500929         0.000000  \n",
       "                                f-beta      1.372604         0.000000  \n",
       "                   smote        business    1.469402         0.000000  \n",
       "                                f-beta      1.446526         0.000000  \n",
       "LGBMClassifier     class_weight business   23.331135         0.000000  \n",
       "                                f-beta     66.248429         0.000505  \n",
       "                   none         business   35.279004         0.000000  \n",
       "                                f-beta     38.124226         0.000000  \n",
       "                   smote        business   77.495906         0.000000  \n",
       "                                f-beta     37.399123         0.000000  \n",
       "LogisticRegression class_weight business  549.009265         0.000000  \n",
       "                                f-beta    545.758310         0.000000  \n",
       "                   none         business  365.198692         0.000000  \n",
       "                                f-beta    353.174884         0.000000  \n",
       "                   smote        business  239.145348         0.000000  \n",
       "                                f-beta    237.910887         0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby(['Model', 'Strategy', 'Metric']).apply(lambda x: x.loc[x['Business Score'].idxmin() if 'Business Score' in x.columns else x['F-beta'].idxmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a51adbb1-a086-4c82-b3d1-c61c56662e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisation des résultats\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Model', y='AUC', hue='Strategy', data=results_df)\n",
    "plt.title('Comparaison des AUC des modèles selon les stratégies d\\'équilibrage')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Model', y='Business Score', hue='Strategy', data=results_df)\n",
    "plt.title('Comparaison des Business Scores des modèles selon les stratégies d\\'équilibrage')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Model', y='F-beta', hue='Strategy', data=results_df)\n",
    "plt.title('Comparaison des F-beta des modèles selon les stratégies d\\'équilibrage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6ec07-ee0c-4052-b469-d2a56c5ddc2e",
   "metadata": {},
   "source": [
    "ANALYSE : \n",
    "- Modèle Recommandé : LGBMClassifier\n",
    "- Stratégie d'Équilibrage : class_weight pour sa robustesse sur les différentes métriques.\n",
    "- Métriques : Utilisation combinée de Business Score et F-beta pour une évaluation complète."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245ece6-9246-4e6c-92bd-25059925568e",
   "metadata": {},
   "source": [
    "#### Entrainement du dataset complet avec les paramètres choisis : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b4cf9-f1f1-4f1c-93d8-3a8131f63d55",
   "metadata": {},
   "source": [
    "##### Modèle initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bdcbf-4527-4fd8-8b5e-644eccda6bb7",
   "metadata": {},
   "source": [
    "Le modèle initial est principalement utilisé pour obtenir une compréhension de l'importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc96c92-5f52-4ebd-b5af-6761bcd33766",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = application_train_encoded.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = application_train_encoded[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d22de66d-3f92-40ba-807f-1b3c50ccc644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3321\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 232\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Entraîner un Modèle Initial avec class_weight\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('application_train_petit_encoded.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = df[\"TARGET\"]\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Calculer les poids de classe\n",
    "nb_0 = (y_train == 0).sum()\n",
    "nb_1 = (y_train == 1).sum()\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "# Entraîner un modèle initial avec class_weight\n",
    "initial_model = LGBMClassifier(random_state=42, class_weight=class_weights)\n",
    "initial_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les importances des features\n",
    "feature_importances = initial_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Créer un DataFrame pour les importances des features\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Trier les features par importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualiser les importances des features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'][:20], feature_importance_df['Importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Features par Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1479b-2f53-4793-9089-8316ffebda08",
   "metadata": {},
   "source": [
    "##### Sélection de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e07a43f-0151-4d4c-92c7-092ffda3b258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Sélectionner les Features Importantes\n",
    "\n",
    "# Définir le nombre de features à sélectionner\n",
    "num_features_to_select = 50  # Ajustez ce nombre selon vos besoins\n",
    "\n",
    "# Sélectionner les top features\n",
    "selected_features = feature_importance_df['Feature'][:num_features_to_select].values\n",
    "\n",
    "# Réduire les datasets à ces features\n",
    "X_train_reduced = pd.DataFrame(X_train, columns=features)[selected_features].values\n",
    "X_test_reduced = pd.DataFrame(X_test, columns=features)[selected_features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e650e7-3b2d-4a38-85ab-ef32e4477ad3",
   "metadata": {},
   "source": [
    "##### Optimisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "075426f2-d750-4c68-b2e9-a81f4b1bab50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/28 15:29:45 INFO mlflow.tracking.fluent: Experiment with name 'LightGBM_Optimization' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "Trial 50 complete\n",
      "Temps écoulé: 235628.26 secondes\n",
      "    Learning Rate  Num Leaves  N Estimators  Threshold       AUC  Accuracy  \\\n",
      "43       0.017345          38           571       0.51  0.701828  0.695861   \n",
      "16       0.020091          39           592       0.51  0.701640  0.700601   \n",
      "48       0.008973          64           883       0.53  0.701515  0.732800   \n",
      "46       0.012405          58           699       0.50  0.701873  0.690270   \n",
      "21       0.020574          35           460       0.51  0.701762  0.692895   \n",
      "\n",
      "    Business Score    F-beta   Fit Time  Prediction Time  \n",
      "43          148237  0.238049  23.477229            0.000  \n",
      "16          148251  0.238852  21.871479            0.000  \n",
      "48          148392  0.244921  40.956085            0.000  \n",
      "46          148396  0.236832  31.030549            0.001  \n",
      "21          148426  0.237219  18.089062            0.000  \n"
     ]
    }
   ],
   "source": [
    "### Optimiser les Hyperparamètres pour LightGBM avec class_weight\n",
    "\n",
    "import optuna\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, fbeta_score, roc_curve\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('LightGBM_Optimization')\n",
    "\n",
    "results = []\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=lr,\n",
    "        num_leaves=num_leaves,\n",
    "        n_estimators=n_estimators,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X_train_reduced, y_train, cv=StratifiedKFold(n_splits=5), method=\"predict_proba\")[:, 1]\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y_train, y_prob)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    fbeta = fbeta_score(y_train, y_pred, beta=1.0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "\n",
    "    results.append({\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"N Estimators\": n_estimators,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost,\n",
    "        \"F-beta\": fbeta,\n",
    "        \"Fit Time\": fit_duration,\n",
    "        \"Prediction Time\": pred_duration\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Learning Rate\", lr)\n",
    "        mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "        mlflow.log_param(\"N Estimators\", n_estimators)\n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        mlflow.log_metric(\"F-beta\", fbeta)\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_train, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, callbacks=[lambda study, trial: print(f\"Trial {trial.number + 1} complete\", end='\\r', flush=True)])\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTemps écoulé: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "print(results_df.sort_values(by=\"Business Score\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55d82064-e80f-44af-bac5-130bfcfdeb96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Num Leaves</th>\n",
       "      <th>N Estimators</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Business Score</th>\n",
       "      <th>F-beta</th>\n",
       "      <th>Fit Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011001</td>\n",
       "      <td>65</td>\n",
       "      <td>223</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.693383</td>\n",
       "      <td>0.801116</td>\n",
       "      <td>155370</td>\n",
       "      <td>0.246834</td>\n",
       "      <td>14.151775</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009588</td>\n",
       "      <td>53</td>\n",
       "      <td>193</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.688460</td>\n",
       "      <td>0.636344</td>\n",
       "      <td>153424</td>\n",
       "      <td>0.221646</td>\n",
       "      <td>11.945591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013990</td>\n",
       "      <td>32</td>\n",
       "      <td>289</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.696352</td>\n",
       "      <td>0.624468</td>\n",
       "      <td>151870</td>\n",
       "      <td>0.222713</td>\n",
       "      <td>14.601422</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036893</td>\n",
       "      <td>57</td>\n",
       "      <td>180</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.701334</td>\n",
       "      <td>0.682267</td>\n",
       "      <td>148788</td>\n",
       "      <td>0.234885</td>\n",
       "      <td>10.364678</td>\n",
       "      <td>0.006505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098345</td>\n",
       "      <td>64</td>\n",
       "      <td>102</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.698326</td>\n",
       "      <td>0.770557</td>\n",
       "      <td>150857</td>\n",
       "      <td>0.248945</td>\n",
       "      <td>6.972280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007678</td>\n",
       "      <td>31</td>\n",
       "      <td>115</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.670951</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>161812</td>\n",
       "      <td>0.229272</td>\n",
       "      <td>7.327165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001144</td>\n",
       "      <td>49</td>\n",
       "      <td>172</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.659381</td>\n",
       "      <td>0.376822</td>\n",
       "      <td>182371</td>\n",
       "      <td>0.178118</td>\n",
       "      <td>10.684080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006547</td>\n",
       "      <td>36</td>\n",
       "      <td>283</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.686384</td>\n",
       "      <td>0.700580</td>\n",
       "      <td>153017</td>\n",
       "      <td>0.230419</td>\n",
       "      <td>14.486479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009016</td>\n",
       "      <td>50</td>\n",
       "      <td>393</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.696762</td>\n",
       "      <td>0.523517</td>\n",
       "      <td>159303</td>\n",
       "      <td>0.205591</td>\n",
       "      <td>20.338109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004615</td>\n",
       "      <td>69</td>\n",
       "      <td>384</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.688914</td>\n",
       "      <td>0.614589</td>\n",
       "      <td>154613</td>\n",
       "      <td>0.217798</td>\n",
       "      <td>23.534292</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.045930</td>\n",
       "      <td>57</td>\n",
       "      <td>771</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.691806</td>\n",
       "      <td>0.633509</td>\n",
       "      <td>152708</td>\n",
       "      <td>0.222422</td>\n",
       "      <td>26.121397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.097026</td>\n",
       "      <td>61</td>\n",
       "      <td>105</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.698342</td>\n",
       "      <td>0.812155</td>\n",
       "      <td>154601</td>\n",
       "      <td>0.252409</td>\n",
       "      <td>6.812423</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.045583</td>\n",
       "      <td>44</td>\n",
       "      <td>142</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.701060</td>\n",
       "      <td>0.720134</td>\n",
       "      <td>148832</td>\n",
       "      <td>0.241459</td>\n",
       "      <td>8.395376</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028993</td>\n",
       "      <td>43</td>\n",
       "      <td>151</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.698277</td>\n",
       "      <td>0.701235</td>\n",
       "      <td>149616</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>9.220180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.031590</td>\n",
       "      <td>43</td>\n",
       "      <td>137</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.698199</td>\n",
       "      <td>0.572048</td>\n",
       "      <td>154863</td>\n",
       "      <td>0.214038</td>\n",
       "      <td>8.479176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.049144</td>\n",
       "      <td>43</td>\n",
       "      <td>253</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.701112</td>\n",
       "      <td>0.718027</td>\n",
       "      <td>148612</td>\n",
       "      <td>0.241452</td>\n",
       "      <td>11.067952</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020091</td>\n",
       "      <td>39</td>\n",
       "      <td>592</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701640</td>\n",
       "      <td>0.700601</td>\n",
       "      <td>148251</td>\n",
       "      <td>0.238852</td>\n",
       "      <td>21.871479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.019212</td>\n",
       "      <td>38</td>\n",
       "      <td>631</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.701726</td>\n",
       "      <td>0.620916</td>\n",
       "      <td>150988</td>\n",
       "      <td>0.223618</td>\n",
       "      <td>23.983745</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002865</td>\n",
       "      <td>39</td>\n",
       "      <td>545</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.683976</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>154514</td>\n",
       "      <td>0.235697</td>\n",
       "      <td>26.749337</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.065605</td>\n",
       "      <td>46</td>\n",
       "      <td>913</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.686147</td>\n",
       "      <td>0.823990</td>\n",
       "      <td>159171</td>\n",
       "      <td>0.243521</td>\n",
       "      <td>28.000787</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.017870</td>\n",
       "      <td>36</td>\n",
       "      <td>520</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701726</td>\n",
       "      <td>0.693143</td>\n",
       "      <td>148545</td>\n",
       "      <td>0.237057</td>\n",
       "      <td>20.522875</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.020574</td>\n",
       "      <td>35</td>\n",
       "      <td>460</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701762</td>\n",
       "      <td>0.692895</td>\n",
       "      <td>148426</td>\n",
       "      <td>0.237219</td>\n",
       "      <td>18.089062</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.019763</td>\n",
       "      <td>35</td>\n",
       "      <td>499</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.701740</td>\n",
       "      <td>0.678687</td>\n",
       "      <td>148696</td>\n",
       "      <td>0.234483</td>\n",
       "      <td>19.354670</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.020288</td>\n",
       "      <td>39</td>\n",
       "      <td>395</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701313</td>\n",
       "      <td>0.691853</td>\n",
       "      <td>148880</td>\n",
       "      <td>0.236268</td>\n",
       "      <td>17.057374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.014719</td>\n",
       "      <td>34</td>\n",
       "      <td>618</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.701650</td>\n",
       "      <td>0.608324</td>\n",
       "      <td>151635</td>\n",
       "      <td>0.221437</td>\n",
       "      <td>23.531488</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.024910</td>\n",
       "      <td>39</td>\n",
       "      <td>483</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.742060</td>\n",
       "      <td>148555</td>\n",
       "      <td>0.246682</td>\n",
       "      <td>18.674633</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.004802</td>\n",
       "      <td>31</td>\n",
       "      <td>762</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.661139</td>\n",
       "      <td>150741</td>\n",
       "      <td>0.228690</td>\n",
       "      <td>31.707156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013651</td>\n",
       "      <td>35</td>\n",
       "      <td>441</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.700432</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>149189</td>\n",
       "      <td>0.243089</td>\n",
       "      <td>19.343725</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.022581</td>\n",
       "      <td>47</td>\n",
       "      <td>336</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.701507</td>\n",
       "      <td>0.631307</td>\n",
       "      <td>150342</td>\n",
       "      <td>0.225686</td>\n",
       "      <td>16.023227</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.012422</td>\n",
       "      <td>41</td>\n",
       "      <td>631</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.701380</td>\n",
       "      <td>0.786302</td>\n",
       "      <td>151424</td>\n",
       "      <td>0.252118</td>\n",
       "      <td>26.007536</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002255</td>\n",
       "      <td>36</td>\n",
       "      <td>776</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.685464</td>\n",
       "      <td>0.678231</td>\n",
       "      <td>153272</td>\n",
       "      <td>0.226802</td>\n",
       "      <td>34.833376</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.027356</td>\n",
       "      <td>40</td>\n",
       "      <td>516</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.701253</td>\n",
       "      <td>0.745193</td>\n",
       "      <td>148955</td>\n",
       "      <td>0.246611</td>\n",
       "      <td>19.241157</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.016431</td>\n",
       "      <td>37</td>\n",
       "      <td>448</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.701456</td>\n",
       "      <td>0.719682</td>\n",
       "      <td>148529</td>\n",
       "      <td>0.241932</td>\n",
       "      <td>19.174675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.016212</td>\n",
       "      <td>34</td>\n",
       "      <td>332</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.699575</td>\n",
       "      <td>0.649808</td>\n",
       "      <td>149998</td>\n",
       "      <td>0.228402</td>\n",
       "      <td>15.457827</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.010569</td>\n",
       "      <td>53</td>\n",
       "      <td>588</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.701302</td>\n",
       "      <td>0.764336</td>\n",
       "      <td>150037</td>\n",
       "      <td>0.249093</td>\n",
       "      <td>27.934790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.016532</td>\n",
       "      <td>33</td>\n",
       "      <td>464</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701208</td>\n",
       "      <td>0.688346</td>\n",
       "      <td>148905</td>\n",
       "      <td>0.235653</td>\n",
       "      <td>19.189489</td>\n",
       "      <td>0.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.034078</td>\n",
       "      <td>37</td>\n",
       "      <td>215</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.700928</td>\n",
       "      <td>0.657501</td>\n",
       "      <td>149268</td>\n",
       "      <td>0.230530</td>\n",
       "      <td>11.022137</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.006292</td>\n",
       "      <td>41</td>\n",
       "      <td>682</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.698002</td>\n",
       "      <td>0.717046</td>\n",
       "      <td>149681</td>\n",
       "      <td>0.239289</td>\n",
       "      <td>31.451870</td>\n",
       "      <td>0.004510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.010979</td>\n",
       "      <td>32</td>\n",
       "      <td>424</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.698062</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>150317</td>\n",
       "      <td>0.243904</td>\n",
       "      <td>18.592163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.008567</td>\n",
       "      <td>46</td>\n",
       "      <td>972</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.701598</td>\n",
       "      <td>0.710801</td>\n",
       "      <td>148777</td>\n",
       "      <td>0.239770</td>\n",
       "      <td>40.205354</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.042356</td>\n",
       "      <td>31</td>\n",
       "      <td>364</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.701290</td>\n",
       "      <td>0.670619</td>\n",
       "      <td>149014</td>\n",
       "      <td>0.232761</td>\n",
       "      <td>13.511205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.024402</td>\n",
       "      <td>37</td>\n",
       "      <td>490</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.701545</td>\n",
       "      <td>0.726721</td>\n",
       "      <td>148590</td>\n",
       "      <td>0.243248</td>\n",
       "      <td>18.357001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.060896</td>\n",
       "      <td>41</td>\n",
       "      <td>275</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.699781</td>\n",
       "      <td>0.760808</td>\n",
       "      <td>149788</td>\n",
       "      <td>0.248712</td>\n",
       "      <td>11.437016</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.017345</td>\n",
       "      <td>38</td>\n",
       "      <td>571</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.701828</td>\n",
       "      <td>0.695861</td>\n",
       "      <td>148237</td>\n",
       "      <td>0.238049</td>\n",
       "      <td>23.477229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.016399</td>\n",
       "      <td>53</td>\n",
       "      <td>714</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.701238</td>\n",
       "      <td>0.695393</td>\n",
       "      <td>148532</td>\n",
       "      <td>0.237459</td>\n",
       "      <td>29.337350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.007196</td>\n",
       "      <td>53</td>\n",
       "      <td>821</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.701063</td>\n",
       "      <td>0.644661</td>\n",
       "      <td>149724</td>\n",
       "      <td>0.228181</td>\n",
       "      <td>38.281349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.012405</td>\n",
       "      <td>58</td>\n",
       "      <td>699</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.701873</td>\n",
       "      <td>0.690270</td>\n",
       "      <td>148396</td>\n",
       "      <td>0.236832</td>\n",
       "      <td>31.030549</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.012929</td>\n",
       "      <td>60</td>\n",
       "      <td>584</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.701510</td>\n",
       "      <td>0.639811</td>\n",
       "      <td>149809</td>\n",
       "      <td>0.227464</td>\n",
       "      <td>27.653302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.008973</td>\n",
       "      <td>64</td>\n",
       "      <td>883</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.701515</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>148392</td>\n",
       "      <td>0.244921</td>\n",
       "      <td>40.956085</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.005293</td>\n",
       "      <td>68</td>\n",
       "      <td>845</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.699590</td>\n",
       "      <td>0.708791</td>\n",
       "      <td>149334</td>\n",
       "      <td>0.238396</td>\n",
       "      <td>44.842185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Learning Rate  Num Leaves  N Estimators  Threshold       AUC  Accuracy  \\\n",
       "0        0.011001          65           223       0.58  0.693383  0.801116   \n",
       "1        0.009588          53           193       0.49  0.688460  0.636344   \n",
       "2        0.013990          32           289       0.48  0.696352  0.624468   \n",
       "3        0.036893          57           180       0.50  0.701334  0.682267   \n",
       "4        0.098345          64           102       0.56  0.698326  0.770557   \n",
       "5        0.007678          31           115       0.55  0.670951  0.798561   \n",
       "6        0.001144          49           172       0.48  0.659381  0.376822   \n",
       "7        0.006547          36           283       0.52  0.686384  0.700580   \n",
       "8        0.009016          50           393       0.43  0.696762  0.523517   \n",
       "9        0.004615          69           384       0.48  0.688914  0.614589   \n",
       "10       0.045930          57           771       0.41  0.691806  0.633509   \n",
       "11       0.097026          61           105       0.60  0.698342  0.812155   \n",
       "12       0.045583          44           142       0.53  0.701060  0.720134   \n",
       "13       0.028993          43           151       0.52  0.698277  0.701235   \n",
       "14       0.031590          43           137       0.45  0.698199  0.572048   \n",
       "15       0.049144          43           253       0.52  0.701112  0.718027   \n",
       "16       0.020091          39           592       0.51  0.701640  0.700601   \n",
       "17       0.019212          38           631       0.46  0.701726  0.620916   \n",
       "18       0.002865          39           545       0.54  0.683976  0.747475   \n",
       "19       0.065605          46           913       0.57  0.686147  0.823990   \n",
       "20       0.017870          36           520       0.51  0.701726  0.693143   \n",
       "21       0.020574          35           460       0.51  0.701762  0.692895   \n",
       "22       0.019763          35           499       0.50  0.701740  0.678687   \n",
       "23       0.020288          39           395       0.51  0.701313  0.691853   \n",
       "24       0.014719          34           618       0.46  0.701650  0.608324   \n",
       "25       0.024910          39           483       0.54  0.701613  0.742060   \n",
       "26       0.004802          31           762       0.50  0.695175  0.661139   \n",
       "27       0.013651          35           441       0.54  0.700432  0.731425   \n",
       "28       0.022581          47           336       0.47  0.701507  0.631307   \n",
       "29       0.012422          41           631       0.58  0.701380  0.786302   \n",
       "30       0.002255          36           776       0.51  0.685464  0.678231   \n",
       "31       0.027356          40           516       0.54  0.701253  0.745193   \n",
       "32       0.016431          37           448       0.53  0.701456  0.719682   \n",
       "33       0.016212          34           332       0.49  0.699575  0.649808   \n",
       "34       0.010569          53           588       0.56  0.701302  0.764336   \n",
       "35       0.016532          33           464       0.51  0.701208  0.688346   \n",
       "36       0.034078          37           215       0.49  0.700928  0.657501   \n",
       "37       0.006292          41           682       0.53  0.698002  0.717046   \n",
       "38       0.010979          32           424       0.55  0.698062  0.745400   \n",
       "39       0.008567          46           972       0.52  0.701598  0.710801   \n",
       "40       0.042356          31           364       0.49  0.701290  0.670619   \n",
       "41       0.024402          37           490       0.53  0.701545  0.726721   \n",
       "42       0.060896          41           275       0.55  0.699781  0.760808   \n",
       "43       0.017345          38           571       0.51  0.701828  0.695861   \n",
       "44       0.016399          53           714       0.50  0.701238  0.695393   \n",
       "45       0.007196          53           821       0.48  0.701063  0.644661   \n",
       "46       0.012405          58           699       0.50  0.701873  0.690270   \n",
       "47       0.012929          60           584       0.47  0.701510  0.639811   \n",
       "48       0.008973          64           883       0.53  0.701515  0.732800   \n",
       "49       0.005293          68           845       0.52  0.699590  0.708791   \n",
       "\n",
       "    Business Score    F-beta   Fit Time  Prediction Time  \n",
       "0           155370  0.246834  14.151775         0.000000  \n",
       "1           153424  0.221646  11.945591         0.000000  \n",
       "2           151870  0.222713  14.601422         0.000995  \n",
       "3           148788  0.234885  10.364678         0.006505  \n",
       "4           150857  0.248945   6.972280         0.000000  \n",
       "5           161812  0.229272   7.327165         0.000000  \n",
       "6           182371  0.178118  10.684080         0.000000  \n",
       "7           153017  0.230419  14.486479         0.000000  \n",
       "8           159303  0.205591  20.338109         0.000000  \n",
       "9           154613  0.217798  23.534292         0.001065  \n",
       "10          152708  0.222422  26.121397         0.000000  \n",
       "11          154601  0.252409   6.812423         0.000000  \n",
       "12          148832  0.241459   8.395376         0.000000  \n",
       "13          149616  0.236565   9.220180         0.000000  \n",
       "14          154863  0.214038   8.479176         0.000000  \n",
       "15          148612  0.241452  11.067952         0.000000  \n",
       "16          148251  0.238852  21.871479         0.000000  \n",
       "17          150988  0.223618  23.983745         0.000000  \n",
       "18          154514  0.235697  26.749337         0.000000  \n",
       "19          159171  0.243521  28.000787         0.000000  \n",
       "20          148545  0.237057  20.522875         0.000000  \n",
       "21          148426  0.237219  18.089062         0.000000  \n",
       "22          148696  0.234483  19.354670         0.000000  \n",
       "23          148880  0.236268  17.057374         0.000000  \n",
       "24          151635  0.221437  23.531488         0.002572  \n",
       "25          148555  0.246682  18.674633         0.000000  \n",
       "26          150741  0.228690  31.707156         0.000000  \n",
       "27          149189  0.243089  19.343725         0.000000  \n",
       "28          150342  0.225686  16.023227         0.000000  \n",
       "29          151424  0.252118  26.007536         0.000000  \n",
       "30          153272  0.226802  34.833376         0.000000  \n",
       "31          148955  0.246611  19.241157         0.000000  \n",
       "32          148529  0.241932  19.174675         0.000000  \n",
       "33          149998  0.228402  15.457827         0.000000  \n",
       "34          150037  0.249093  27.934790         0.000000  \n",
       "35          148905  0.235653  19.189489         0.000557  \n",
       "36          149268  0.230530  11.022137         0.001000  \n",
       "37          149681  0.239289  31.451870         0.004510  \n",
       "38          150317  0.243904  18.592163         0.000000  \n",
       "39          148777  0.239770  40.205354         0.000000  \n",
       "40          149014  0.232761  13.511205         0.000000  \n",
       "41          148590  0.243248  18.357001         0.000000  \n",
       "42          149788  0.248712  11.437016         0.001000  \n",
       "43          148237  0.238049  23.477229         0.000000  \n",
       "44          148532  0.237459  29.337350         0.000000  \n",
       "45          149724  0.228181  38.281349         0.000000  \n",
       "46          148396  0.236832  31.030549         0.001000  \n",
       "47          149809  0.227464  27.653302         0.000000  \n",
       "48          148392  0.244921  40.956085         0.000000  \n",
       "49          149334  0.238396  44.842185         0.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9af59-0377-492e-b4a4-feade5f29c88",
   "metadata": {},
   "source": [
    "##### Seriation, signature MLFlow et sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90f6263d-2a89-46a2-9e8f-46b6aab613b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19842, number of negative: 225940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3321\n",
      "[LightGBM] [Info] Number of data points in the train set: 245782, number of used features: 232\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15873, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 196625, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000038\n",
      "[LightGBM] [Info] Start training from score -0.000038\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2750\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2747\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Info] Number of positive: 15874, number of negative: 180752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2745\n",
      "[LightGBM] [Info] Number of data points in the train set: 196626, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500006 -> initscore=0.000025\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "Best Parameters: {'lr': 0.03871967950335181, 'num_leaves': 34, 'n_estimators': 171, 'threshold': 0.51}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'signature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_params(best_params)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m### Sauvegarde du pipeline : \u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39msave_model(pipeline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m, signature\u001b[38;5;241m=\u001b[39m\u001b[43msignature\u001b[49m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Calcul des performances sur les données de test\u001b[39;00m\n\u001b[0;32m    172\u001b[0m X_test_reduced \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test, columns\u001b[38;5;241m=\u001b[39mfeatures)[selected_features]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signature' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, fbeta_score, roc_curve\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('application_train_petit_encoded.csv')\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X = df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = df[\"TARGET\"]\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Calculer les poids de classe\n",
    "nb_0 = (y_train == 0).sum()\n",
    "nb_1 = (y_train == 1).sum()\n",
    "class_weights = {0: 1, 1: nb_0 / nb_1}\n",
    "\n",
    "# Entraîner un modèle initial avec class_weight pour obtenir les importances des features\n",
    "initial_model = LGBMClassifier(random_state=42, class_weight=class_weights)\n",
    "initial_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les importances des features\n",
    "feature_importances = initial_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Créer un DataFrame pour les importances des features\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Trier les features par importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Définir le nombre de features à sélectionner\n",
    "num_features_to_select = 50\n",
    "\n",
    "# Sélectionner les top features\n",
    "selected_features = feature_importance_df['Feature'][:num_features_to_select].values\n",
    "\n",
    "# Réduire les datasets à ces features\n",
    "X_train_reduced = pd.DataFrame(X_train, columns=features)[selected_features].values\n",
    "X_test_reduced = pd.DataFrame(X_test, columns=features)[selected_features].values\n",
    "\n",
    "# Configuration initiale de MLflow\n",
    "mlflow.set_experiment('LightGBM_Optimization')\n",
    "\n",
    "results = []\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.1, log=True)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 31, 70)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.01)\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=lr,\n",
    "        num_leaves=num_leaves,\n",
    "        n_estimators=n_estimators,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    start_fit_time = time.time()\n",
    "    y_prob = cross_val_predict(model, X_train_reduced, y_train, cv=StratifiedKFold(n_splits=5), method=\"predict_proba\")[:, 1]\n",
    "    fit_duration = time.time() - start_fit_time\n",
    "\n",
    "    start_pred_time = time.time()\n",
    "    y_pred = y_prob > threshold\n",
    "    pred_duration = time.time() - start_pred_time\n",
    "\n",
    "    auc = roc_auc_score(y_train, y_prob)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    fbeta = fbeta_score(y_train, y_pred, beta=1.0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "    cost = fp + 10 * fn\n",
    "\n",
    "    results.append({\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Num Leaves\": num_leaves,\n",
    "        \"N Estimators\": n_estimators,\n",
    "        \"Threshold\": threshold,\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Business Score\": cost,\n",
    "        \"F-beta\": fbeta,\n",
    "        \"Fit Time\": fit_duration,\n",
    "        \"Prediction Time\": pred_duration\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"Learning Rate\", lr)\n",
    "        mlflow.log_param(\"Num Leaves\", num_leaves)\n",
    "        mlflow.log_param(\"N Estimators\", n_estimators)\n",
    "        mlflow.log_param(\"Threshold\", round(threshold, 2))\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        mlflow.log_metric(\"Business Score\", cost)\n",
    "        mlflow.log_metric(\"F-beta\", fbeta)\n",
    "        mlflow.log_metric(\"Fit Time\", fit_duration)\n",
    "        mlflow.log_metric(\"Prediction Time\", pred_duration)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_train, y_prob)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, label=f'AUC: {auc:.2f}')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, callbacks=[lambda study, trial: print(f\"Trial {trial.number + 1} complete\", end='\\r', flush=True)])\n",
    "\n",
    "# Obtenez les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "\n",
    "# Affichez les meilleurs paramètres pour vérification\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Créer et entraîner le pipeline avec les meilleurs paramètres\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('regressor', LGBMClassifier(\n",
    "        learning_rate=best_params['lr'], \n",
    "        num_leaves=best_params['num_leaves'], \n",
    "        n_estimators=best_params['n_estimators'], \n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Réduire les datasets à ces features\n",
    "X_reduced = pd.DataFrame(X, columns=features)[selected_features].values\n",
    "\n",
    "# Apprentissage sur les données train\n",
    "pipeline.fit(X_reduced, y)\n",
    "\n",
    "# Enregistrer le modèle final dans MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(pipeline, \"final_model\")\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "### Sauvegarde du pipeline : \n",
    "mlflow.sklearn.save_model(pipeline, 'final_model', signature=signature)\n",
    "\n",
    "# Calcul des performances sur les données de test\n",
    "X_test_reduced = pd.DataFrame(X_test, columns=features)[selected_features].values\n",
    "y_pred = pipeline.predict(X_test_reduced)\n",
    "accuracy = pipeline.score(X_test_reduced, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4c943-ad80-4cfb-97ce-55ef8c5cbdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
